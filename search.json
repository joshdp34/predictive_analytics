[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 5320 Predictive Analytics",
    "section": "",
    "text": "Introduction\nThese are the lecture notes for STA 5320 - Predictive Analytics.\nPrerequisites: Regression Analysis\n\nCourse Description:\nConcepts, methods, and tools used for predictive modeling and data analytics with applications are considered. The focus of this course is on advanced tools using various multivariate regression techniques, statistical modeling, machine learning, and simulation for forecasting. Practical applications are emphasized."
  },
  {
    "objectID": "01_Tidymodels.html#the-tidymodels-framework",
    "href": "01_Tidymodels.html#the-tidymodels-framework",
    "title": "1  Introduction to Tidymodels",
    "section": "1.1 The tidymodels Framework",
    "text": "1.1 The tidymodels Framework\nThe tidymodels framework brings together the principles of tidy data and the tidyverse, making it easy to integrate data preprocessing, model training, and evaluation into a unified workflow. Below, we’ll explore the core components of the tidymodels framework, each of which plays a vital role in the modeling process."
  },
  {
    "objectID": "01_Tidymodels.html#recipes",
    "href": "01_Tidymodels.html#recipes",
    "title": "1  Introduction to Tidymodels",
    "section": "1.2 recipes",
    "text": "1.2 recipes\nIn predictive analytics, the data we work with often needs to be cleaned and transformed before it can be used for modeling. The recipes package in tidymodels is designed to help automate and streamline this process. A recipe is a blueprint for data preprocessing, allowing you to specify how to transform raw data into a form suitable for modeling. This can include tasks like normalizing numerical variables, encoding categorical variables, handling missing values, and creating new features.\n\n1.2.1 The Core Structure of a Recipe\nA recipe starts by defining a formula, typically in the form of a dependent variable (outcome) and independent variables (predictors), followed by one or more step_ functions that specify the transformations to be applied to the data. These steps are applied in the order they are specified in the recipe, ensuring that your preprocessing is both consistent and repeatable.\nlibrary(recipes)\n\n# Define the recipe\nrecipe_obj = recipe(outcome ~ ., data = data) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_factor_predictors())\nIn this example, we define a recipe where outcome is the dependent variable and all other variables are predictors. We first normalize all numeric predictors using step_normalize(), and then create dummy variables for all factor predictors using step_dummy().\n\n\n1.2.2 Common step_ Functions\nThe recipes package includes several commonly used step_ functions to handle different data transformation tasks. Here are a few of the most frequently used steps:\n\nstep_normalize()\nThe step_normalize() function is used to standardize numeric predictors by scaling them so that they have a mean of 0 and a standard deviation of 1. This is particularly useful when predictors vary greatly in scale, such as when comparing different player statistics like points, assists, and rebounds. Normalization ensures that all variables contribute equally to the model.\n\n\nstep_dummy()\nThe step_dummy() function is used to convert categorical variables (factors) into dummy (binary) variables, which are required for most machine learning models.\n\n\nstep_naomit()\nThe step_naomit() function is used to remove rows with missing values. In sports datasets, it’s common to encounter missing data, and this function can help clean the data by excluding those rows. This step is often used when missing data can’t be easily imputed or when the model cannot handle missing values.\n\n\nstep_center()\nThe step_center() function centers numeric variables by subtracting the mean of each variable from the values in that variable. This can be useful when you want to ensure that all variables are centered around zero, particularly when working with models that are sensitive to the scale of the data, such as linear regression or neural networks.\n\n\nstep_interact()\nThe step_interact() function creates interaction terms between two or more variables. Interaction terms can be important in predictive analytics, where the effect of one variable might depend on the value of another variable. This step allows you to automatically create these interaction terms.\n\n\nstep_zv()\nThe step_zv() function removes any predictors with zero variance, meaning those variables that do not vary at all across the data (e.g., a column where all the values are the same). These variables do not provide any useful information for the model and can safely be removed.\n\n\n\n1.2.3 Combining Steps\nYou can combine multiple step_ functions in a single recipe to handle a variety of preprocessing tasks. This ensures that all the necessary transformations are applied consistently across both training and testing datasets.\n\n\n1.2.4 Applying the Recipe\nOnce a recipe is defined, it can be applied to the data using the prep() function, which prepares the recipe by applying the specified transformations to the dataset. After the recipe is prepared, it can be used to bake (i.e., apply the preprocessing steps) to both training and test datasets:\nprepped_recipe = prep(recipe_obj, training = player_data)\n\n# Apply the transformations to the training and test datasets\ntrain_data = bake(prepped_recipe, new_data = player_data)\ntest_data = bake(prepped_recipe, new_data = player_data)"
  },
  {
    "objectID": "01_Tidymodels.html#parsnip",
    "href": "01_Tidymodels.html#parsnip",
    "title": "1  Introduction to Tidymodels",
    "section": "1.3 parsnip",
    "text": "1.3 parsnip\nThe parsnip package in tidymodels provides a unified interface for specifying machine learning models in R. It allows you to define the type of model you want to fit (such as linear regression, decision trees, or random forests) without worrying about the underlying implementation details. This consistency across different model types makes it easy to switch between models or experiment with multiple models within the same framework.\nThe power of parsnip lies in its simplicity and flexibility. You only need to specify the model type and the engine (the underlying R package or algorithm used to fit the model), and parsnip handles the rest. This approach not only simplifies the modeling process but also allows you to experiment with different modeling techniques using the same set of tools and functions.\n\n1.3.1 Specifying Models with parsnip\nWhen using parsnip, you first define the type of model you want to create. This is done using a function for the specific model, such as linear_reg() for linear regression or rand_forest() for random forests. Once the model type is specified, you set the engine, which determines how the model is fitted (e.g., using lm for linear regression or rpart for decision trees). The next step is to fit the model to the data.\nHere’s an example of how to specify and fit a linear regression model:\nlibrary(parsnip)\n\n# Specify a linear regression model\nmodel = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n# Fit the model to the data\ntrained_model = model |&gt;\n  fit(outcome ~ x1 + x2, data = dat)\nIn this example, linear_reg() specifies a linear regression model, and set_engine(\"lm\") tells parsnip to use the lm engine to fit the model. The fit() function then trains the model using the dat dataset, predicting the outcome variable from the x1 and x2 predictors.\n\n\n1.3.2 Common Models in parsnip\nHere are some of the most common models you can specify using parsnip:\n\nLinear Regression (linear_reg)\nLinear regression is one of the simplest and most commonly used models for predicting a continuous outcome variable. In parsnip, the function linear_reg() is used to specify this model, and you can choose the engine (e.g., lm for the standard linear regression implementation or glmnet for regularized regression).\n# Linear regression model\nmodel_lr = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n\nLogistic Regression (logistic_reg)\nLogistic regression is commonly used when the outcome variable is binary (e.g., win/loss, success/failure). In parsnip, logistic regression is specified using logistic_reg() and can be fitted using engines such as glm or spark.\n# Logistic regression model\nmodel_logistic = logistic_reg() |&gt;\n  set_engine(\"glm\")\n\n\nRandom Forest (rand_forest)\nRandom forests are a powerful ensemble learning method that can be used for both classification and regression. In parsnip, you can specify a random forest model using the rand_forest() function. You can choose the engine (e.g., ranger or randomForest) to fit the model.\n# Random forest model\nmodel_rf = rand_forest() |&gt;\n  set_engine(\"ranger\")\n\n\nDecision Trees (decision_tree)\nDecision trees are a non-linear model that can be used for both classification and regression tasks. In parsnip, decision trees are specified with decision_tree(), and the engine can be set to rpart or party for different implementations.\n# Decision tree model\nmodel_tree = decision_tree() |&gt;\n  set_engine(\"rpart\")\n\n\nSupport Vector Machines (svm_rbf)\nSupport Vector Machines (SVM) are used for both classification and regression tasks and are particularly effective in high-dimensional spaces. In parsnip, you can specify an SVM model using svm_rbf(), and you can choose engines like kernlab or e1071.\n# SVM model with radial basis function kernel\nmodel_svm = svm_rbf() |&gt;\n  set_engine(\"kernlab\")\n\n\nBoosted Trees (boost_tree)\nBoosted trees, such as Gradient Boosting Machines (GBM), are another powerful ensemble method that can be used for both classification and regression. In parsnip, you can specify a boosted tree model using boost_tree() and set the engine to xgboost or lightgbm.\n# Boosted tree model\nmodel_boost = boost_tree() |&gt;\n  set_engine(\"xgboost\")\n\n\n\n1.3.3 Tuning Models\nMany models, such as random forests, SVMs, and boosted trees, have hyperparameters that can be tuned to improve model performance. parsnip provides a consistent interface for setting and tuning these hyperparameters. After specifying a model, you can use the tune package to perform grid search or random search to find the optimal parameters.\nFor example, to tune a random forest model, you can use the tune_grid() function to search over different values for the number of trees (trees) and the maximum number of splits (min_n):\nlibrary(tune)\n\n# Define a grid of hyperparameters\ngrid = grid_regular(trees(range = c(100, 1000)), min_n(range = c(2, 20)), levels = 5)\n\n# Tune the model\ntuned_rf = tune_grid(\n  trained_model_rf,\n  resamples = vfold_cv(dat),\n  grid = grid\n)\nThis code runs a grid search over the number of trees and the minimum number of splits to optimize the performance of the random forest model."
  },
  {
    "objectID": "01_Tidymodels.html#workflows",
    "href": "01_Tidymodels.html#workflows",
    "title": "1  Introduction to Tidymodels",
    "section": "1.4 workflows",
    "text": "1.4 workflows\nThe workflows package in tidymodels is designed to streamline the process of building, training, and evaluating machine learning models by combining the different stages of the modeling process into a single, unified object. This object allows you to encapsulate the entire modeling pipeline, from preprocessing steps to model specification, into a single workflow. Using workflows ensures that your data preprocessing and modeling steps are tightly integrated and that the entire pipeline is reusable, reproducible, and easy to manage.\nThe power of workflows lies in its ability to combine preprocessing (via recipes) with model specification (via parsnip) into a cohesive object that can be easily trained, evaluated, and tuned. This simplifies the process of applying a series of steps to both training and testing data, making your model development process more efficient and organized.\n\n1.4.1 The Structure of a Workflow\nA workflow consists of two main components: a recipe and a model. The recipe defines the preprocessing steps (e.g., data normalization, encoding, etc.), while the model specifies the type of machine learning model to be fit (e.g., linear regression, random forest, etc.). Once the recipe and model are combined into a workflow, the workflow object can be used to fit, evaluate, and tune the model.\nHere’s how you can create a simple workflow:\nlibrary(workflows)\n\n# Define the recipe\nrecipe_obj = recipe(outcome ~ x1 + x2, data = dat) |&gt;\n  step_normalize(x1) |&gt;\n  step_dummy(x2)\n\n# Define the model\nmodel_obj = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n# Create the workflow\nworkflow_obj = workflow() |&gt;\n  add_recipe(recipe_obj) |&gt;\n  add_model(model_obj)\n\n# Fit the workflow to the data\nfitted_workflow = workflow_obj |&gt;\n  fit(data = dat)\nIn this example, we first define a recipe that normalizes the points, assists, and rebounds variables, as well as creates dummy variables for the team variable. We then specify a linear regression model using linear_reg(). Finally, we combine the recipe and model into a single workflow object using workflow(), and the fit() function is used to train the model on the dat dataset.\n\n\n1.4.2 Adding Recipes and Models to a Workflow\nYou can easily add both the recipe and the model to a workflow using the add_recipe() and add_model() functions, respectively. Once these components are added, the workflow object is ready for training, evaluation, and tuning.\n# Add the recipe and model to the workflow\nworkflow_obj = workflow() |&gt;\n  add_recipe(recipe_obj) |&gt;\n  add_model(model_obj)\nThis step ensures that your workflow is built with the appropriate preprocessing steps and model, making it ready to be applied to your data.\n\n\n1.4.3 Tuning a Workflow\nOne of the key benefits of using workflows is that you can integrate tuning directly into the workflow process. For instance, when tuning hyperparameters for models like random forests or support vector machines, you can pass the workflow to the tune_grid() function along with a grid of hyperparameters to optimize.\nlibrary(tune)\n\n# Define a grid of hyperparameters\ngrid = grid_regular(trees(range = c(100, 1000)), min_n(range = c(2, 20)), levels = 5)\n\n# Tune the workflow\ntuned_workflow = tune_grid(\n  workflow_obj,\n  resamples = vfold_cv(dat),\n  grid = grid\n)\nIn this example, we use tune_grid() to tune a random forest model by varying the number of trees (trees) and the minimum number of splits (min_n). The grid search is performed over these hyperparameters using cross-validation, ensuring that the best-performing model configuration is selected.\n\n\n1.4.4 Evaluating a Workflow\nOnce the workflow is trained, you can evaluate its performance using the yardstick package, which provides a consistent set of functions for model evaluation. For instance, after fitting the workflow, you can generate predictions on a test set and calculate metrics such as accuracy, RMSE, or AUC.\nlibrary(yardstick)\n\n# Generate predictions\npredictions = predict(fitted_workflow, new_data = test_data)\n\n# Evaluate the performance\naccuracy(predictions, truth = test_data$outcome)\nHere, the predict() function is used to generate predictions from the fitted workflow, and the accuracy() function is used to calculate the accuracy of the model’s predictions on the test data.\n\n\n1.4.5 Workflow and Resampling\nWorkflows can also be integrated with resampling methods to evaluate the model’s performance across different subsets of the data. For example, you can use rsample to perform cross-validation or bootstrapping and pass the resampled data to the workflow for evaluation.\nlibrary(rsample)\n\n# Create a 5-fold cross-validation resampling object\ncv_folds = vfold_cv(dat, v = 5)\n\n# Tune the workflow with cross-validation\ntuned_workflow_cv = tune_grid(\n  workflow_obj,\n  resamples = cv_folds,\n  grid = grid\n)\nThis example uses vfold_cv() to create a cross-validation object, and the workflow is tuned using this resampling method to get a better estimate of the model’s performance across different data splits."
  },
  {
    "objectID": "02_ML.html#what-is-machine-learning",
    "href": "02_ML.html#what-is-machine-learning",
    "title": "2  Introduction to Machine Learning",
    "section": "2.1 What is Machine Learning?",
    "text": "2.1 What is Machine Learning?\nMachine Learning (ML) is a branch of artificial intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to perform specific tasks without being explicitly programmed. Instead of relying on manually coded instructions, ML algorithms learn patterns from data, improving their performance over time as they process more examples. The core concept of machine learning is to create models that can learn from past data and make predictions or decisions based on that information.\nIn business, ML applications can range from predicting customer churn and optimizing marketing campaigns to forecasting sales and managing risk.\nThe process of machine learning begins with data collection, followed by cleaning and preparing the data for analysis. Once the data is ready, an appropriate machine learning model is chosen based on the type of problem to be solved (e.g., classification or regression). The model is then trained using a training dataset and evaluated on a validation set to ensure it can make accurate predictions on new, unseen data."
  },
  {
    "objectID": "02_ML.html#why-opt-for-machine-learning",
    "href": "02_ML.html#why-opt-for-machine-learning",
    "title": "2  Introduction to Machine Learning",
    "section": "2.2 Why Opt for Machine Learning?",
    "text": "2.2 Why Opt for Machine Learning?\nMachine learning offers a powerful approach for solving complex problems in predictive analytics where traditional methods often fall short. One of the key strengths of machine learning is its ability to improve predictions and decision-making as more data becomes available. Unlike traditional programming, where explicit rules are coded, ML models learn from data, continually refining their accuracy as they are exposed to new information.\nIn predictive analytics, machine learning can handle the complexity and large volumes of data generated by modern technologies. Traditional statistical methods may struggle to identify subtle patterns in these large datasets, but ML algorithms are designed to extract valuable insights from vast and often noisy data sources."
  },
  {
    "objectID": "02_ML.html#machine-learning-applications",
    "href": "02_ML.html#machine-learning-applications",
    "title": "2  Introduction to Machine Learning",
    "section": "2.3 Machine Learning Applications",
    "text": "2.3 Machine Learning Applications\nMachine Learning applications are vast and varied, including image and speech recognition, natural language processing, and more. These tasks employ different techniques, such as neural networks and deep learning, to analyze images, understand speech and language, or forecast trends.\n\nImage and Speech Recognition\nMachine learning algorithms, particularly those in deep learning, have dramatically improved the accuracy of image and speech recognition systems. In image recognition, ML models are trained with vast datasets of images to recognize patterns, objects, and even faces with remarkable precision. This technology underpins applications ranging from security systems that use facial recognition to authenticate identities, to medical imaging software that assists in diagnosing diseases by identifying abnormal patterns in X-rays or MRI scans.\nSpeech recognition has similarly benefited from ML, enabling devices and software to understand and transcribe human speech with high accuracy. This technology is the backbone of virtual assistants like Siri and Alexa, real-time translation services, and accessibility tools for those with disabilities, making technology more accessible and interactive.\n\n\nNatural Language Processing (NLP)\nNLP uses machine learning to understand, interpret, and generate human language in a way that is both meaningful and useful. Applications of NLP include chatbots and virtual assistants that can understand and respond to user queries in natural language, sentiment analysis tools that gauge public opinion from social media content, and machine translation systems that break down language barriers by translating text from one language to another.\n\n\nAutonomous Vehicles\nMachine learning plays a crucial role in the development of autonomous vehicles (AVs). By processing data from various sensors and cameras, ML algorithms help AVs understand their surroundings, make decisions in real-time, and navigate safely through complex environments. This technology promises to revolutionize transportation by reducing human error, enhancing traffic efficiency, and increasing accessibility for those unable to drive.\n\n\nPredictive Analytics\nIn industries ranging from finance to healthcare, ML is used for predictive analytics, leveraging historical data to predict future trends, behaviors, and outcomes. In finance, ML models predict stock market trends, assess credit risk, and detect fraudulent activities. In healthcare, predictive analytics can forecast disease outbreaks, patient admissions, and potential complications, improving patient care and operational efficiency.\n\n\nPersonalization and Recommendation Systems\nMachine learning drives the personalized experiences users have come to expect from online platforms. By analyzing user behavior, preferences, and interactions, ML algorithms can tailor content, recommendations, and services to each user. This technology powers the recommendation engines behind streaming services like Netflix and Spotify, e-commerce platforms like Amazon, and social media feeds, enhancing user engagement and satisfaction.\n\n\nRobotics and Automation\nML is also critical in robotics, where it enables robots to learn from their environment and experience, adapt to new tasks, and perform complex actions with a degree of autonomy. This has applications in manufacturing, where robots can assist in or automate assembly lines, in logistics for warehouse automation, and in service robots that assist in homes and healthcare settings.\nMachine learning applications are diverse and continually evolving, touching nearly every aspect of modern life. From improving how we communicate and access information to enhancing healthcare, financial services, and transportation, ML technologies are at the forefront of digital innovation. As machine learning continues to advance, we can expect its applications to expand further, creating new opportunities and solving challenges in ways we have yet to imagine."
  },
  {
    "objectID": "02_ML.html#types-of-machine-learning-systems",
    "href": "02_ML.html#types-of-machine-learning-systems",
    "title": "2  Introduction to Machine Learning",
    "section": "2.4 Types of Machine Learning Systems",
    "text": "2.4 Types of Machine Learning Systems\nMachine learning systems can be categorized based on the approach they take to learn from data. These categories include supervised versus unsupervised learning, batch versus online learning, and instance-based versus model-based learning. Understanding the differences between these types of systems is essential for applying machine learning effectively in predictive analytics. In this section, we’ll explore these distinctions.\n\n2.4.1 Supervised vs. Unsupervised Learning\n\nSupervised Learning\nIn supervised learning, the algorithm learns from a labeled dataset, where each example in the training data is paired with the correct output. The model is trained to map input data to an output variable. This process is called supervised learning because the model is “supervised” during training by the known labels, guiding the algorithm to make accurate predictions.\nSupervised learning is typically used for two types of problems in predictive analytics: classification and regression. For example:\n\nClassification: Predicting categorical outcomes.\nRegression: Predicting continuous outcomes.\n\n\n\nUnsupervised Learning\nIn contrast to supervised learning, unsupervised learning works with data that does not have labels. The goal is to find hidden patterns, relationships, or groupings within the data without any predefined outcome variable. Unsupervised learning is often used for clustering or dimensionality reduction. Applications include customer segmentation, market basket analysis, and anomaly detection.\n\n\n\n2.4.2 Batch vs. Online Learning\n\nBatch Learning\nBatch learning refers to training a machine learning model on the entire dataset at once. This approach is typically used when the dataset is static and does not change frequently. Once the model is trained, it is deployed to make predictions, and it will not learn or adapt until it is retrained with a new batch of data.\nWhile batch learning is effective for static datasets, it can be inefficient when new data arrives continuously, as the model requires retraining on the entire dataset.\n\n\nOnline Learning\nOnline learning, also known as incremental learning, allows the model to learn continuously as new data becomes available. Instead of retraining the entire model on the full dataset, online learning updates the model incrementally with each new data point. This approach is useful for real-time applications where data is continuously generated, and the model needs to adapt quickly to new patterns or trends.\n\n\n\n2.4.3 Instance-Based vs. Model-Based Learning\n\nInstance-Based Learning\nInstance-based learning is a method where the algorithm memorizes the training data and makes predictions by comparing new data points to the stored instances. The key idea is that similar inputs will have similar outputs, so the model looks for the most similar previous instances to make a prediction. Instance-based learning does not explicitly build a general model but rather relies on the “memory” of previous examples.\nInstance-based learning is particularly useful in situations where it is difficult to derive a formal model and when the relationships between data points are highly local rather than global.\n\n\nModel-Based Learning\nIn contrast to instance-based learning, model-based learning involves creating a general model that learns the relationships between input features and output predictions. These models generalize from the training data to make predictions on unseen data. Examples of model-based learning include decision trees, linear regression, and neural networks.\nModel-based learning is more efficient than instance-based learning for making predictions on new data since the model has learned the underlying patterns and does not need to search through the entire dataset."
  },
  {
    "objectID": "02_ML.html#challenges-in-machine-learning",
    "href": "02_ML.html#challenges-in-machine-learning",
    "title": "2  Introduction to Machine Learning",
    "section": "2.5 Challenges in Machine Learning",
    "text": "2.5 Challenges in Machine Learning\nWhile machine learning offers powerful tools for analyzing and predicting outcomes, it also presents several challenges that must be addressed to build accurate and effective models. These challenges can arise at various stages of the machine learning process, from data collection and preparation to model training and evaluation. In this section, we explore some of the key challenges in machine learning and how they apply to data.\n\n2.5.1 Sufficient and Representative Training Data\nOne of the most important factors in building a successful machine learning model is having access to sufficient and representative training data. For a model to learn effectively, it must be exposed to a wide variety of examples that reflect the real-world scenarios it will encounter.\nSufficient Data: Having a large enough dataset is essential to capture the complexity of the data. Without enough data, a model may fail to learn the underlying patterns and may perform poorly when applied to new data.\nRepresentative Data: It’s not just the size of the data that matters, but also its diversity. If the dataset is not diverse enough, the model may fail to generalize and perform poorly in real-world scenarios.\n\n\n2.5.2 Poor-Quality Data\nData quality is another significant challenge in machine learning. Data can be noisy, incomplete, or inconsistent, and poor-quality data can significantly hinder a model’s ability to make accurate predictions. This issue is especially common in real-time data collection.\nTo address these issues, it’s important to clean and preprocess the data before training the model. This can involve filling in missing values, correcting errors, and smoothing noisy data.\n\n\n2.5.3 Selecting Relevant Features\nFeature selection—the process of identifying the most important variables or attributes to include in the model—is another critical challenge. Occasionally, the number of potential features can be vast. Determining which features will provide the most valuable information for the model requires careful thought and experimentation.\nUsers often use techniques like feature importance analysis or recursive feature elimination to identify the most relevant features and ensure that only the most informative variables are included in the model.\n\n\n2.5.4 Overfitting and Underfitting\nOverfitting and underfitting are two common pitfalls in machine learning that occur when a model either learns the training data too well or fails to learn enough from it.\nOverfitting occurs when a model learns not only the underlying patterns in the training data but also the noise and random fluctuations. As a result, the model performs exceptionally well on the training data but fails to generalize to new, unseen data.\nUnderfitting, on the other hand, occurs when a model is too simple and fails to capture the underlying patterns in the data. This typically happens when the model is not complex enough to handle the intricacies of the dataset. To strike the right balance between overfitting and underfitting, users must choose models with an appropriate level of complexity and use techniques like hyperparameter tuning and regularization to refine model performance.\n\n\n2.5.5 Data Imbalance\nIn many datasets, certain classes of data may be underrepresented, leading to imbalanced datasets. To address this issue, analysts can use techniques like oversampling the minority class, undersampling the majority class, or employing specialized algorithms designed to handle imbalanced data, such as SMOTE (Synthetic Minority Over-sampling Technique).\n\n\n2.5.6 Real-Time Data Challenges\nProcessing and analyzing real-time data can be challenging, especially when decisions need to be made instantly. Machine learning models must be fast and efficient, capable of processing data and providing actionable insights within seconds. This requires sophisticated infrastructure and algorithms capable of handling large volumes of data quickly and accurately, often under pressure."
  },
  {
    "objectID": "02_ML.html#testing-and-validation",
    "href": "02_ML.html#testing-and-validation",
    "title": "2  Introduction to Machine Learning",
    "section": "2.6 Testing and Validation",
    "text": "2.6 Testing and Validation\nTesting and validation are essential stages in the development of machine learning models, ensuring that the models generalize well to new, unseen data. Without proper testing and validation, it is difficult to assess how well a model will perform in real-world situations, where data can vary significantly from the training set. This section will cover key testing and validation techniques used in predictive analytics, including data splitting, cross-validation, and hyperparameter tuning.\n\n2.6.1 Data Splitting\nA foundational step in model evaluation is splitting the dataset into different subsets: the training set, the validation set, and the test set. Each set plays a crucial role in ensuring that the model performs well on unseen data, preventing issues like overfitting.\n\nTraining Set: The training set is used to fit the machine learning model. It contains both the input data and the corresponding target variable. The model learns from this data by identifying patterns and relationships.\nValidation Set: The validation set is used to tune the model’s parameters and help in model selection. This set allows analysts to evaluate the model’s performance during the training process and make adjustments, such as selecting the best algorithm or optimizing hyperparameters, to improve accuracy.\nTest Set: The test set is used to assess the final model’s performance once it has been trained and tuned. This set is completely separate from the training and validation sets, ensuring that the model is evaluated on new, unseen data.\n\nThe goal of splitting the data is to avoid “data leakage,” where information from the test set accidentally influences the model’s training, leading to overly optimistic results. By ensuring that each subset is used for its intended purpose, analysts can better estimate the model’s true performance.\n\n\n2.6.2 Cross-Validation\nCross-validation is a more advanced technique for assessing a model’s effectiveness, particularly when the dataset is limited in size. The most common form of cross-validation is k-fold cross-validation, where the dataset is randomly divided into k equal-sized parts, or “folds.” The model is then trained on k-1 of these folds and tested on the remaining fold. This process is repeated k times, each time with a different fold held out for testing, and the performance across all folds is averaged.\nBy using multiple validation sets, analysts ensure that the model’s performance is not dependent on any one particular subset of data and can provide a more reliable estimate of its ability to generalize.\nBenefits of Cross-Validation: Cross-validation is particularly useful when there are not enough data points to reliably train and test a model. By using each fold for both training and testing, cross-validation provides a more accurate estimate of a model’s performance across all data points, making the results less sensitive to random fluctuations in the data.\n\n\n2.6.3 Hyperparameter Tuning\nMachine learning models often have parameters, called hyperparameters, that control the learning process. These parameters can significantly influence model performance, and selecting the right values for them is critical for building an effective model.\nFor example, in a decision tree model, hyperparameters such as the maximum depth of the tree or the minimum samples per leaf can be adjusted to control the complexity of the model. In predictive analytics, these hyperparameters might affect how a model predicts outcomes. The goal is to find the best combination of hyperparameters that results in the most accurate predictions on unseen data.\nThere are several techniques for hyperparameter tuning, including:\n\nGrid Search: Grid search involves specifying a range of values for each hyperparameter and exhaustively trying all combinations to find the optimal settings. For example, when training a random forest model to predict performance, a grid search might test different values for the number of trees and the depth of each tree.\nRandom Search: Random search, in contrast, randomly samples hyperparameter combinations from a defined search space. While it may not explore every possible combination, it is often more efficient than grid search, especially when dealing with a large number of hyperparameters.\nBayesian Optimization: Bayesian optimization uses a probabilistic model to predict which combinations of hyperparameters are most likely to yield the best results. This technique is more efficient than grid and random search, as it focuses the search on promising areas of the hyperparameter space.\n\nHyperparameter tuning is a crucial step in improving the accuracy and performance of machine learning models. By experimenting with different settings and evaluating the results on the validation set, analysts can refine their models and ensure that they perform optimally on real-world data.\n\n\n2.6.4 Evaluating Model Performance\nOnce the model has been trained and tuned, it is essential to evaluate its performance using a variety of metrics. The choice of performance metrics depends on the type of model and the problem being solved.\n\nClassification Metrics: For classification tasks, common evaluation metrics include:\n\nAccuracy: The percentage of correct predictions out of all predictions made.\nPrecision: The percentage of true positive predictions among all predicted positives.\nRecall: The percentage of true positive predictions among all actual positives.\nF1-Score: The harmonic mean of precision and recall, providing a single metric that balances the two.\n\nRegression Metrics: For regression tasks, typical evaluation metrics include:\n\nMean Absolute Error (MAE): The average absolute difference between the predicted and actual values.\nMean Squared Error (MSE): The average squared difference between the predicted and actual values, with larger errors penalized more heavily.\nR-squared: A measure of how well the model explains the variance in the data.\n\n\n\n\n2.6.5 Validation Techniques for Time-Series Data\nIn predictive analytics, time-series data is common, especially when predicting future outcomes based on historical data. Standard cross-validation techniques can sometimes be inappropriate for time-series data, as they do not account for the temporal order of observations.\nTo properly evaluate models using time-series data, analysts can employ time-based splitting or forward-chaining techniques:\n\nTime-Based Splitting: The dataset is split based on time, with the training set using past data and the test set using future data. This ensures that the model is tested on data that reflects future events, which is important for forecasting tasks.\nForward-Chaining: In forward-chaining, the training set is progressively expanded to include new data points, while the test set always consists of the most recent data. This simulates the real-world scenario of making predictions as new data arrives."
  },
  {
    "objectID": "02_ML.html#navigating-machine-learning",
    "href": "02_ML.html#navigating-machine-learning",
    "title": "2  Introduction to Machine Learning",
    "section": "2.7 Navigating Machine Learning",
    "text": "2.7 Navigating Machine Learning\nUnderstanding machine learning (ML) involves not only mastering the technical aspects of algorithms and data manipulation but also recognizing when and how to apply these methods effectively. To successfully navigate the complexities of machine learning, it is essential to stay informed about ongoing advancements, collaborate across disciplines, address ethical concerns, and manage the deployment of models.\n\n2.7.1 Staying Updated with Advances\nThe field of machine learning is rapidly evolving, with new algorithms, techniques, and best practices emerging regularly. As data becomes more sophisticated and computational power continues to increase, it’s important for analysts to stay up to date with the latest developments in machine learning. This includes exploring emerging models, such as deep learning, that can process complex.\nTo keep up with advancements in machine learning, analysts should:\n\nFollow academic research and industry publications related to thier area of study.\nParticipate in online courses, workshops, and conferences to learn new techniques and tools.\nExperiment with new libraries and frameworks, such as TensorFlow or PyTorch, that are pushing the boundaries of what’s possible in predictive analytics.\n\nBy staying current with the latest machine learning techniques, analysts can continue to push the boundaries of predictive analytics.\n\n\n2.7.2 Ethical Considerations and Bias Mitigation\nAs with all fields involving data, machine learning in predictive analytics comes with its ethical considerations. It is crucial to ensure that machine learning models are developed and deployed fairly, avoiding unintended consequences like bias, discrimination, or privacy violations.\nFor instance, if a model is trained using biased historical data, such as skewed customer data, the model might make inaccurate predictions that disadvantage customers.\nTo mitigate bias, analysts should:\n\nEnsure that training data is diverse and representative of the scenarios it is meant to model.\nRegularly test models for fairness and adjust them if they show bias against certain groups.\nBe transparent about the data and assumptions used in the model, particularly when these models are used for decision-making.\n\n\n\n2.7.3 Collaboration Across Disciplines\nMachine learning is an interdisciplinary field that requires collaboration between domain experts, data scientists, and engineers. In predictive analytics, this collaboration is essential because machine learning models must be contextualized within the specific domain. For example, while a data scientist may have the technical skills to build a predictive model, a domain expert can provide valuable insights into the nuances of the area of research.\nBy working together, interdisciplinary teams can create more relevant, impactful machine learning models that support better decision-making.\n\n\n2.7.4 Scalability and Deployment\nBuilding machine learning models is only one part of the process. The real challenge lies in deploying these models into real-world settings where they can provide ongoing value. This often means deploying models that can handle real-time data.\nScalability is a key consideration when deploying machine learning models. A model trained on historical data might perform well in a controlled environment, but when deployed in real-time, it must be able to handle streaming data, process new information quickly, and update predictions in near real-time.\nTo ensure successful deployment, analysts should:\n\nWork closely with engineers and software developers to ensure that models can be integrated into existing systems used by coaches and managers.\nTest models in a live setting to ensure that they can handle the volume and velocity of real-time data.\nContinuously monitor model performance after deployment to identify potential issues and make necessary adjustments."
  },
  {
    "objectID": "03_Logistic.html#key-concepts-of-logistic-regression",
    "href": "03_Logistic.html#key-concepts-of-logistic-regression",
    "title": "\n3  Logistic Regression\n",
    "section": "\n3.1 Key Concepts of Logistic Regression:",
    "text": "3.1 Key Concepts of Logistic Regression:\nBinary Outcome: Binary outcomes are central to the concept of logistic regression, a statistical method tailored for scenarios where the outcome variable is dichotomous, meaning it has two possible states such as “yes” or “no,” “success” or “failure,” or is represented numerically as 0 and 1. This characteristic is pivotal for modeling and analyzing situations where the response is bifurcated and does not adhere to a continuous scale. Logistic regression excels in these settings by estimating the probability that a given input set belongs to a particular category (e.g., success or failure).\nExamples of Binary Outcome Applications\n\nMedical Diagnosis: In the medical field, logistic regression can be used to predict the presence or absence of a disease based on various predictors, such as age, sex, blood pressure, cholesterol levels, and other clinical measurements. For instance, it could model the likelihood of a patient having heart disease, where 1 represents the presence of heart disease and 0 represents its absence.\nEmail Spam Filtering: This application involves classifying emails as spam (1) or not spam (0), based on features extracted from the emails, such as the frequency of certain words, the presence of specific phrases, or the sender’s details. Logistic regression can be trained on a dataset of emails that have been manually classified to predict the categorization of new, unseen emails.\nCredit Approval Processes: Financial institutions often use logistic regression to predict the probability of a loan applicant defaulting on a loan. The binary outcome would be default (1) or no default (0), with predictors including credit score, income level, employment history, and other financial metrics.\nCustomer Churn Prediction: Companies use logistic regression to identify the likelihood of a customer discontinuing service or subscription, where 1 indicates churn (the customer leaves) and 0 indicates retention (the customer stays). Predictive factors might include usage patterns, customer service interactions, payment history, and satisfaction levels.\nPolitical Election Outcomes: In the context of binary electoral outcomes (win or lose), logistic regression can analyze polling data to predict whether a candidate will win (1) or lose (0) an election. The model might consider variables such as voter demographics, campaign spending, and historical voting patterns.\n\nThese examples underscore the versatility and practicality of logistic regression in handling binary outcomes across various domains, providing valuable insights and predictions that aid in decision-making processes.\nOdds Ratio\nIn logistic regression, the odds ratio is used to measure the association between a predictor variable and the probability of a particular outcome. Specifically, the odds ratio (OR) quantifies how the odds of the outcome change with a one-unit increase in the predictor variable, assuming all other variables in the model are held constant.\nThe odds of an event is defined as the probability of the event occurring divided by the probability of the event not occurring. Mathematically, if \\(p\\) represents the probability of the event, then the odds are given by \\(\\frac{p}{1-p}\\).\nAn odds ratio of:\n\n\nOR = 1 suggests there is no association between the predictor and the outcome.\n\nOR &gt; 1 indicates that the event is more likely to occur as the predictor increases.\n\nOR &lt; 1 suggests that the event is less likely to occur as the predictor increases.\n\nExamples of Odds Ratio\n\nMedical Research: Suppose a study investigates the effect of a new drug on reducing the risk of a disease. If the odds of the disease in the group receiving the drug are 0.5 and the odds in the placebo group are 1.0, the odds ratio is 0.5. This means the odds of getting the disease are 50% lower in the drug group compared to the placebo group.\nEducational Study: In an educational context, consider a study examining the effect of a special tutoring program on passing an exam. If students in the tutoring program have odds of passing the exam of 3 (i.e., they are three times as likely to pass than fail) and the odds for students not in the program are 1 (equal likelihood of passing and failing), the odds ratio is 3. This suggests that students in the tutoring program are three times as likely to pass the exam compared to those not in the program.\nMarket Research: A market research might explore the influence of customer satisfaction on the likelihood of repeat purchases. If the odds of making a repeat purchase for satisfied customers are 2 (twice as likely to repurchase as not) and for unsatisfied customers the odds are 0.5, the odds ratio would be 4 (2 divided by 0.5). This indicates that satisfied customers are four times as likely to make a repeat purchase compared to unsatisfied customers.\n\nUnderstanding and correctly interpreting the odds ratio in logistic regression allows for meaningful insights into the relationship between variables and outcomes. By quantifying how changes in predictor variables affect the odds of a particular outcome, researchers and practitioners can make informed decisions and predictions across various fields including medicine, education, and business.\nSigmoid Curve\nThe Sigmoid Curve, characterized by its S-shaped curve, is used in logistic regression to model the probability that a given input belongs to a particular category (e.g., success or failure). This curve ranges between 0 and 1, making it an effective tool for representing probabilities in binary outcomes. The formula for the logistic function is:\n\\[\nP(Y=1) = \\frac{1}{1 + e^{-(\\beta_0 +        \\beta_1X)}}\n\\]\nwhere:\n\n\n\\(P(Y=1)\\) is the probability that the dependent variable equals 1 (e.g., success, presence of a condition).\n\n\\(e\\) is the base of the natural logarithm.\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are the coefficients that the model estimates.\n\n\\(X\\) is the independent variable.\n\nThe logistic function essentially transforms linear combinations of predictors into probabilities by taking values from \\(-\\infty\\) to \\(+\\infty\\) and mapping them into a (0, 1) range.\nThe function that links the linear combination to the sigmoid curve is called the link function. There are a number of possible link functions that could be used. The one we will use (and the one presented in the equation above) is called the logit link function.\nEstimation\nIn logistic regression, the estimation of model coefficients is a crucial step that directly influences the accuracy and interpretability of the model’s predictions. The process relies on Maximum Likelihood Estimation (MLE), a statistical method used to estimate the parameters of a statistical model. MLE works by finding the parameter values that maximize the likelihood function, which measures how probable the observed data is, given a set of parameter values.\nThe likelihood function for a logistic regression model is a product of individual probabilities, each representing the likelihood of observing a particular outcome (e.g., success or failure) for a given set of predictor values. The goal of MLE is to adjust the model’s coefficients (parameters) such that this product of probabilities (likelihood) is maximized across all observations in the dataset.\nA practical way to visualize the MLE process is by plotting the likelihood function over a range of values for a model coefficient. Initially, the likelihood might be low for arbitrary coefficient values. However, as the MLE algorithm iterates, adjusting the coefficients to maximize the likelihood, we can observe the likelihood function reaching a peak. This peak represents the maximum likelihood estimates of the model coefficients, providing the best fit to the observed data.\nBy employing MLE, logistic regression models can accurately estimate the probability of binary outcomes based on a set of predictors. This method’s ability to find the most likely parameter values for a given dataset makes it a powerful tool in the hands of data analysts and researchers across various fields.\nInterpretation\nIn logistic regression, the coefficients obtained from the model provide the log odds of the dependent variable being 1 (for binary outcomes) for a one-unit increase in the predictor variable, keeping all other predictors constant. This log odds can be a bit abstract, so it’s common to convert these log odds into odds ratios by exponentiating the coefficients. This makes interpretation more intuitive:\n\nOdds Ratio (OR) &gt; 1: Indicates that the likelihood of the event (outcome=1) increases with a one-unit increase in the predictor.\nOdds Ratio (OR) &lt; 1: Suggests that the likelihood of the event decreases with a one-unit increase in the predictor.\nOdds Ratio (OR) = 1: Means there is no effect of the predictor on the likelihood of the event.\nExample 1: Health Research: Suppose a logistic regression model is developed to study the impact of smoking (predictor variable) on the likelihood of developing heart disease (outcome variable). If the coefficient for smoking is 0.5, exponentiating this gives an odds ratio of exp(0.5) ≈ 1.65. This can be interpreted as smokers having a 65% higher odds of developing heart disease compared to non-smokers, holding all other variables constant.\nExample 2: Marketing Analytics: Consider a model analyzing the impact of online ad exposure (predictor) on the probability of a purchase (outcome). If the coefficient for ad exposure is -0.3, the odds ratio is exp(-0.3) ≈ 0.74. This suggests that with each additional exposure to the ad, the odds of making a purchase decrease by 26%, assuming all other factors remain constant.\nExample 3: Education: A logistic regression could examine the effect of study hours (predictor) on passing an exam (outcome). If the coefficient for study hours is 0.2, the odds ratio is exp(0.2) ≈ 1.22. This means for each additional hour of study, the odds of passing the exam increase by 22%, with other factors held constant.\n\nInterpreting logistic regression coefficients through odds ratios provides valuable insights into the influence of predictors on an outcome. It helps in understanding which factors are more critical and how changes in these factors can increase or decrease the likelihood of the outcome. This makes logistic regression a powerful tool for decision-making in various fields such as health, marketing, and education, allowing practitioners to make informed decisions based on quantifiable evidence."
  },
  {
    "objectID": "03_Logistic.html#implementing-logistic-regression-with-tidymodels",
    "href": "03_Logistic.html#implementing-logistic-regression-with-tidymodels",
    "title": "\n3  Logistic Regression\n",
    "section": "\n3.2 Implementing Logistic Regression with tidymodels",
    "text": "3.2 Implementing Logistic Regression with tidymodels\nWhen implementing logistic regression with tidymodels, you typically follow these steps:\n\n\nData Preprocessing: Before fitting a logistic regression model, data must be cleaned and prepared. tidymodels offers the recipes package for this purpose, enabling feature engineering, normalization, and data splitting. A typical workflow might involve:\n\n\n\nCreating a recipe: Specify preprocessing steps such as normalization, dummy variable creation for categorical variables, and missing data imputation.\n\nData splitting: Use rsample to split the data into training and testing sets, ensuring that the model can be trained on one subset and validated on another to test its generalizability.\n\nExample: In this example, we’ll use the palmerpenguins dataset, specifically aiming to predict the species of penguins based on physical measurements. The dataset includes variables such as bill length, bill depth, flipper length, body mass, and sex.\nLoading the Dataset\nFirst, ensure the palmerpenguins package is installed and loaded, along with tidymodels.\n\nlibrary(palmerpenguins)\nlibrary(tidymodels)\n\nInitial Dataset Examination\nExamine the dataset to understand its structure and identify any preprocessing needs.\n\nglimpse(penguins)\n\nRows: 344\nColumns: 7\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n\n\nWe can see right away that there are some missing values. Let’s remove these observations. We will also make the species binary since we are doing logistic regression and species has three levels in this dataset. We will only model species as chinstrap or other (not chinstrap).\n\npenguins = drop_na(penguins)\npenguins = penguins |&gt; \n  mutate(\n    species = factor(if_else(species==\"Chinstrap\", \"Chinstrap\", \"Other\"))\n  )\nglimpse(penguins)\n\nRows: 333\nColumns: 7\n$ species           &lt;fct&gt; Other, Other, Other, Other, Other, Other, Other, Oth…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n\n\nCreating a Recipe\n\nDefine the outcome variable and predictors.\nSpecify preprocessing steps, such as normalization for numerical variables and encoding for categorical variables.\n\n\npenguin_recipe = recipe(species ~ bill_length_mm + bill_depth_mm +\n                           flipper_length_mm + body_mass_g + sex, \n                         data = penguins) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors())\n\nData Splitting\nSplit the data into training and testing sets to prepare for model training and evaluation.\n\nset.seed(123)\npenguin_split = initial_split(penguins, prop = 0.75)\npenguin_train = training(penguin_split)\npenguin_test = testing(penguin_split)\n\nPreparing Processed Data\nApply the recipe to the training data to prepare it for model fitting.\n\nprepared_data = prep(penguin_recipe, training = penguin_train) |&gt;\n  bake(new_data = NULL)\n\nBy following these steps, you will have effectively prepared your dataset for logistic regression analysis with tidymodels. This preprocessing includes normalization and encoding necessary for the logistic regression model to properly interpret and learn from the data.\n\n\nModel Specification with parsnip: After preparing the dataset, the next step in implementing logistic regression with tidymodels is to specify the model. We use the parsnip package for this purpose, which offers a unified interface for model specification across various modeling techniques, including logistic regression. The logistic_reg() function is utilized to specify a logistic regression model, which can be customized with engine-specific options according to the analysis needs.\n\nExample: Continuing with the palmerpenguins dataset example, we aim to predict the species of a penguin based on measurements like bill length, bill depth, flipper length, body mass, and sex. Given that the species prediction is a classification task, we specify a logistic regression model for binary or multinomial outcomes, depending on the number of species we wish to distinguish between.\nSpecify a logistic regression model using parsnip. For this example, we focus on a binary classification, though the palmerpenguins dataset allows for multinomial classification as well.\n\n# Specify a logistic regression model\npenguin_model = logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  set_mode(\"classification\")\n\n\n\nModel Fitting: Fit the model to the training data. This step involves estimating the model coefficients using the data prepared in the previous steps.\n\n\n# Fit the model to the training data\npenguin_fit = fit(penguin_model, species ~ bill_length_mm + \n                     bill_depth_mm + flipper_length_mm + \n                     body_mass_g + sex, \n                   data = penguin_train)\n\nThis command initiates the fitting process, where the glm engine estimates the coefficients that best predict the penguin species from the specified predictors. The set_mode(\"classification\") indicates that we are dealing with a classification problem, aiming to categorize penguins into different species.\n\n\nSummary and Interpretation: Once the model is fitted, we can summarize its findings to understand the impact of each predictor on the species classification.\n\n\n# Summarize the model fit\ntidy(penguin_fit)\n\n# A tibble: 6 × 5\n  term              estimate std.error statistic p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        73.2     38.0         1.93   0.0538\n2 bill_length_mm     -2.75     1.21       -2.27   0.0231\n3 bill_depth_mm      -1.22     0.718      -1.70   0.0893\n4 flipper_length_mm   0.0622   0.121       0.514  0.607 \n5 body_mass_g         0.0151   0.00721     2.10   0.0359\n6 sexmale             5.99     4.24        1.41   0.158 \n\n\nThe summary output will provide insights into the relationship between the predictors and the probability of a penguin belonging to a specific species. Coefficients with positive values indicate an increase in the likelihood of a penguin belonging to the target species with an increase in the predictor variable, while negative coefficients suggest the opposite.\nWe can obtain the impact on the odds ratio for each coefficient:\n\ntidy(penguin_fit, exponentiate = TRUE)\n\n# A tibble: 6 × 5\n  term              estimate std.error statistic p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)       6.14e+31  38.0         1.93   0.0538\n2 bill_length_mm    6.42e- 2   1.21       -2.27   0.0231\n3 bill_depth_mm     2.95e- 1   0.718      -1.70   0.0893\n4 flipper_length_mm 1.06e+ 0   0.121       0.514  0.607 \n5 body_mass_g       1.02e+ 0   0.00721     2.10   0.0359\n6 sexmale           4.01e+ 2   4.24        1.41   0.158 \n\n\n\n\nModel Evaluation: After fitting our logistic regression model to the penguin dataset, we aim to evaluate its performance to understand how well it predicts penguin species. We will use the yardstick package to compute the accuracy and ROC AUC of our model on the test set, and then generate a confusion matrix for a detailed breakdown of prediction results.\n\n\n# Load necessary library\nlibrary(yardstick)\n\n# Make predictions on the test set\npenguin_pred = predict(penguin_fit, new_data = penguin_test, \n                        type = \"prob\")\npenguin_class = predict(penguin_fit, new_data = penguin_test, \n                        type = \"class\")\npenguin_results = bind_cols(penguin_test, penguin_pred, penguin_class)\n\n# Calculate Accuracy\npenguin_results |&gt;\n  metrics(truth = species, estimate = .pred_class) |&gt;\n  filter(.metric == \"accuracy\")\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.988\n\n#plot ROC\npenguin_results |&gt; \n  roc_curve(truth = species, .pred_Chinstrap) |&gt; \n  autoplot()\n\n\n\n# Calculate ROC AUC\npenguin_results |&gt;\n  roc_auc(truth = species, .pred_Chinstrap)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary             1\n\n# Generate and display a Confusion Matrix\npenguin_conf_matrix = penguin_results |&gt;\n  conf_mat(truth = species, estimate = .pred_class)\n\npenguin_conf_matrix\n\n           Truth\nPrediction  Chinstrap Other\n  Chinstrap        20     0\n  Other             1    63\n\n\nIn this example: - We predict the species of penguins in the test set using our fitted logistic regression model. - We compute the accuracy of our predictions, providing a simple metric that indicates the overall proportion of correct predictions. - We calculate the ROC AUC (Area Under the Receiver Operating Characteristic Curve), which provides a measure of model performance across all classification thresholds. This metric is particularly useful for evaluating binary classification models in scenarios with imbalanced class distributions. - We generate a confusion matrix, offering a detailed view of the model’s predictions, including true positives, false positives, true negatives, and false negatives. This matrix helps us understand the types of errors our model is making.\nThis approach to model evaluation offers a comprehensive view of performance, balancing simplicity with depth of insight, and equips us with the information needed to make informed decisions about potential model improvements.\n\n\nModel Tuning: Model tuning aims to optimize the performance of a logistic regression model by systematically searching for the best hyperparameter values. In this example, we focus on using regularization to prevent overfitting and improve model generalizability. Regularization adds a penalty to the size of coefficients to shrink them towards zero, thus simplifying the model. We will use the glmnet engine, which supports both L1 (lasso) and L2 (ridge) regularization, and the tune package to find the optimal mix of regularization type and strength.\nSpecify the Model with Regularization Parameters\nFirst, we specify a logistic regression model and indicate that the regularization parameters penalty (lambda) and mixture (alpha) are to be tuned.\n\nlibrary(glmnet)\n\nlogistic_spec = logistic_reg(penalty = tune(), mixture = tune()) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\") \n\nDefine the Tuning Grid\nWe define a tuning grid that specifies a range of values for penalty and a set of values for mixture. The penalty controls the strength of the regularization, while the mixture determines the balance between L1 and L2 regularization.\n\ntuning_grid = grid_regular(\n  penalty(range = c(0.001, 1), trans = log10_trans()),\n  mixture( c(0.1, 0.9)),\n  levels = 20\n)\n\nCross-Validation Setup\nWe prepare for cross-validation by splitting the data into training and testing sets and defining the resampling method.\n\nset.seed(123)\ndata_split = initial_split(penguins, prop = 0.75)\ncv_folds = vfold_cv(training(data_split), v = 5)\n\nModel Tuning\nUsing the tune_grid() function, we search over the grid of hyperparameters within the cross-validation framework to find the combination that maximizes the model’s performance, typically using accuracy or ROC AUC as the criterion.\n\ntune_results = tune_grid(\n  object = workflow() |&gt;\n    add_recipe(penguin_recipe) |&gt;\n    add_model(logistic_spec),\n  resamples = cv_folds,\n  grid = tuning_grid,\n  metrics = metric_set(roc_auc, accuracy)\n)\n\nSelect Best Hyperparameters\nAfter the tuning process, we identify the best hyperparameters based on the chosen performance metric.\n\nbest_params = tune_results |&gt; select_best(metric = \"accuracy\")\nbest_params\n\n# A tibble: 1 × 3\n  penalty mixture .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1    1.00     0.1 Preprocessor1_Model001\n\n\nFinalize the Model\nWith the best hyperparameters identified, we finalize the model by fitting it to the full training set using these parameters.\n\nfinal_model = finalize_workflow(\n  workflow() |&gt;\n    add_recipe(penguin_recipe) |&gt;\n    add_model(logistic_spec),\n  best_params\n) |&gt;\n  fit(data = training(data_split))\n\nThis model tuning example demonstrates how to systematically explore the hyperparameter space to find the optimal settings for a logistic regression model, enhancing its predictive performance and generalizability."
  },
  {
    "objectID": "03_Logistic.html#advanced-considerations",
    "href": "03_Logistic.html#advanced-considerations",
    "title": "\n3  Logistic Regression\n",
    "section": "\n3.3 Advanced Considerations",
    "text": "3.3 Advanced Considerations\n\n3.3.1 Feature Interactions\n\n\nIn-depth Exploration: Beyond simply investigating interactions, it’s crucial to understand the nature and extent of the effect these interactions have on the outcome. This involves exploring different combinations of features to see if they produce synergistic (where the combined effect is greater than the sum of individual effects) or antagonistic interactions (where the combined effect is less than expected).\n\nPractical Implementation: Use the step_interact() function in the recipes package to create new interaction terms in your dataset. It’s important to limit the number of interaction terms to avoid overfitting and to focus on interactions that are theoretically plausible or supported by previous research.\n\nVisualization and Interpretation: Visualize these interactions using interaction plots or partial dependence plots to better understand their effects on the predicted outcome. This helps in interpreting the model in the context of these interactions and making more informed decisions.\n\n3.3.2 Model Resampling\n\n\nAdvanced Techniques: Explore beyond basic bootstrapping or k-fold cross-validation by incorporating techniques like stratified sampling or repeated cross-validation, which can provide more robust estimates of model performance, especially in datasets with imbalanced classes or complex structures.\n\nImpact Analysis: Evaluate how different resampling strategies affect model stability and performance. This can be done by comparing metrics across different resampling techniques and noting variations in model performance, which can inform the choice of the most appropriate resampling strategy.\n\nSoftware Implementation: Utilize the rsample package within tidymodels to implement advanced resampling techniques. Functions like vfold_cv() for cross-validation and bootstraps() for bootstrapping are highly customizable and can be adapted to specific requirements.\n\n3.3.3 Post-modeling Analysis\n\n\nDiagnostic Tools: Expand the toolbox for model diagnostics by including techniques like variance inflation factor (VIF) to check for multicollinearity, residual plots for heteroscedasticity, and influence plots to identify outliers or influential observations that could unduly affect the model.\n\nModel Comparison and Selection: Beyond diagnostics, employ model comparison techniques such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) for logistic regression models to select the model that best balances fit and complexity.\n\nSensitivity Analysis: Conduct sensitivity analyses to understand how changes in model inputs or assumptions affect outcomes. This can involve scenarios like changing the threshold for classification, altering the class balance, or simulating variations in predictor variables.\n\n3.3.4 Additional Considerations\n\n\nModel Interpretability: Leverage tools like vip (Variable Importance Plots) and DALEX to enhance model interpretability, providing insights into which features are driving predictions and how changes in feature values affect the predicted outcome.\n\nDeployment and Monitoring: Outline strategies for deploying logistic regression models in real-world applications, including considerations for model updating, monitoring model performance over time, and dealing with concept drift.\n\nEthical Considerations: Address potential ethical issues, such as bias and fairness, in logistic regression models. This includes assessing the model for disparate impact on different groups and taking steps to mitigate any identified biases."
  },
  {
    "objectID": "04_SVM.html#linear-svm-classification",
    "href": "04_SVM.html#linear-svm-classification",
    "title": "\n4  Support Vector Machines\n",
    "section": "\n4.1 Linear SVM Classification",
    "text": "4.1 Linear SVM Classification\nLinear Support Vector Machine (SVM) Classification is a powerful method in machine learning that aims to find the optimal hyperplane which separates different classes in a dataset with the widest possible margin, hence ensuring robust classification boundaries. This section elaborates on the concept using the iris dataset as an example. This famous data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.\nLet’s first only consider the features petal length and petal width. We will also only focus on binary classification. First, let’s consider only the species setosa and versicolor.\n\ndata(iris)\nlibrary(tidyverse)\n# Filter the dataset\niris_filtered = iris |&gt;\n  filter(Species %in% c(\"versicolor\", \"setosa\")) |&gt; \n  mutate(Species = factor(Species))\n\niris_filtered |&gt; \n  ggplot(aes(x = Petal.Length, y = Petal.Width, color = Species))+\n  geom_point()\n\n\n\n\nIn this example, there is a clear separation of the two species in terms of the features petal length and petal width. A linear SVM tries to find a line that will separate the two classes. In this example, there are a lot of lines that would do this. However, if you have a line that is too close to the versicolor class, then when we have new observations that we want to predict, we are more likely to have some that would cross that line and be missclassified as setosa.\nIf we were to move to line too close to the setosa group, then we are more likely to have a new observation that would be missclassified as versicolor. So we need a line that will be somewhere in the “middle”. That is, we want the line that stays as far away from the closest training instances as possible.\nAn SVM classifier can be visualized as creating the broadest possible “street” (denoted by parallel dashed lines) separating the classes, a process known as large margin classification.\n\n\n\n\n\nThere are other lines that would give us this “street” that would separate the classes, however, the margin (width) of this street will be smaller like below.\n\n\n\n\n\nWhen we have a narrower street, then we have a higher chance of missclassifying observations when we have new data. Thus, our goal is to make the street as wide as possible. The middle solid line is then the decision boundary. Any new observation on the side closest to the blue points would be classified as versicolor and any observation on the other side of that line would be classified as setosa.\nNote that incorporating additional training observations away from the “street” does not impact the decision boundary; it is entirely shaped by (or “supported” by) the samples situated on the street’s edge. These particular samples are known as the support vectors.\nSoft Margin Classification\nImagine we have a situation where there is no clear separation in the two classes. Or perhaps we have an outlier like in the example below.\n\n\n\n\n\nWe would not be able to find a “street” that would separate these classes. When we enforce a rule requiring all data points to be off the street and positioned on the correct side, this approach is termed hard margin classification. However, hard margin classification encounters two primary challenges. The first challenge is that it demands the data to be linearly separable for effective application. The second challenge is its vulnerability to outliers, meaning that even a few anomalous data points can significantly impact the classification outcome.\nTo circumvent these problems, it’s beneficial to adopt a more adaptable approach. The goal is to strike an optimal balance between maximizing the width of the street and minimizing margin violations, which occur when observations land in the middle of the street or on the incorrect side. This approach is known as soft margin classification.\nThe balance between the width of the street and the margin violations is governed by the hyperparemeter \\(C\\). If \\(C\\) is to large, the street becomes too narrow but with few margin violations. If \\(C\\) is too small, then the street is too wide which results in a lot of margin violations. We will need to tune this hyperparameter. That is, try different values of \\(C\\) and find which one does the best job at predicting new observations.\n\n4.1.1 Feature Scaling\nIn the context of SVMs, the importance of feature scaling cannot be overstated. SVMs are sensitive to the scale of the features because the aim is to maximize the margin between classes. If one feature dominates because of its larger scale, the SVM might not perform effectively. Hence, preprocessing steps involving normalization or standardization of features are crucial.\nExample: Linear SVM Classification with the Iris Dataset Using tidymodels\nTo illustrate linear SVM classification using the tidymodels framework in R, we will focus on classifying two species of the iris dataset: virginica and versicolor.\nLoading the Dataset and Preprocessing\nFirst, we load the iris dataset and filter it to only include the two species of interest: virginica and versicolor (instead of setosa and versicolor above). We then split the dataset into a training set and a testing set to evaluate the model’s performance. We also will setup the training data to do cross validation to tune the hyperparameter $C4.\n\nlibrary(tidymodels)\nlibrary(tidyverse)\n\ndata(iris)\n# Filter the dataset\niris_filtered = iris |&gt;\n  filter(Species %in% c(\"versicolor\", \"virginica\")) |&gt; \n  mutate(Species = factor(Species))\n\n# Create a data split\nset.seed(123)\ndata_split = initial_split(iris_filtered, prop = 0.75)\ntrain_data = training(data_split)\ntest_data = testing(data_split)\n\ndat_folds = vfold_cv(train_data, v = 5, strata = Species)\n\nBelow is a plot of these species in the training data in terms of petal length and petal width. When we fit this model, we will use all four features (sepal width, sepal length, petal width, and petal length).\n\ntrain_data |&gt; \nggplot(aes(x = Petal.Length,y = Petal.Width, color = Species))+\n  geom_point()\n\n\n\n\nWhen we incorporate all four features, we can do a better job at predicting, however, it becomes difficult to visualize what the “street” look like. Instead of lines, the street is made up of hyperplanes which we cannot visualize.\nSpecifying the Model and Preprocessing Steps\nWe specify a linear SVM model using svm_linear() from the parsnip package, which is part of the tidymodels framework. The \\(C\\) hyperparameter is called cost in the svm_linear function. We will tell the model that we want to tune this hyperparameter. Additionally, we define preprocessing steps, including feature scaling, to ensure that all features contribute equally to the model.\n\n# Specify the model\nsvm_model = svm_linear(cost = tune()) |&gt;\n  set_engine(\"kernlab\") |&gt;\n  set_mode(\"classification\")\n\n# Preprocessing\nrecipe = recipe(Species ~ ., data = train_data) |&gt;\n  step_normalize(all_predictors())\n\nTune the Model\nWe will setup a grid of possible values of \\(C\\) to try. The function grid_regular checks from -10 to 5 on the log base 2 scale. That is, it checks from \\(2^{-10}=0.00098\\) to \\(2^5 = 32\\). The values of levels tells the function how many equally spaced values on this interval to check.\nWe then pass this tuning grid along with the 5-folded data to a workflow that includes the recipe and the model. We will find the value of \\(C\\) that maximizes accuracy\n\n#setup the possible values of C to check\ntuning_grid = grid_regular(\n  cost(),\n  levels = 20\n)\n\ntune_results = tune_grid(\n  object = workflow() |&gt;\n    add_recipe(recipe) |&gt;\n    add_model(svm_model),\n  resamples = dat_folds,\n  grid = tuning_grid\n)\n\nbest_params = select_best(tune_results, metric = \"accuracy\")\nbest_params\n\n# A tibble: 1 × 2\n   cost .config              \n  &lt;dbl&gt; &lt;chr&gt;                \n1 0.402 Preprocessor1_Model12\n\n\nTraining the Model\nWith the model and preprocessing steps specified, we can now train the SVM model on the training data.\n\n# Workflow\nfitted_model = finalize_workflow(\n  workflow() |&gt;\n    add_recipe(recipe) |&gt;\n    add_model(svm_model),\n  best_params\n) |&gt;\n  fit(data = train_data)\n\n Setting default kernel parameters  \n\n\nEvaluating the Model\nFinally, we evaluate the model’s performance on the testing set to understand its effectiveness in classifying the two species.\n\n# Predictions\npredictions = predict(fitted_model, new_data = test_data) |&gt;\n  bind_cols(test_data)\n\n# Evaluate\nmetrics = metrics(predictions, truth = Species, estimate = .pred_class)\nconf_mat = conf_mat(predictions, truth = Species, estimate = .pred_class)\n\nprint(metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.96 \n2 kap      binary         0.920\n\nprint(conf_mat)\n\n            Truth\nPrediction   versicolor virginica\n  versicolor         12         0\n  virginica           1        12\n\n\nThis example demonstrates how to implement linear SVM classification using the tidymodels framework in R, focusing on the nuances of dealing with a subset of the iris dataset. Through this example, we see the robustness of SVM in creating a model that effectively separates the two chosen species based on their feature measurements, underscoring the importance of feature scaling and model specification in the process."
  },
  {
    "objectID": "04_SVM.html#nonlinear-svm-classification",
    "href": "04_SVM.html#nonlinear-svm-classification",
    "title": "\n4  Support Vector Machines\n",
    "section": "\n4.2 Nonlinear SVM Classification",
    "text": "4.2 Nonlinear SVM Classification\nFor datasets that are not linearly separable, SVMs can be extended to perform nonlinear classification. By adding polynomial features or using similarity features, datasets can often be transformed into a linearly separable form. We will discuss methods to handle nonlinear datasets, including using polynomial features and the kernel trick, which allows for operating in a high-dimensional space without explicitly computing the coordinates of the data points in that space.\n\n4.2.1 Introduction to Kernel Trick\nThe kernel trick is central to enabling SVMs to perform complex nonlinear classifications. This technique involves mapping input features into high-dimensional spaces where the data points that are not linearly separable in the original space might become linearly separable. Crucially, the kernel trick does this mapping without explicitly computing the coordinates in the high-dimensional space, thereby avoiding the computational complexity that such calculations would entail. Common kernels include:\n\n\nPolynomial Kernel: Adds polynomial features of a given degree. This kernel is powerful for capturing the interaction between features up to a specific degree.\n\nRadial Basis Function (RBF) or Gaussian Kernel: Considers all possible polynomials of all degrees, giving more weight to the features that are closer to the target. This kernel is particularly effective for cases where the relationship between the class boundaries is not only nonlinear but also varies in complexity across the dataset.\n\nSigmoid Kernel: Mirrors the use of sigmoid functions in neural networks and can transform the feature space in ways that are beneficial for certain types of datasets.\n\nAdvantages of Kernel SVM\nThe primary advantage of using kernel SVMs lies in their flexibility and ability to handle real-world data that often display complex patterns and nonlinear relationships. Kernel SVMs can capture intricate structures without requiring a massive increase in computational resources typically associated with high-dimensional space mapping.\nChoosing the Right Kernel\nThe choice of kernel significantly affects the model’s performance. No single kernel universally outperforms others across all tasks; the decision is highly data-dependent. Cross-validation can help in selecting the best kernel and its parameters for a given dataset.\n\n\nPolynomial kernels are suitable when the relationship between variables is expected to be polynomial.\n\nRBF kernels are a good default when there is little prior knowledge about the data.\n\nSigmoid kernels can be useful but are less commonly used than RBF or polynomial kernels.\n\n4.2.2 Parameter Tuning in Kernel SVM\nTwo critical parameters in kernel SVMs are \\(C\\) (the regularization parameter) and the kernel-specific parameter (like degree in polynomial kernels or \\(\\gamma\\) in RBF kernels). The parameter \\(C\\) controls the trade-off between achieving a low training error and a low testing error (generalization), whereas kernel-specific parameters control the shape of the boundary.\nBelow are examples of different hyperparameter values using RBF kernels.\n\n\n4.2.3 Implementing Nonlinear SVM\nWhen implementing nonlinear SVMs, the process typically involves several key steps:\n\nFeature Preprocessing: Scaling features to a similar scale is crucial because kernel SVMs are sensitive to the feature scales.\nModel Selection: Choosing between different SVM kernels based on the problem at hand and the nature of the data.\nCross-Validation: Employing cross-validation techniques to fine-tune hyperparameters (such as \\(C\\), kernel parameters like degree for polynomial kernel, or \\(\\gamma\\) for the RBF kernel) is essential for balancing the model’s complexity with its ability to generalize to unseen data.\nEvaluation: Assessing the model’s performance using appropriate metrics (like accuracy, precision, recall, F1 score for classification tasks) on a validation set not seen by the model during the training phase.\nApplication: Once tuned and evaluated, the model can be applied to new, unseen data for prediction tasks.\n\nChallenges and Considerations\nWhile nonlinear SVMs are powerful, they come with their own set of challenges and considerations:\n\n\nComputation Cost: The computational complexity can be higher than for linear models, especially for large datasets and complex kernels.\n\nModel Interpretability: The decision boundaries created by nonlinear SVMs can be difficult to interpret compared to linear SVMs.\n\nOverfitting Risk: There is a higher risk of overfitting, especially with very flexible models like those using high-degree polynomial kernels or small \\(\\gamma\\) values in RBF kernels. Regularization and proper parameter tuning are vital to mitigate this risk."
  },
  {
    "objectID": "04_SVM.html#svm-regression",
    "href": "04_SVM.html#svm-regression",
    "title": "\n4  Support Vector Machines\n",
    "section": "\n4.3 SVM Regression",
    "text": "4.3 SVM Regression\nSVMs can also be applied to regression problems by reversing the objective: instead of trying to maximize the margin while keeping the instances outside the margin, SVM regression attempts to fit as many instances as possible within the margin while limiting margin violations. This approach is known as (\\(\\epsilon\\))-insensitive loss, where the model tries to find a line that captures as many instances as possible within a specified margin.\nHere’s an example demonstrating how to use SVM Regression with the tidymodels framework in R, specifically applied to the diamonds dataset. We’ll predict the price of diamonds based on their features such as carat, cut, color, and clarity.\nPrepare the Data\nThe diamonds dataset is available in the ggplot2 package. We’ll use a subset of the dataset to make the training process faster for this example.\n\ndata(\"diamonds\", package = \"ggplot2\")\nset.seed(123)\ndiamonds_sample = diamonds |&gt; sample_n(size = 2000)\n\n# Split the data into training and testing sets\nsplit = initial_split(diamonds_sample, prop = 0.75)\ntrain_data = training(split)\ntest_data = testing(split)\n\nDefine the Recipe\n\nrec = recipe(price ~ carat + cut + color + clarity, data = train_data) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_normalize(all_numeric_predictors())\n\nSpecify the Model\nWe’ll use an SVM model for regression. The svm_rbf() function from the parsnip package is suitable for this task. The RBF (Radial Basis Function) kernel is commonly used for non-linear regression problems.\n\nsvm_mod = svm_rbf(cost = tune(), rbf_sigma = tune()) |&gt;\n  set_engine(\"kernlab\") |&gt;\n  set_mode(\"regression\")\n\nTune the Model\nWe’ll use cross-validation to tune the hyperparameters of the model.\n\n# Create a 5-fold cross-validation object\nfolds = vfold_cv(train_data, v = 5)\n\n# Create a grid for tuning\ngrid = grid_regular(cost(range = c(-5, 2)),\n                     rbf_sigma(range = c(-5, 2)),\n                     levels = 5)\n\n# Tune the model\ntune_res = tune_grid(\n  object = workflow() |&gt;\n        add_recipe(rec) |&gt;\n        add_model(svm_mod),\n  resamples = folds,\n  grid = grid\n)\n\n# Select the best hyperparameters\nbest_params = select_best(tune_res, metric=\"rmse\")\nbest_params\n\n# A tibble: 1 × 3\n   cost rbf_sigma .config              \n  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                \n1     4    0.0316 Preprocessor1_Model15\n\n\nFit the Final Model\nAfter finding the best hyperparameters, we fit the final model to the training data.\n\nfinal_svm = finalize_workflow(\n  workflow() |&gt;\n    add_recipe(rec) |&gt;\n    add_model(svm_mod),\n  best_params\n) |&gt;\n  fit(data = train_data)\n\nEvaluate the Model\nFinally, we evaluate the model’s performance on the testing set.\n\npredictions = predict(final_svm, new_data = test_data) |&gt;\n  bind_cols(test_data)\n\nmetrics = metrics(predictions, truth = price, estimate = .pred)\nmetrics\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     780.   \n2 rsq     standard       0.962\n3 mae     standard     491."
  },
  {
    "objectID": "05_Decision.html#introduction",
    "href": "05_Decision.html#introduction",
    "title": "\n5  Decision Trees\n",
    "section": "\n5.1 Introduction",
    "text": "5.1 Introduction\nDecision trees are a fundamental machine learning technique used for both classification and regression tasks. At their core, decision trees model decisions and their possible consequences, including chance event outcomes, resource costs, and utility. They are a non-parametric supervised learning method used for classifying data points (in classification trees) and predicting continuous values (in regression trees).\nHistorical Context and Evolution: Decision trees have been a part of the computational and statistical landscape since the 1960s, evolving from their initial use in decision analysis and research into an indispensable tool in the machine learning toolkit. The development of algorithms such as ID3 by Ross Quinlan in the 1980s and its successors, C4.5 and CART (Classification and Regression Trees), marked significant milestones in making decision trees more efficient, accurate, and applicable to a broader range of data types and machine learning problems.\nIntuition Behind Decision Trees: At the heart of decision trees is a simple yet powerful idea—mimicking human decision-making processes through a structured, hierarchical approach. By breaking down complex decisions into a series of simpler choices, each based on specific attributes or features of the data, decision trees can navigate the intricacies of real-world data and arrive at predictions or classifications. This intuitive approach to problem-solving, mirroring the “if-then-else” decision-making logic, makes decision trees both accessible and powerful for addressing diverse analytical challenges.\nSignificance in Machine Learning and Artificial Intelligence: Decision trees serve as foundational building blocks for more complex models, such as random forests and gradient boosting machines, highlighting their importance not only as standalone models but also as components in ensemble methods. Their ability to handle both numerical and categorical data, accommodate non-linear relationships without assuming data distribution, and provide interpretable models that can be easily visualized and understood, underscores their versatility and broad applicability. Furthermore, the role of decision trees in feature importance analysis, where they help in identifying the most significant predictors in a dataset, showcases their utility in exploratory data analysis and model refinement processes."
  },
  {
    "objectID": "05_Decision.html#structure-of-decision-trees",
    "href": "05_Decision.html#structure-of-decision-trees",
    "title": "\n5  Decision Trees\n",
    "section": "\n5.2 Structure of Decision Trees",
    "text": "5.2 Structure of Decision Trees\nDecision trees are graphical representations of decision-making processes. They consist of nodes and branches, where each node represents a decision point or an outcome, and branches represent the choices leading to those decisions or outcomes. The structure can be divided into three main components:\n1. Root Node:\nThis is the starting point of the tree, where the entire dataset is considered before any splits are made. The root node identifies the feature that provides the most significant information gain or the best split based on a specific criterion, like Gini impurity or entropy for classification trees, and variance reduction for regression trees.\nGini Impurity\nGini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. The Gini impurity of a dataset is minimal (zero) when all the elements belong to a single class, indicating perfect homogeneity.\nThe Gini impurity of a node can be calculated using the formula:\n\\[\nGini(t) = 1 - \\sum_{i=1}^{J} p_i^2\n\\]\nwhere \\(p_i\\) is the proportion of the samples that belong to class \\(i\\) at a given node \\(t\\), and \\(J\\) is the number of classes. The goal in splitting a node is to achieve the lowest possible Gini impurity in the resulting child nodes.\nExample: Iris dataset\nThe Iris dataset consists of 150 observations of iris flowers, each with 4 features (sepal length, sepal width, petal length, and petal width) and classified into one of three species (setosa, versicolor, virginica).\nImagine we have a subset of the Iris dataset with 10 observations:\n\n6 of Iris setosa\n3 of Iris versicolor\n1 of Iris virginica\n\nFor our subset:\n\nThe proportion of Iris setosa (\\(p_{setosa}\\)) = 6/10 = 0.6\nThe proportion of Iris versicolor (\\(p_{versicolor}\\)) = 3/10 = 0.3\nThe proportion of Iris virginica (\\(p_{virginica}\\)) = 1/10 = 0.1\n\nPlugging these into the formula gives us:\n\\[\nGini(t) = 1 - (0.6^2 + 0.3^2 + 0.1^2) = 1 - (0.36 + 0.09 + 0.01) = 1 - 0.46 = 0.54\n\\]\nInterpretation\nThe Gini impurity of 0.54 indicates a moderate level of impurity or mixture of classes within this subset of the Iris dataset. A Gini impurity of 0 would mean the subset is perfectly homogeneous (all observations belong to a single class), while a Gini impurity of 0.5 (for a binary classification) or higher in multi-class scenarios like ours suggests a higher level of mixture among classes.\nIn the context of building a decision tree, the goal would be to find the feature and split point that decrease the Gini impurity the most for the resulting subsets. For example, if splitting by petal length at a certain threshold results in two groups—one with mainly Iris setosa and another with a mixture of Iris versicolor and Iris virginica—but both with lower Gini impurity scores than 0.54, that split improves the homogeneity of our subsets with respect to the target variable (species).\nEntropy\nEntropy, a concept borrowed from information theory, measures the level of uncertainty or disorder within a dataset. In the context of decision trees, entropy can be used to quantify the impurity or randomness in the dataset’s class distribution at a node. A dataset with elements from only one class has zero entropy (no disorder), while a dataset evenly split between two or more classes has higher entropy.\nThe entropy of a node is given by the formula:\n\\[\nEntropy(t) = -\\sum_{i=1}^{J} p_i \\log_2(p_i)\n\\]\nHere, \\(p_i\\) represents the proportion of the samples belonging to class \\(i\\) at node \\(t\\), and \\(J\\) is the total number of classes. The decision to split at a particular node aims to produce subsets with lower entropy compared to the parent node.\nComparison and Usage in Decision Trees\nBoth Gini impurity and entropy aim to measure the homogeneity of a dataset; they are just different mathematical methods for doing so. The choice between using Gini impurity or entropy for building a decision tree often depends on the specific dataset and problem context, though in practice, the difference in the trees they produce is often quite small.\n\n\nGini Impurity is faster to compute, as it doesn’t involve logarithmic functions, which makes it a good default choice for decision tree learning algorithms.\n\nEntropy might produce slightly more balanced trees because it tends to penalize non-uniform class distributions more heavily than Gini impurity.\n2. Internal Nodes:\nInternal nodes play a crucial role in the structure and functioning of decision trees, acting as decision points that guide the splitting of the dataset into increasingly homogeneous subsets based on the values of different features. Each internal node represents a “test” or “question” applied to a particular attribute or feature, determining the path down the tree that an observation will follow.\nCharacteristics of Internal Nodes\n\nFeature Selection: Each internal node tests a specific attribute or feature of the data. The choice of which feature to test at each node is determined by a criterion that aims to maximize the homogeneity of the subsets created by the split. For classification tasks, this criterion is often Gini impurity or entropy, whereas for regression tasks, variance reduction is commonly used.\nDecision Point: An internal node divides the dataset into two or more paths, typically representing different outcomes of the test applied. In binary decision trees, which are the most common, each internal node has exactly two branches, but multiway splits are also possible in other types of decision trees.\nRecursive Splitting: After a dataset is split at an internal node, the process of feature selection and splitting is applied recursively to each child subset. This process continues until a stopping criterion is met, such as reaching a maximum tree depth, achieving a subset size smaller than a predefined threshold, or when no further improvement in homogeneity can be achieved.\n\nProcess of Creating Internal Nodes\nThe process of creating internal nodes involves several key steps:\n\nFeature Selection and Split Criterion: At each step, the decision tree algorithm evaluates each feature to determine which one to use for splitting the dataset at the current node. This involves calculating the chosen metric (e.g., Gini impurity, entropy, or variance reduction) for every possible split and selecting the feature and split point that result in the highest gain (i.e., the greatest reduction in impurity or variance).\nDataset Splitting: Once the best feature and split point are identified, the dataset is divided into two or more subsets according to the outcomes of the test at the internal node. Each subset corresponds to a branch leading to a child node.\nRecursive Application: The algorithm then recursively applies the same process of feature selection and dataset splitting to each child subset, creating further internal nodes and branches, until stopping criteria are met.\n\nExample: The Iris Dataset\nConsider a decision tree being trained on the Iris dataset. An internal node might test the petal length of an iris flower. Suppose the best split identified at this node is to divide the dataset into flowers with a petal length less than 2.5 cm and those with a petal length greater than or equal to 2.5 cm. This test on petal length is the “question” posed by the internal node, and it effectively separates Iris setosa (which typically has shorter petals) from Iris versicolor and Iris virginica (which have longer petals). Each of the resulting subsets is then passed to child nodes where further tests (internal nodes) are applied based on other features, such as petal width or sepal length, following the same process of maximizing homogeneity within the subsets.\nInternal nodes are thus the mechanism by which decision trees make sequential decisions, leading to the classification or regression predictions at the leaf nodes. Their creation and organization within the tree are fundamental to the model’s ability to accurately represent and predict the underlying relationships in the data.\n3. Leaf Nodes: Leaf nodes in decision trees are the terminal points where decisions are made or values are predicted, concluding the path from the root through the internal nodes based on the attributes of the data being analyzed. These nodes do not split any further and represent the final output of the decision-making process within the tree structure. In the context of decision trees, there are two primary types of tasks they can be used for, each defining what a leaf node represents:\n\nClassification Trees: In classification tasks, each leaf node is associated with a class label. The path from the root to a leaf node corresponds to a series of decisions based on the attributes of the dataset that leads to a classification decision. For example, in a decision tree designed to classify emails as “spam” or “not spam”, a leaf node would represent one of these two categories. The classification at a leaf is determined by the majority class of the samples that end up in the leaf after applying the decision rules encoded in the tree’s structure. The decision-making process is straightforward; starting from the root, the tree evaluates the attributes of the instance at each internal node, making a decision at each step on which branch to follow, until it reaches a leaf node that provides the final classification.\nRegression Trees: For regression tasks, each leaf node predicts a continuous value. Instead of leading to a categorical outcome, the path through the tree culminates in a prediction of a numeric quantity. This could be, for instance, the predicted price of a house based on features like its size, age, and location. In regression trees, the value assigned to a leaf node is usually the mean or median of the values of the instances that fall into that leaf, providing a prediction for new instances that reach the leaf.\n\nThe Role and Importance of Leaf Nodes:\n\n\nFinal Output: Leaf nodes represent the decision or prediction outcome of the tree, making them critically important to the tree’s overall purpose. They are the actionable results derived from the series of tests conducted at internal nodes.\n\nInterpretability: The path to a leaf node, consisting of a series of decisions based on clear criteria, contributes to the interpretability of decision trees. One can easily trace back the decisions made by the tree to understand why a particular prediction or classification was made.\n\nSimplicity: Despite their simplicity, leaf nodes encapsulate the complex decision-making process of a decision tree, making complex data relationships understandable through a series of binary decisions.\n\nModel Performance: The creation of leaf nodes is directly influenced by the criteria used to split data at internal nodes (e.g., Gini impurity, entropy, variance reduction). These criteria aim to maximize the homogeneity of the data points within each leaf, which is crucial for the model’s accuracy and ability to generalize."
  },
  {
    "objectID": "05_Decision.html#example-using-tidymodels",
    "href": "05_Decision.html#example-using-tidymodels",
    "title": "\n5  Decision Trees\n",
    "section": "\n5.3 Example Using tidymodels\n",
    "text": "5.3 Example Using tidymodels\n\nTo illustrate the use of decision trees with tidymodels in R, let’s consider a simple example where we predict the species of the iris flower based on its measurements. This example involves creating a decision tree classification model.\nSetting Up the Environment\nFirst, ensure you have tidymodels and rpart packages installed. The rpart package is used for creating decision trees.\n\nlibrary(tidymodels)\nlibrary(rpart)\n\nPreparing the Data\n\ndata(iris)\nset.seed(123) # For reproducibility\n\n# Split the data into training and testing sets\niris_split = initial_split(iris, prop = 0.75)\niris_train = training(iris_split)\niris_test = testing(iris_split)\n\n#setup data for cross validation to tune the model\ndat_folds = vfold_cv(iris_train, v = 5, strata = Species)\n\nCreating the Model and Recipe\nNote that there are two hyperparameters we need to determine for decision trees. They are:\n\n\ntree_depth which is the maximum number of node levels the tree can have\n\nmin_n1 which is the minimum number of observations in a node that are required for the node to be split further.\n\nWe will fine tune these hyperparameters using cross validation.\n\n# Define the model specification\ndt_model = decision_tree(tree_depth = tune(), \n                          min_n = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\n\nrecipe = recipe(Species ~ ., data = iris_train)\n\nFine Tune the Model\n\n#setup the possible values of the hyperparameters.\n#by default, tree_depth checks values from 1 to 15\n#min_n defaults to values between, 2 and 40\ntuning_grid = grid_regular(\n  tree_depth(),\n  min_n(),\n  levels = 10\n)\n\n\n#fine tune the model\ntune_results = tune_grid(\n  object = workflow() |&gt;\n    add_recipe(recipe) |&gt;\n    add_model(dt_model),\n  resamples = dat_folds,\n  grid = tuning_grid,\n  metrics = metric_set(accuracy)\n)\n\nbest_params = select_best(tune_results, metric = \"accuracy\")\nbest_params\n\n# A tibble: 1 × 3\n  tree_depth min_n .config               \n       &lt;int&gt; &lt;int&gt; &lt;chr&gt;                 \n1          4     2 Preprocessor1_Model003\n\n\nFit the final model\n\nfitted_model = finalize_workflow(\n      workflow() |&gt;\n        add_recipe(recipe) |&gt;\n        add_model(dt_model),\n      best_params\n    ) |&gt;\n      fit(data = iris_train)\n\nLet’s first examine how well we fit the training data.\n\npred_train = predict(fitted_model, new_data = iris_train) |&gt;\n  bind_cols(iris_train)\ntrain_metrics = metrics(pred_train, truth = Species, estimate = .pred_class)\ntrain_conf_mat = conf_mat(pred_train, truth = Species, estimate = .pred_class)\n\nprint(train_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.991\n2 kap      multiclass     0.987\n\nprint(train_conf_mat)\n\n            Truth\nPrediction   setosa versicolor virginica\n  setosa         38          0         0\n  versicolor      0         32         0\n  virginica       0          1        41\n\n\nWe can visualize the decision tree using the rpart.plot library.\n\nlibrary(rpart.plot)\ntree_fit = fitted_model |&gt; \n  extract_fit_parsnip()\nrpart.plot(tree_fit$fit)\n\n\n\n\nPredicting and Evaluating\nLet’s now predict the testing data.\n\n# Make predictions\npredictions = predict(fitted_model, iris_test) |&gt;\n  bind_cols(iris_test)\n\n# Evaluate the model\ntest_metrics = metrics(predictions, truth = Species, estimate = .pred_class)\ntest_conf_mat = conf_mat(predictions, truth = Species, estimate = .pred_class)\n\nprint(test_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.947\n2 kap      multiclass     0.920\n\nprint(test_conf_mat)\n\n            Truth\nPrediction   setosa versicolor virginica\n  setosa         12          0         0\n  versicolor      0         15         0\n  virginica       0          2         9\n\n\nNote that the accuracy is slightly below the training data accuracy."
  },
  {
    "objectID": "05_Decision.html#decision-trees-for-regression",
    "href": "05_Decision.html#decision-trees-for-regression",
    "title": "\n5  Decision Trees\n",
    "section": "\n5.4 4. Decision Trees for Regression",
    "text": "5.4 4. Decision Trees for Regression\nRegression trees are used when the target variable is continuous. Instead of predicting a class for each instance, regression trees predict a real number. For example, a regression tree might predict the price of a house based on features such as the number of bedrooms, the house’s age, and its location.\nExample: Diamonds dataset\n\nlibrary(tidymodels)\nlibrary(ggplot2) # For the diamonds dataset\n\n# Load the diamonds dataset\ndata(diamonds)\n\n# Sample for a manageable size\nset.seed(123)\ndiamonds_sample = diamonds |&gt; sample_n(2000)\n\n# Initial data split\nsplit = initial_split(diamonds_sample, prop = 0.75)\ntrain_data = training(split)\ntest_data = testing(split)\n\n#setup data for cross validation to tune the model\ndat_folds = vfold_cv(train_data, v = 5)\n\n# Create a recipe\nrecipe = recipe(price ~ ., data = train_data) |&gt;\n  step_dummy(all_nominal(), -all_outcomes())\n\n#setup model\ndt_model = decision_tree(tree_depth = tune(), \n                          min_n = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n#setup tuning grid\ntuning_grid = grid_regular(\n  tree_depth(),\n  min_n(),\n  levels = 10\n)\n\n\n#fine tune the model\ntune_results = tune_grid(\n  object = workflow() |&gt;\n    add_recipe(recipe) |&gt;\n    add_model(dt_model),\n  resamples = dat_folds,\n  grid = tuning_grid,\n  metrics = metric_set(rmse)\n)\n\n#get the best hyperparameters\nbest_params = select_best(tune_results, metric = \"rmse\")\nbest_params\n\n# A tibble: 1 × 3\n  tree_depth min_n .config               \n       &lt;int&gt; &lt;int&gt; &lt;chr&gt;                 \n1          5     2 Preprocessor1_Model004\n\n#fit the final model\nfitted_model = finalize_workflow(\n  workflow() |&gt;\n    add_recipe(recipe) |&gt;\n    add_model(dt_model),\n  best_params\n) |&gt;\n  fit(data = train_data)\n\n\n#see how well the model fit the training data\npred_train = predict(fitted_model, new_data = train_data) |&gt;\n  bind_cols(train_data)\ntrain_metrics = metrics(pred_train, truth = price, estimate = .pred)\nprint(train_metrics)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard    1222.   \n2 rsq     standard       0.905\n3 mae     standard     802.   \n\n#visualize the tree\nlibrary(rpart.plot)\ntree_fit = fitted_model |&gt;\nextract_fit_parsnip()\nrpart.plot(tree_fit$fit)\n\n\n\n# Make predictions and gauge performance on test data\npredictions = predict(fitted_model, test_data) |&gt;\n  bind_cols(test_data)\ntest_metrics = metrics(predictions, truth = price, estimate = .pred)\nprint(test_metrics)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard    1412.   \n2 rsq     standard       0.876\n3 mae     standard     916."
  },
  {
    "objectID": "05_Decision.html#advantages-and-disadvantages",
    "href": "05_Decision.html#advantages-and-disadvantages",
    "title": "\n5  Decision Trees\n",
    "section": "\n5.5 Advantages and Disadvantages",
    "text": "5.5 Advantages and Disadvantages\nDecision trees offer several advantages. First, they are simple and interpretable—trees can be visualized and easily understood, even by non-experts, making them especially useful for decision support. They are also flexible, as they can handle both numerical and categorical data. Additionally, their non-parametric nature means they do not assume any specific distribution of the data, making them well-suited for capturing non-linear relationships.\nHowever, decision trees also have notable disadvantages. They are prone to overfitting; without proper pruning, they can become overly complex and fail to generalize to new data. They can also be unstable, as small changes in the dataset can result in significantly different trees. Finally, because the algorithm used to build decision trees is greedy—making the best local choice at each step—it may miss the globally optimal solution."
  },
  {
    "objectID": "06_Ensemble.html#introduction",
    "href": "06_Ensemble.html#introduction",
    "title": "\n6  Ensemble Learning\n",
    "section": "\n6.1 Introduction",
    "text": "6.1 Introduction\nEnsemble learning is a powerful technique in predictive analytics and machine learning that combines multiple models to improve the accuracy and robustness of predictions. By leveraging the strengths and compensating for the weaknesses of individual models, ensemble methods can achieve higher performance than any single model could on its own.\nEnsemble learning stands out as a meta-algorithmic framework that strategically combines multiple predictive models to construct a more accurate and stable composite model. This approach has proven to be a game-changer in enhancing the performance of machine learning tasks, effectively addressing complex challenges such as overfitting, variance, and bias.\nThe concept of ensemble learning is not entirely new; its roots can be traced back to the late 20th century when the foundational principles of aggregating different models to improve overall performance were first explored. Since then, the field has seen substantial growth, driven by both theoretical advancements and practical successes in various domains, including but not limited to finance, healthcare, and image recognition."
  },
  {
    "objectID": "06_Ensemble.html#bias-variance-trade-off",
    "href": "06_Ensemble.html#bias-variance-trade-off",
    "title": "\n6  Ensemble Learning\n",
    "section": "\n6.2 Bias-Variance Trade-off",
    "text": "6.2 Bias-Variance Trade-off\nThe bias-variance trade-off is a fundamental concept in machine learning that describes the tension between the error introduced by the model’s assumptions about the underlying data structure (bias) and the error introduced by sensitivity to fluctuations in the training set (variance). Understanding this trade-off is crucial for developing models that generalize well to unseen data.\nBias\nBias refers to the error due to overly simplistic assumptions in the learning algorithm. High bias can cause the model to miss relevant relations between features and target outputs (underfitting), leading to poor performance on both the training and test data.\nVariance\nVariance refers to the error due to too much complexity in the learning algorithm. High variance can cause the model to model the random noise in the training data (overfitting), leading to poor generalization to new data.\nTrade-off\nIdeally, one wants to choose a model that both accurately captures the regularities in its training data and generalizes well to unseen data. However, in practice, there is often a trade-off between bias and variance. Increasing a model’s complexity will typically decrease bias but increase variance. Conversely, reducing a model’s complexity increases its bias but decreases its variance. The goal is to find the right balance between bias and variance, minimizing the total error.\nExample\nLet’s illustrate the bias-variance trade-off with a simple example in R. We will use polynomial regression to fit a model to synthetic data. This example demonstrates how increasing the polynomial degree (model complexity) affects the bias and variance of the model predictions.\nFirst, we’ll generate some synthetic data that follows a sine curve with some added noise:\n\nlibrary(tidyverse)\nset.seed(42) # For reproducibility\nx = seq(0, 2*pi, length.out=50)\ny = sin(x) + rnorm(50, sd=0.2) # Sine wave with noise\n\ndat = data_frame(x, y)\n\ndat |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point() +\n  ggtitle(\"Synthetic Sine Data with Noise\")\n\n\n\n\nNext, we’ll fit polynomial regression models of varying degrees to this data and visualize how model complexity influences the fit:\n\n# Function to fit and predict using polynomial regression\nnewdat = data_frame(x=seq(0, 2*pi, length.out=100))\npredict_poly = function(degree) {\n  model = lm(y ~ poly(x, degree, \n                       raw=TRUE), data = dat)\n  return(predict(model, newdat))\n}\n\nnewdat = bind_cols(newdat,\n          deg1 = predict_poly(1),\n          deg3 = predict_poly(3),\n          deg10 = predict_poly(10),\n          )\n\n# Plotting the data and models of different degrees\ndat |&gt;  ggplot(aes(x=x, y=y)) +\n  geom_point() +\n  geom_line(data = newdat, aes(x=x, y=deg1), color='blue') +\n  geom_line(data = newdat, aes(x=x, y=deg3), color='red') +\n  geom_line(data = newdat, aes(x=x, y=deg10), color='green') +\n  labs(title=\"Polynomial Regression with Different Degrees\", x=\"X\", y=\"Y\") +\n  scale_color_manual(values=c(\"blue\", \"red\", \"green\")) +\n  theme_minimal()\n\n\n\n\nIn this example:\n\nA low-degree polynomial (blue line) may not capture the underlying trend well, exhibiting high bias and low variance.\nA moderate-degree polynomial (red line) might strike a good balance, capturing the underlying trend while not being too sensitive to the noise in the data.\nA high-degree polynomial (green line) might fit the training data very closely, capturing the noise as if it were a signal, thus exhibiting low bias but high variance.\n\nBy adjusting the complexity of the model (in this case, the degree of the polynomial), we can observe the impact on bias and variance, illustrating the bias-variance trade-off. The key in practice is to find the complexity level that minimizes the total error on unseen data."
  },
  {
    "objectID": "06_Ensemble.html#voting-classifiers",
    "href": "06_Ensemble.html#voting-classifiers",
    "title": "\n6  Ensemble Learning\n",
    "section": "\n6.3 Voting Classifiers",
    "text": "6.3 Voting Classifiers\nImagine you’ve developed several classifiers, each achieving an accuracy of around 80%. Among these, you have a diverse array: a Logistic Regression classifier, an SVM classifier, and potentially others. A straightforward strategy to enhance classification performance is to combine the predictions from each of these classifiers and select the class that receives the most votes. This method, known as a hard voting classifier, can lead to a composite classifier that outperforms even the most accurate member of the ensemble.\nSurprisingly, a voting classifier can achieve higher precision than any individual classifier in the group. This improvement in accuracy occurs even if each classifier performs only marginally better than chance. With a large and diverse set of such weak learners, the ensemble can become a strong learner, capable of high accuracy. How does this phenomenon occur? Consider the following analogy for clarity.\nImagine a coin that is slightly biased, with a 51% chance of landing heads and a 49% chance of landing tails. If you flip this coin 1,000 times, you’re likely to observe around 510 heads and 490 tails, leading to a majority of heads. Calculations reveal that the likelihood of seeing more heads than tails after 1,000 flips is about 75%. This probability increases with the number of flips, exceeding 97% after 10,000 flips. This outcome is a result of the law of large numbers, which posits that as the number of flips increases, the proportion of heads approaches the true likelihood of a head flip (51%).\nWe can visualize this by simulating a slightly biased coin in R.\n\nlibrary(tidyverse)\n\nn = 20000\n\nset.seed(1004)\n\n\nflips = rbinom(n, 1, .51)\nratio_heads = cummean(flips)\ndat = tibble(flip = 1:n, ratio_heads, sim = 1)\n\nfor(i in 2:20){\n  flips = rbinom(n, 1, .51)\nratio_heads = cummean(flips)\ndat = bind_rows(dat, tibble(flip = 1:n, ratio_heads, sim = i))\n}\n\n\ndat |&gt; \n  ggplot(aes(x = flip, y = ratio_heads, group = sim))+\n  geom_line(col = \"gray\")+\n  ylim(.4, .6)+\n  geom_hline(yintercept = .5)+\n  geom_hline(yintercept = .51, lty = 2)+\n  ylab(\"Proportion of Heads\")+\n  xlab(\"Number of Flips\")+\n  ggtitle(\"20 Simulated series of flipping a biased coin 20,000 times\")\n\n\n\n\nApplying this principle to ensemble learning, consider an ensemble of 1,000 classifiers, each with a 51% chance of making the correct prediction (just above random chance). By aggregating their predictions and choosing the class with the majority vote, you might expect accuracy rates as high as 75%. This scenario assumes, however, that all classifiers make independent errors, an idealization not often met since the classifiers are trained on the same data and may make similar mistakes. Consequently, the likelihood of incorrect majority votes increases, potentially lowering the ensemble’s overall accuracy.\nSoft Voting\nWhen your classifiers are capable of estimating class probabilities you can leverage a technique known as soft voting. Soft voting calculates the class with the highest average probability across all classifiers, potentially leading to more accurate ensemble predictions. This method can outperform hard voting because it assigns greater influence to predictions made with high confidence."
  },
  {
    "objectID": "06_Ensemble.html#bagging-and-pasting",
    "href": "06_Ensemble.html#bagging-and-pasting",
    "title": "\n6  Ensemble Learning\n",
    "section": "\n6.4 Bagging and Pasting",
    "text": "6.4 Bagging and Pasting\nAchieving a diverse collection of classifiers can be accomplished in two primary ways: employing a variety of training algorithms or using a single algorithm but training it on distinct random subsets of the training data. The latter approach manifests in two techniques: bagging and pasting, based on how samples are drawn from the training set.\nBagging—short for bootstrap aggregating—involves drawing random subsets of the training set with replacement. That means the same training instance can appear multiple times in the same subset. Pasting, on the other hand, draws subsets without replacement, ensuring that training instances don’t repeat within a subset.\nThe essence of both bagging and pasting is to permit each model in the ensemble to learn from a slightly different perspective of the data, thereby introducing diversity. While bagging allows the same data point to be used multiple times by the same predictor, pasting does not.\nOnce training is complete, the ensemble predicts new instances by aggregating the outcomes of all models. For classification tasks, this aggregation typically involves selecting the most common prediction (analogous to a hard voting classifier), whereas for regression tasks, the average prediction is used. Although each model may exhibit higher bias when trained on a subset of the training data, the aggregation process tends to decrease both bias and variance. The result is an ensemble that generally retains a similar bias but exhibits a lower variance compared to a model trained on the entire training set.\nA significant advantage of bagging and pasting is their scalability. Since each model can be trained independently on a separate subset of data, the training process can be easily parallelized across multiple CPU cores or different servers. This parallelization extends to making predictions, making bagging and pasting highly efficient and popular for building scalable ensemble models."
  },
  {
    "objectID": "06_Ensemble.html#key-concepts-of-ensemble-learning",
    "href": "06_Ensemble.html#key-concepts-of-ensemble-learning",
    "title": "\n6  Ensemble Learning\n",
    "section": "\n6.5 Key Concepts of Ensemble Learning",
    "text": "6.5 Key Concepts of Ensemble Learning\n\nDiversity: The core idea behind ensemble learning is to create a group of diverse models. Diversity among models can arise from using different algorithms, training on different subsets of data, or varying the input features. This diversity enables the ensemble to capture various aspects of the data, reducing the likelihood of overfitting and improving generalization to unseen data.\nAggregation: Once a collection of models is created, their predictions are aggregated to produce a final ensemble prediction. The aggregation method depends on the task (classification or regression) and can range from simple techniques like voting or averaging to more complex methods like weighted averaging.\nBias-Variance Tradeoff: Ensemble learning helps in balancing the bias-variance tradeoff. High bias can lead to underfitting, where the model is too simple to capture the underlying structure of the data. High variance can lead to overfitting, where the model captures noise as if it were a signal. Ensembles, by combining multiple models, can reduce variance without significantly increasing bias, leading to more accurate predictions."
  },
  {
    "objectID": "06_Ensemble.html#random-forests",
    "href": "06_Ensemble.html#random-forests",
    "title": "\n6  Ensemble Learning\n",
    "section": "\n6.6 Random Forests",
    "text": "6.6 Random Forests\nRandom Forests are one of the most popular and powerful ensemble learning methods, applicable to both classification and regression tasks. A Random Forest builds upon the Decision Tree algorithm, creating an ensemble of trees to improve predictive performance.\nHow Random Forests Work\n\nBootstrap Aggregating (Bagging): Random Forests use bagging, where multiple trees are trained on different bootstrap samples of the data. A bootstrap sample is a random sample of the training data, drawn with replacement, which means some observations may be repeated in each sample.\nFeature Randomness: When building each tree, Random Forests introduce additional randomness by considering a random subset of features at each split. This approach ensures that the trees are de-correlated, reducing the ensemble’s variance without significantly increasing bias.\n\nAggregation of Predictions:\n\nFor classification, the Random Forest aggregates the predictions of all trees through majority voting. Each tree votes for a class, and the class with the most votes becomes the model’s prediction.\nFor regression, it averages the predictions of all trees. The final output is the mean of the predictions from all the trees in the forest.\n\n\nAdvantages of Random Forests\n\n\nHigh Accuracy: By combining multiple trees, Random Forests can achieve high accuracy, often outperforming individual decision trees.\n\nRobustness to Overfitting: Thanks to the randomness introduced during tree building and the aggregation of multiple predictions, Random Forests are less prone to overfitting than individual decision trees.\n\nVersatility: They can handle both numerical and categorical data, work well with both small and large datasets, and can be used for both classification and regression tasks.\n\nFeature Importance: Random Forests can provide insights into which features are most important in predicting the target variable.\nLimitations\n\n\nComplexity and Interpretability: A Random Forest model can be more complex and less interpretable than a single decision tree.\n\nComputationally Intensive: Training a large number of trees on large datasets can be computationally expensive, requiring significant resources.\n\nExample: Iris data\nThe Iris dataset consists of 150 observations of iris flowers, each with 4 features (sepal length, sepal width, petal length, and petal width) and classified into one of three species (setosa, versicolor, virginica).\nThe engine for fitting a random forest is in the ranger library.\n\nlibrary(tidymodels)\nlibrary(ranger)\n\nSetup the data:\n\ndata(iris)\nset.seed(123) # For reproducibility\n\n# Split the data into training and testing sets\niris_split = initial_split(iris, prop = 0.75)\niris_train = training(iris_split)\niris_test = testing(iris_split)\n\n#setup data for cross validation to tune the model\ndat_folds = vfold_cv(iris_train, v = 5, strata = Species)\n\nCreate the model and recipe. Note that there are two hyperparameters we need to determine for random forests. They are:\n\n\nmtry which is the number of predictors that will be randomly sampled at each split when creating the tree models.\n\nmin_n1 which is the minimum number of observations in a node that are required for the node to be split further.\n\nWe will fine tune these hyperparameters using cross validation.\n\nrf_model = rand_forest(trees = 100,\n                        mtry = tune(),\n                        min_n = tune()) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"classification\")\n\n\nrecipe = recipe(Species ~ ., data = iris_train)\n\nFine tune the model:\n\ntuning_grid = grid_regular(\n  mtry(range = c(1, 4)), #there are four possible features in this dataset\n  min_n(),\n  levels = 10\n)\n\n\n#fine tune the model\ntune_results = tune_grid(\n  object = workflow() |&gt;\n    add_recipe(recipe) |&gt;\n    add_model(rf_model),\n  resamples = dat_folds,\n  grid = tuning_grid\n)\n\nbest_params = select_best(tune_results, metric = \"accuracy\")\nbest_params\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     2     2 Preprocessor1_Model02\n\n\nFit the final model:\n\nfitted_model = finalize_workflow(\n      workflow() |&gt;\n        add_recipe(recipe) |&gt;\n        add_model(rf_model),\n      best_params\n    ) |&gt;\n      fit(data = iris_train)\n\nLet’s first examine how well we fit the training data.\n\npred_train = predict(fitted_model, new_data = iris_train) |&gt;\n  bind_cols(iris_train)\ntrain_metrics = metrics(pred_train, truth = Species, estimate = .pred_class)\ntrain_conf_mat = conf_mat(pred_train, truth = Species, estimate = .pred_class)\n\nprint(train_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass         1\n2 kap      multiclass         1\n\nprint(train_conf_mat)\n\n            Truth\nPrediction   setosa versicolor virginica\n  setosa         38          0         0\n  versicolor      0         33         0\n  virginica       0          0        41\n\n\nLet’s now predict the testing data.\n\n# Make predictions\npredictions = predict(fitted_model, iris_test) |&gt;\n  bind_cols(iris_test)\n\n# Evaluate the model\ntest_metrics = metrics(predictions, truth = Species, estimate = .pred_class)\ntest_conf_mat = conf_mat(predictions, truth = Species, estimate = .pred_class)\n\nprint(test_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.974\n2 kap      multiclass     0.959\n\nprint(test_conf_mat)\n\n            Truth\nPrediction   setosa versicolor virginica\n  setosa         12          0         0\n  versicolor      0         16         0\n  virginica       0          1         9"
  },
  {
    "objectID": "06_Ensemble.html#conclusion",
    "href": "06_Ensemble.html#conclusion",
    "title": "\n6  Ensemble Learning\n",
    "section": "\n6.7 Conclusion",
    "text": "6.7 Conclusion\nEnsemble learning, with Random Forests as a prime example, represents a potent set of techniques in the field of predictive analytics. By harnessing the power of multiple models, ensemble methods can deliver superior predictive performance, making them a crucial tool in the data scientist’s arsenal. Understanding and applying ensemble methods, such as Random Forests, can greatly enhance the effectiveness of predictive models in classification and regression tasks."
  },
  {
    "objectID": "07_Dimension.html",
    "href": "07_Dimension.html",
    "title": "\n7  Dimensionality Reduction\n",
    "section": "",
    "text": "Introduction\nIn the realm of statistics and data science, the ability to effectively analyze and draw insights from data is paramount. As we venture into the era of big data, we encounter datasets of increasing complexity and size. These datasets often comprise a vast number of variables, a situation described as high dimensionality. Understanding the concept of dimensionality reduction is essential, not just as an abstract mathematical idea but as a practical tool for making sense of complex data.\nDimensionality reduction sits at the heart of predictive analytics, serving as a bridge between raw data and actionable insights. It addresses several critical challenges in data analysis, including the curse of dimensionality, noise in the dataset, and the difficulties involved in visualizing multidimensional data. By simplifying the data without significant loss of information, dimensionality reduction techniques enable us to build models that are not only more efficient but also more interpretable.\nThe study of dimensionality reduction offers a glimpse into the interdisciplinary nature of data science, where statistics, computer science, and domain expertise converge. This area highlights the importance of understanding both the theoretical foundations and the practical applications of statistical methods. While the mathematical underpinnings, such as linear algebra, are crucial, the focus here is on grasping the conceptual framework and the impact of dimensionality reduction on data analysis.\nThe introduction to dimensionality reduction begins with the rationale behind it. As datasets grow in size and complexity, the limitations of traditional analytical tools become apparent. The curse of dimensionality, a phenomenon where the data space expands so much that our data becomes sparse, affects not only the computational feasibility of models but also their performance. Reducing the number of input variables helps mitigate these issues, making models simpler, faster, and more generalizable.\nMoreover, in a practical setting, the reduction of dimensionality can be pivotal for noise reduction and visualization. By filtering out irrelevant or redundant features, we improve the model’s accuracy and reliability. Similarly, the transformation of high-dimensional data into a more manageable form allows for effective visualization, which is indispensable for data exploration and hypothesis generation.\nDimensionality reduction techniques, categorized into feature selection and feature extraction, offer a toolkit for addressing these challenges. Feature selection methods focus on identifying the most relevant features for the models. On the other hand, feature extraction techniques like Principal Component Analysis (PCA), transform the original features into a new set of variables that better capture the underlying structure of the data.\nReasons for Dimensionality Reduction\n\n\nCurse of Dimensionality: The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. It’s essential in understanding how dimensionality affects data analysis, leading to specific issues like data sparsity and increased computational complexity.\nUnderstanding the Curse of Dimensionality\nWhen the dimensionality increases, the volume of the space increases so rapidly that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Additionally, high-dimensional datasets are often accompanied by increased computational complexity and a greater chance of overfitting, making models less generalizable to new data.\nExample using the Protein dataset\nThe Protein dataset in the MultBiplotR library consists of data on protein consumption in various countries.\n\nLoading and Exploring the Data\n\n\nlibrary(tidyverse)\nlibrary(MultBiplotR)\nlibrary(GGally)\n\ndata(Protein)\nglimpse(Protein)\n\nRows: 25\nColumns: 11\n$ Comunist          &lt;fct&gt; Yes, No, No, Yes, Yes, No, Yes, No, No, No, Yes, No,…\n$ Region            &lt;fct&gt; South, Center, Center, South, Center, North, Center,…\n$ Red_Meat          &lt;dbl&gt; 10.1, 8.9, 13.5, 7.8, 9.7, 10.6, 8.4, 9.5, 18.0, 10.…\n$ White_Meat        &lt;dbl&gt; 1.4, 14.0, 9.3, 6.0, 11.4, 10.8, 11.6, 4.9, 9.9, 3.0…\n$ Eggs              &lt;dbl&gt; 0.5, 4.3, 4.1, 1.6, 2.8, 3.7, 3.7, 2.7, 3.3, 2.8, 2.…\n$ Milk              &lt;dbl&gt; 8.9, 19.9, 17.5, 8.3, 12.5, 25.0, 11.1, 33.7, 19.5, …\n$ Fish              &lt;dbl&gt; 0.2, 2.1, 4.5, 1.2, 2.0, 9.9, 5.4, 5.8, 5.7, 5.9, 0.…\n$ Cereal            &lt;dbl&gt; 42.3, 28.0, 26.6, 56.7, 34.3, 21.9, 24.6, 26.3, 28.1…\n$ Starch            &lt;dbl&gt; 0.6, 3.6, 5.7, 1.1, 5.0, 4.8, 6.5, 5.1, 4.8, 2.2, 4.…\n$ Nuts              &lt;dbl&gt; 5.5, 1.3, 2.1, 3.7, 1.1, 0.7, 0.8, 1.0, 2.4, 7.8, 5.…\n$ Fruits_Vegetables &lt;dbl&gt; 1.7, 4.3, 4.0, 4.2, 4.0, 2.4, 3.6, 1.4, 6.5, 6.5, 4.…\n\n\n\nBelow is a scatterplot matrix of the pairs of the quantitative features.\n\n\nProtein |&gt; \n  select(-c(Comunist, Region)) |&gt; \n  ggpairs()\n\n\n\n\nWith so many variables, it becomes cumbersome to visualize the relationships between the variables. The above scatterplot matrix shows the scatterplots of the 36 pairs of variables when you have nine features. It becomes difficult to examine all of these scatterplots. These plots also do not show any relationships between three variables at a time, and so forth.\n\nIn addition to visualization, the curse of dimenstionality also implies the sparseness of the data when we have more variables.\n\nLet’s first examine the variable Milk. Below is a dotplot for this variable.\n\nProtein |&gt; \n  ggplot(aes(x = Milk, y = 0))+\n  geom_point()+\n  ylab(\"\")+\n  theme(\n    axis.ticks.y=element_blank(),\n    axis.text.y=element_blank()\n    )\n\n\n\n\nNote that the min value of Milk is 4.9 and the max value is 33.7. There are 25 observations so we can think of the data for this variable as 25 pieces of information in that variable’s dimension (the \\(x\\) dimenstion on the plot above). Let’s calculate the ratio of information to the dimensional space:\n\\[\n\\frac{25}{33.7-4.9} = 0.8681\n\\]\nLet’s now examine another variable along with Milk. Below is the scatterplot of Eggs and Milk\n\nProtein |&gt; \n  ggplot(aes(x = Milk, y = Eggs))+\n  geom_point()\n\n\n\n\nFor Eggs, the range of values are min = 0.5 and max = 4.7. The total dimensional space that these two variables take is \\[\\begin{align*}\n\\text{dimensional space for Milk}\\times\\text{dimensional space for Eggs} &= 28.8 \\times 4.2\\\\\n&= 120.96\n\\end{align*}\\]\nWe still only have 25 observations. The 25 pieces of information that we have in this two-dimensional space gives us the ratio \\[\n\\frac{25}{120.96} = 0.2067\n\\]\nSo in two-dimensional space, the amount of data we have is a much lower ratio of the space than when we had in only one dimension.\nLet’s now add in a third variable Fish. Note that the dimensional space for Fish is min = 0.2 and max = 14.2. The size of the total dimensional space is \\[\\begin{align*}\n\\text{dim. space for Milk}\\times\\text{dim. space for Eggs}\\times\\text{dim. space for Fish} &= 28.8 \\times 4.2 \\times 14\\\\\n&= 1693.44\n\\end{align*}\\]\nAgain, we still only have 25 observations. So our 25 pieces of information only take up a ratio of \\[\n\\frac{25}{1693.44}=0.0148\n\\] of the 3-dimensional space. The more variables we have, the higher the dimensional space our observations are in. The ratio of our observations to the area of the dimensional space will continue to decrease. Thus, the amount of data available in that high dimension is sparse. This is the curse of dimensionality.\n\nTechniques of Dimensionality Reduction\nThere are several techniques for reducing the dimensionality of data, broadly categorized into Feature Selection and Feature Extraction.\nFeature Selection\nFeature selection involves selecting a subset of the most relevant features for use in model construction. There are three main types of feature selection methods:\n\n\nFilter Methods: Filter Methods are among the first steps you can take in preprocessing your data for machine learning models. They are computationally less expensive than Wrapper and Embedded Methods because they do not involve training models as part of the feature selection process. Instead, they rely on general characteristics of the data, such as correlation coefficients, Chi-square tests, and mutual information.\nAdvantages:\n\n\nSpeed: They are fast and scalable to high-dimensional datasets because they evaluate features in isolation from the model.\n\nSimplicity: These methods are straightforward to understand and implement.\n\nModel Agnostic: They can be applied regardless of the choice of machine learning algorithm.\n\nDisadvantages:\n\nLess Accurate: They might not capture feature interactions well because they evaluate each feature independently.\nNo Model Context: They do not consider how features will interact when combined in a model, potentially overlooking combinations that would improve model performance.\n\nCommon Techniques in Filter Methods\n\nCorrelation Coefficient: This measures the linear relationship between two variables. Features with very low correlation to the target variable can be removed.\nChi-Square Test: This statistical test is used to determine if there is a significant association between two categorical variables. It can be used to select relevant features for classification problems.\nMutual Information: This measures the amount of information one can obtain from one variable through another. A higher value means more information shared, making it useful for feature selection.\nVariance Threshold: This method removes all features whose variance doesn’t meet some threshold. Since variables with a low variance are less likely to affect the target variable, they can be considered for removal.\n\nApplication of Filter Methods\nFilter Methods are widely used at the beginning stages of the feature selection process, especially when dealing with very high-dimensional data. They help in narrowing down the feature set to a more manageable size, which can then be further refined using more sophisticated techniques like Wrapper and Embedded Methods.\nExample: Protein data\nSuppose Milk is the response variable. We can examine the correlation between Milk and all other features.\n\ncorrelation_matrix &lt;- Protein |&gt; \n  select(where(is.numeric)) |&gt; \n  cor()\n\nmilk_correlations &lt;- correlation_matrix['Milk', ]\nprint(milk_correlations)\n\n         Red_Meat        White_Meat              Eggs              Milk \n        0.5029311         0.2814839         0.5755331         1.0000000 \n             Fish            Cereal            Starch              Nuts \n        0.1378837        -0.5927366         0.2224112        -0.6210875 \nFruits_Vegetables \n       -0.4083641 \n\n# Visualize the correlations for better understanding\nmilk_correlations %&gt;% as_tibble() %&gt;%\n  rownames_to_column(\"Feature\") %&gt;%\n  ggplot(aes(x = reorder(Feature, -value), y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(y = \"Correlation with Milk\", \n       x = \"Feature\", \n       title = \"Feature Correlation with Milk\")\n\n\n\n# Select features based on a correlation threshold, for example, features with absolute correlation &gt; 0.3\nrelevant_features &lt;- names(milk_correlations[which(abs(milk_correlations) &gt; 0.3)])\nprint(relevant_features)\n\n[1] \"Red_Meat\"          \"Eggs\"              \"Milk\"             \n[4] \"Cereal\"            \"Nuts\"              \"Fruits_Vegetables\"\n\n\nWhile Filter Methods are an efficient way to reduce dimensionality, especially in the preliminary stages of model development, they should be part of a broader feature selection strategy that may include more sophisticated methods. Combining various methods thoughtfully can lead to the development of more accurate and robust predictive models.\n\n\nWrapper Methods: Wrapper methods select features based on the performance of a predictive model, where features are added or removed according to their contribution to model accuracy. This approach differs from filter methods, which rely on the general characteristics of the data, and embedded methods, which perform feature selection as part of the model training process.\nExample: Applying Wrapper Methods to the Protein Dataset\nAssuming we are interested in modeling Milk consumption based on the other dietary habits reflected in the dataset (like Red_Meat, White_Meat, Eggs, etc.), we could use a wrapper method to select the most relevant features for predicting Milk consumption. This process involves iteratively adding or removing features based on their impact on the model’s predictive accuracy.\nWe use a stepwise regression approach, which considers both addition and removal of features based on their statistical significance to the model’s performance. The stepAIC function from the MASS package in R can perform this operation, aiming to minimize the Akaike Information Criterion (AIC) for model selection:\n\nlibrary(MASS)\n\nfit &lt;- lm(Milk ~ ., data=Protein)\nstepModel &lt;- stepAIC(fit, direction=\"both\", trace = 1)\n\nStart:  AIC=68.39\nMilk ~ Comunist + Region + Red_Meat + White_Meat + Eggs + Fish + \n    Cereal + Starch + Nuts + Fruits_Vegetables\n\n                    Df Sum of Sq    RSS    AIC\n- Fruits_Vegetables  1     2.125 149.69 66.743\n- Red_Meat           1     2.736 150.31 66.845\n- White_Meat         1     3.274 150.84 66.934\n- Eggs               1     3.568 151.14 66.983\n- Nuts               1     5.281 152.85 67.265\n- Cereal             1     8.037 155.61 67.712\n- Starch             1     9.911 157.48 68.011\n&lt;none&gt;                           147.57 68.386\n- Fish               1    57.513 205.08 74.613\n- Comunist           1   107.506 255.08 80.067\n- Region             2   180.686 328.26 84.373\n\nStep:  AIC=66.74\nMilk ~ Comunist + Region + Red_Meat + White_Meat + Eggs + Fish + \n    Cereal + Starch + Nuts\n\n                    Df Sum of Sq    RSS    AIC\n- Red_Meat           1     1.861 151.56 65.052\n- White_Meat         1     2.791 152.49 65.205\n- Eggs               1     3.327 153.02 65.293\n- Nuts               1     4.953 154.65 65.557\n- Starch             1     8.849 158.54 66.179\n- Cereal             1    11.784 161.48 66.637\n&lt;none&gt;                           149.69 66.743\n+ Fruits_Vegetables  1     2.125 147.57 68.386\n- Fish               1    69.345 219.04 74.259\n- Comunist           1   131.382 281.08 80.494\n- Region             2   251.016 400.71 87.359\n\nStep:  AIC=65.05\nMilk ~ Comunist + Region + White_Meat + Eggs + Fish + Cereal + \n    Starch + Nuts\n\n                    Df Sum of Sq    RSS    AIC\n- Eggs               1     2.028 153.58 63.384\n- Starch             1     7.037 158.59 64.187\n- Nuts               1     8.774 160.33 64.459\n- White_Meat         1    12.466 164.02 65.028\n&lt;none&gt;                           151.56 65.052\n- Cereal             1    12.641 164.20 65.055\n+ Red_Meat           1     1.861 149.69 66.743\n+ Fruits_Vegetables  1     1.250 150.31 66.845\n- Fish               1    67.778 219.33 72.293\n- Comunist           1   148.324 299.88 80.113\n- Region             2   249.667 401.22 85.391\n\nStep:  AIC=63.38\nMilk ~ Comunist + Region + White_Meat + Fish + Cereal + Starch + \n    Nuts\n\n                    Df Sum of Sq    RSS    AIC\n- Starch             1     5.327 158.91 62.237\n- Nuts               1     9.676 163.26 62.912\n&lt;none&gt;                           153.58 63.384\n- Cereal             1    14.196 167.78 63.594\n- White_Meat         1    17.706 171.29 64.112\n+ Eggs               1     2.028 151.56 65.052\n+ Fruits_Vegetables  1     1.376 152.21 65.159\n+ Red_Meat           1     0.562 153.02 65.293\n- Fish               1    69.467 223.05 70.713\n- Comunist           1   193.735 347.32 81.784\n- Region             2   249.861 403.44 83.529\n\nStep:  AIC=62.24\nMilk ~ Comunist + Region + White_Meat + Fish + Cereal + Nuts\n\n                    Df Sum of Sq    RSS    AIC\n- Nuts               1    11.171 170.08 61.935\n- Cereal             1    12.726 171.64 62.163\n&lt;none&gt;                           158.91 62.237\n+ Starch             1     5.327 153.58 63.384\n- White_Meat         1    22.106 181.02 63.493\n+ Fruits_Vegetables  1     1.168 157.74 64.052\n+ Eggs               1     0.318 158.59 64.187\n+ Red_Meat           1     0.098 158.81 64.221\n- Fish               1    75.734 234.65 69.980\n- Comunist           1   192.708 351.62 80.092\n- Region             2   247.305 406.22 81.700\n\nStep:  AIC=61.94\nMilk ~ Comunist + Region + White_Meat + Fish + Cereal\n\n                    Df Sum of Sq    RSS    AIC\n- Cereal             1      7.49 177.57 61.012\n- White_Meat         1     14.09 184.18 61.925\n&lt;none&gt;                           170.08 61.935\n+ Nuts               1     11.17 158.91 62.237\n+ Starch             1      6.82 163.26 62.912\n+ Eggs               1      0.57 169.51 63.851\n+ Fruits_Vegetables  1      0.34 169.74 63.885\n+ Red_Meat           1      0.34 169.74 63.885\n- Fish               1     77.57 247.65 69.328\n- Comunist           1    181.55 351.63 78.093\n- Region             2    480.69 650.77 91.482\n\nStep:  AIC=61.01\nMilk ~ Comunist + Region + White_Meat + Fish\n\n                    Df Sum of Sq    RSS    AIC\n&lt;none&gt;                           177.57 61.012\n- White_Meat         1     19.40 196.97 61.604\n+ Cereal             1      7.49 170.08 61.935\n+ Nuts               1      5.93 171.64 62.163\n+ Starch             1      5.07 172.50 62.288\n+ Fruits_Vegetables  1      2.15 175.42 62.708\n+ Eggs               1      1.29 176.28 62.830\n+ Red_Meat           1      0.31 177.26 62.969\n- Fish               1    140.03 317.60 73.548\n- Comunist           1    253.74 431.31 81.199\n- Region             2    552.09 729.66 92.342\n\nsummary(stepModel)\n\n\nCall:\nlm(formula = Milk ~ Comunist + Region + White_Meat + Fish, data = Protein)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6316 -1.6205  0.0353  1.4023  5.7698 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   37.4316     3.3803  11.074 9.94e-10 ***\nComunistYes   -8.1633     1.5667  -5.211 4.98e-05 ***\nRegionCenter  -8.8917     2.1961  -4.049 0.000685 ***\nRegionSouth  -17.3002     2.2512  -7.685 3.03e-07 ***\nWhite_Meat    -0.3832     0.2660  -1.441 0.165963    \nFish          -0.9794     0.2530  -3.871 0.001029 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.057 on 19 degrees of freedom\nMultiple R-squared:  0.8535,    Adjusted R-squared:  0.8149 \nF-statistic: 22.13 on 5 and 19 DF,  p-value: 2.555e-07\n\n\nThis code starts with a model that includes all available features (except Milk, which is our target variable) and then iteratively adds or removes features to find a combination that offers the best balance between model complexity and predictive power, as measured by the AIC.\nEvaluating the Selected Features\nThe output of stepAIC will indicate which features have been selected as predictors for Milk consumption. These features are deemed by the stepwise regression process as having significant predictive power for Milk consumption, after considering the potential for overfitting (through AIC minimization).\nThis approach allows for an automated and data-driven selection of features, which can be especially useful when dealing with datasets with many variables. By focusing on the subset of features that contribute most to prediction accuracy, wrapper methods can help create more efficient and interpretable models.\n\n\nEmbedded Methods: Embedded methods are particularly useful as they perform feature selection while the model is being trained, which can lead to a more optimal set of features for the prediction task at hand. Regularization methods like LASSO (Least Absolute Shrinkage and Selection Operator) are common examples of embedded methods because they both train the model and select features by penalizing the absolute size of the regression coefficients.\nEmbedded methods combine the qualities of filter and wrapper methods by performing feature selection as part of the model training process. This approach can lead to more accurate and efficient models because it considers the interaction between features and the model. One key advantage of embedded methods is their ability to capture complex interactions between features, which might be missed by filter methods.\nExample: Protein Data\nIn this example, we’ll use the Protein data to predict Milk consumption using various dietary habits reflected in the dataset (like Red Meat, White Meat, Eggs, etc.). We’ll apply LASSO regression, an embedded method, using the tidymodels framework, which simplifies model training, feature selection, and prediction in R.\n\nlibrary(tidyverse)\nlibrary(MultBiplotR)\nlibrary(tidymodels)\n\ndata(Protein)\n# Protein &lt;- Protein %&gt;% mutate(Comunist = as.numeric(Comunist == \"Yes\"), \n#                            Region = as.numeric(factor(Region)))\n\n# Splitting the dataset\nset.seed(123) \nsplit &lt;- initial_split(Protein, prop = 0.75)\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\nWe’ll use a LASSO regression model, which is suited for datasets with potentially correlated predictors and can help in feature selection by shrinking some coefficients to zero.\n\nlasso_spec &lt;- linear_reg(penalty = 0.1, mixture = 1)  |&gt; \n  set_engine(\"glmnet\") |&gt; \n  set_mode(\"regression\")\n\nrecipe &lt;- recipe(Milk ~ ., data = train_data)  |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_dummy(all_nominal_predictors())\n\nlasso_fit &lt;- workflow() %&gt;%\n  add_model(lasso_spec) %&gt;%\n  add_recipe(recipe) %&gt;%\n  fit(data = train_data)\n\nresults &lt;- lasso_fit %&gt;%\n  predict(new_data = test_data) %&gt;%\n  bind_cols(test_data) %&gt;%\n  metrics(truth = Milk, estimate = .pred)\n\nselected_features &lt;- lasso_fit %&gt;%\n  pull_workflow_fit() %&gt;%\n  tidy() %&gt;%\n  filter(estimate != 0) %&gt;% # Selecting non-zero coefficients\n  pull(term)\n\nprint(results)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       5.88 \n2 rsq     standard       0.563\n3 mae     standard       5.06 \n\nprint(selected_features)\n\n [1] \"(Intercept)\"       \"Red_Meat\"          \"Eggs\"             \n [4] \"Cereal\"            \"Starch\"            \"Nuts\"             \n [7] \"Fruits_Vegetables\" \"Comunist_Yes\"      \"Region_Center\"    \n[10] \"Region_South\"     \n\n\nThis process will provide us with an understanding of which dietary habits (features) are most predictive of Milk consumption, leveraging the embedded method’s ability to perform feature selection in conjunction with model training. By focusing on the subset of features that contribute most to prediction accuracy, we can create more efficient and interpretable models.\n\nFeature Extraction\nFeature extraction transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear or nonlinear, with the transformed features being combinations of the original features. The most common feature extraction techniques include:\n\n\nPrincipal Component Analysis (PCA):\n\n\nPurpose: PCA reduces dimensionality by transforming the original variables into a new set of uncorrelated variables, called principal components, which are ordered by the amount of original variance they capture. The first principal component captures the most variance, the second captures the second most, and so on.\n\nSuppose we had two variables: height and hair color (some measure of darkness of hair) for a Native American tribe. Below is a scatterplot of the two variables: \nIt’s evident that in this tribe, the variation in hair color among individuals is minimal compared to the range of their heights. Therefore, height emerges as a more significant characteristic than hair color. Consequently, by incorporating only the height of individuals from this tribe as a feature in the dataset, we can preserve the majority of the relevant information.\nMost situation do not result in a scatterplot as we see above. Instead, you may see a situation as below.\n\n\n\n\n\nInstead of \\(X\\) or \\(y\\) having small variability, we can imagine a line drawn through the points and the variability of the points about that line is small.\n\n\n\n\n\nIf we rotate the plot so that the blue line is now the horizontal axis, the new axes can then be examined and we can use ony the axis that has the larger variability. These new axes are called the Principal Components.\n\n\n\n\n\nIn this example, the first principal component (PCA1) has large variability. the second principal component (PCA2) has small variability. So if we use only PCA1 as our feature, then we only lose a small amount of information in how the data varies when we do not select PCA2.\n\n\nHow it Works: PCA starts by calculating the covariance matrix of the data to understand how variables are related. It then computes the eigenvectors and eigenvalues of this covariance matrix. Eigenvectors determine the directions of the new space, and eigenvalues determine their magnitude. In essence, the eigenvectors with the highest eigenvalues are selected to form the new set of variables.\n\nApplications: PCA is widely used in exploratory data analysis and for making predictive models. It’s particularly useful in processing images, genomics data, and in areas where the data dimensions are very high.\n\n\n\nLinear Discriminant Analysis (LDA):\n\n\nPurpose: LDA is a supervised dimensionality reduction technique used to find the linear combinations of features that best separate two or more classes of objects or events. The goal is to project the features in higher-dimensional space onto a lower-dimensional space with good class-separability in order to avoid overfitting (“curse of dimensionality”) and also reduce computational costs.\n\nHow it Works: LDA computes the directions (“linear discriminants”) that will represent the axes that maximize the separation between multiple classes. It takes the mean and variance of each class into account and seeks to reduce variance within each class while maximizing variance between the classes.\n\nApplications: LDA is particularly useful in the preprocessing steps for pattern classification and machine learning applications. Its application spans across various fields including face recognition, medical diagnosis, and any domain requiring classification tasks.\n\n\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE):\n\n\nPurpose: t-SNE is a nonlinear technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It converts similarities between data points to joint probabilities and tries to minimize the divergence between these joint probabilities in the high-dimensional and low-dimensional space.\n\nHow it Works: t-SNE starts by calculating the probability that pairs of datapoints in the high-dimensional space are similar, then uses a gradient descent method to minimize the difference between this probability distribution and a similar distribution in the low-dimensional space.\n\nApplications: Because of its ability to preserve local structures and resolve clusters in a small area of the map, t-SNE is highly favored for visualizing high-dimensional data such as genetic data, image data, or text data.\n\n\n\nConsiderations When Choosing a Feature Extraction Technique:\n\nThe nature of the dataset: Is it linear or nonlinear? PCA and LDA assume linear relationships between variables, while t-SNE does not.\n\nSupervised vs. Unsupervised learning: PCA and t-SNE are unsupervised methods (they do not require labeled data), whereas LDA is supervised (requires labeled data).\n\nGoal of dimensionality reduction: Is the goal to improve visualization (t-SNE), to prepare for a classification task (LDA), or to reduce feature space while retaining variance (PCA)?\n\nComputational resources and dataset size: PCA and LDA are relatively more computationally efficient than t-SNE, especially for very large datasets.\nPCA Example: Protein Data\n\n# Load necessary libraries\nlibrary(MultBiplotR)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n# Load the Protein dataset\ndata(\"Protein\")\n\n# Prepare the data for PCA - select only numeric columns\nprotein_numeric &lt;- Protein |&gt; \n  dplyr::select(where(is.numeric))\n\n# PCA with tidymodels\npca_recipe &lt;- recipe(~., data = protein_numeric) %&gt;%\n  step_normalize(all_predictors(), -all_outcomes()) %&gt;%\n  step_pca(all_predictors(), threshold = .75) #Get enough PCs to get 75% of variance\n\n# Prep the recipe to estimate PCA components\ndat_prep &lt;- prep(pca_recipe, training = protein_numeric)\n\n# Extract the PCA results\npca_results &lt;- bake(dat_prep, protein_numeric)\n\n# View the results\nprint(pca_results)\n\n# A tibble: 25 × 3\n      PC1    PC2      PC3\n    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1  3.49  -1.63  -1.76   \n 2 -1.42  -1.04   1.34   \n 3 -1.62   0.159  0.217  \n 4  3.13  -1.30   0.151  \n 5 -0.370 -0.603  1.20   \n 6 -2.37   0.285 -0.752  \n 7 -1.42   0.450  1.30   \n 8 -1.56  -0.596 -2.05   \n 9 -1.49   0.785  0.00188\n10  2.24   1.00  -0.883  \n# ℹ 15 more rows\n\ntidy(dat_prep, number = 2, type = \"variance\") |&gt; \n  filter(terms == \"cumulative percent variance\") |&gt; \n  print()\n\n# A tibble: 9 × 4\n  terms                       value component id       \n  &lt;chr&gt;                       &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n1 cumulative percent variance  44.5         1 pca_zPW5q\n2 cumulative percent variance  62.7         2 pca_zPW5q\n3 cumulative percent variance  75.2         3 pca_zPW5q\n4 cumulative percent variance  85.8         4 pca_zPW5q\n5 cumulative percent variance  91.0         5 pca_zPW5q\n6 cumulative percent variance  94.6         6 pca_zPW5q\n7 cumulative percent variance  97.6         7 pca_zPW5q\n8 cumulative percent variance  98.9         8 pca_zPW5q\n9 cumulative percent variance 100           9 pca_zPW5q\n\n\nWe see that only three principal components are needed to capture 75% of the variability present in the nine predictor variables.\n\n# Visualize PCA results\npca_results %&gt;%\n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_point() +\n  theme_minimal() +\n  ggtitle(\"PCA of Protein Consumption Data\") +\n  xlab(\"Principal Component 1\") +\n  ylab(\"Principal Component 2\")"
  },
  {
    "objectID": "08_Clustering.html#introduction",
    "href": "08_Clustering.html#introduction",
    "title": "\n8  Clustering\n",
    "section": "\n8.1 Introduction",
    "text": "8.1 Introduction\nClustering, as an essential component of unsupervised machine learning, plays a pivotal role in discovering the intrinsic patterns and structures within datasets without the guidance of predefined labels or categories. This characteristic makes it particularly valuable in exploratory data analysis, where the goal is to uncover hidden relationships, groupings, or patterns in the data that are not immediately apparent.\nThe versatility of clustering extends beyond mere data grouping; it is a powerful technique for pattern recognition, enabling the identification of coherent groups or structures within complex datasets. This capability is crucial across a spectrum of fields, from image analysis, where clustering can segment images into meaningful components, to information retrieval, enhancing search engines’ ability to organize and categorize information efficiently.\nIn bioinformatics, clustering facilitates the analysis of genetic and proteomic data, helping scientists uncover functional groups of genes or proteins and their roles in biological processes. Similarly, in the realm of social network analysis, clustering can identify communities or networks of individuals with shared interests or connections, providing insights into social dynamics and behaviors.\nThe introduction of clustering as a cornerstone technique highlights its foundational importance in predictive analytics and data mining. Its unsupervised nature lends itself to a wide range of applications, enabling analysts and researchers to approach data with the goal of discovery, rather than confirmation. This approach is fundamental in scenarios where the underlying structure of the data is unknown or complex, and where traditional supervised learning methods may not be applicable."
  },
  {
    "objectID": "08_Clustering.html#types-of-clustering-algorithms",
    "href": "08_Clustering.html#types-of-clustering-algorithms",
    "title": "\n8  Clustering\n",
    "section": "\n8.2 Types of Clustering Algorithms",
    "text": "8.2 Types of Clustering Algorithms\nClustering algorithms can be broadly categorized into several types, each with its methodology and applications. Here are the most commonly used ones:\n\n\nK-Means Clustering: This algorithm partitions the data into K clusters by minimizing the variance within each cluster. The process iteratively assigns each data point to the nearest cluster centroid and updates the centroid’s position based on the current members. K-Means is simple and efficient but requires the number of clusters to be specified a priori.\nExample: Iris dataset\n\nlibrary(tidymodels)\nlibrary(tidyclust)\n\nLoad the Iris dataset and explore it. It’s important to understand the data’s structure and ensure it’s clean before applying any clustering algorithm.\n\ndata(\"iris\")\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nAlthough the Iris dataset is relatively clean, preprocessing might include scaling the features since K-Means is sensitive to the scale of the data.\n\ndat_recipe = recipe(~ ., data = iris) |&gt; \n  step_rm(Species) |&gt; #remove the labels since clustering is unsupervised\n  step_normalize(all_numeric_predictors()) \n\nUse k_means() to cluster the data. Here, we need to specify the number of clusters. Since we know there are three species of iris in the dataset, we’ll use k = 3. However, in practice, you might use fine tuning methods to determine the optimal number of clusters.\n\nset.seed(1004)  \nkmeans_model &lt;- k_means(num_clusters = 3) |&gt;\n  set_engine(\"stats\")\n\ndat_wf = workflow() |&gt; \n  add_recipe(dat_recipe) |&gt; \n  add_model(kmeans_model)\n\n\nfitted_model = dat_wf |&gt; fit(data = iris)\n\nWhile there are four features, let’s visualize the clustering by plotting the clusters in terms of Petal.Length and Petal.Width.\n\nkmeans_summary &lt;- fitted_model |&gt;\n  extract_fit_summary()\n\niris_clustered &lt;- bind_cols(iris, Cluster = kmeans_summary$cluster_assignments)\nggplot(iris_clustered, aes(x = Petal.Length, y = Petal.Width, color = Cluster)) +\n  geom_point() +\n  labs(title = \"Iris Data Clustering with K-Means\")\n\n\n\n\nNow let’s examine the other two features:\n\nggplot(iris_clustered, aes(x = Sepal.Length, y = Sepal.Width, color = Cluster)) +\n  geom_point() +\n  labs(title = \"Iris Data Clustering with K-Means\")\n\n\n\n\nTune the number of clusters\nIn the Iris data, we know that there should be three groups because the data was originally labeled based on Species. Usually when you use clustering, you don’t know what the groupings should be or even the number of clusters to use. Let’s redo the above example but this time tune the number of clusters.\n\nset.seed(1004)\n\n\n#setup data for cross validation to tune the model\ndat_folds = vfold_cv(iris, v = 5, strata = Species)\n\ndat_recipe = recipe(~ ., data = iris) |&gt; \n  step_rm(Species) |&gt; #remove the labels since clustering is unsupervised\n  step_normalize(all_numeric_predictors()) \n\nkmeans_model &lt;- k_means(num_clusters = tune()) |&gt;\n  set_engine(\"stats\")\n\n#default range for num_clusters is 1 to 10\ntuning_grid &lt;- grid_regular(\n  num_clusters(),\n  levels = 10\n)\n\ndat_wf = workflow() |&gt; \n  add_recipe(dat_recipe) |&gt; \n  add_model(kmeans_model)\n\n\n#fine tune the model\nres &lt;- tune_cluster(\n  dat_wf,\n  resamples = dat_folds,\n  grid = tuning_grid,\n  control = control_grid(save_pred = TRUE, extract = identity),\n  metrics = cluster_metric_set(sse_within_total, sse_total, sse_ratio)\n)\n\nres_metrics &lt;- res |&gt;  collect_metrics()\n\nres_metrics |&gt;\n  filter(.metric == \"sse_ratio\") |&gt;\n  ggplot(aes(x = num_clusters, y = mean)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  ylab(\"mean WSS/TSS ratio, over 5 folds\") +\n  xlab(\"Number of clusters\") +\n  scale_x_continuous(breaks = 1:10)\n\n\n\n\nWe want to choose the number of clusters based on where the SSE Ratio starts to level off. That is, where does the decrease in the ratio become small. In the plot, we see this occurs at 3 or 4. So we should choose 3 or 4 clusters.\n\n\nHierarchical Clustering: Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:\n\n\nAgglomerative: This is a bottom-up approach where each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n\nDivisive: This is a top-down approach where all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n\nThe result of hierarchical clustering is a tree-based representation of the objects, which is also known as a dendrogram. The dendrogram illustrates the arrangement of the clusters produced by the corresponding analyses.\nExample: mtcars\nFor this example, let’s use the mtcars dataset available in R’s datasets package. The mtcars dataset comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(cluster)\n\ndata(mtcars)\ndat &lt;- mtcars %&gt;%\n  scale() # Scale the data\n\nWe will use an agglomerative method for our hierarchical clustering.\n\nset.seed(1004)\nhclust_res &lt;- dist(dat) %&gt;% \n  hclust() \n\nLet’s visualize the dendrogram to interpret our hierarchical clustering result.\n\nplot(hclust_res) # Set labels=FALSE for a cleaner plot\nrect.hclust(hclust_res, k=3, border=\"red\") # Optional: Highlight 3 clusters\n\n\n\n\n\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): DBSCAN groups together closely packed points, marking as outliers the points that lie alone in low-density regions. This method does not require one to specify the number of clusters in advance, and it can find clusters of arbitrary shape.\nExample: storms data\nFor our example, we will use the storms dataset available in the dplyr package. The storms dataset contains information about tropical storms and hurricanes, and we’ll focus on clustering based on the storms’ wind speeds and pressures.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dbscan)\n\n# Load and prepare the data\ndata(\"storms\", package = \"dplyr\")\nstorm_data &lt;- storms %&gt;%\n  select(wind, pressure) %&gt;%\n  na.omit() %&gt;%\n  filter(pressure &gt; 0)  # Filter out missing or erroneous pressure data\n\n# Scale the data\nscaled_data &lt;- storm_data %&gt;% \n  scale()\n\n# DBSCAN clustering\nset.seed(123)\ndbscan_model &lt;- dbscan(scaled_data, eps = .2, minPts = 10)\n\n# Visualize the clusters\nstorm_data_clustered &lt;- bind_cols(storm_data, cluster = dbscan_model$cluster)\nggplot(storm_data_clustered, aes(x = wind, y = pressure, color = as.factor(cluster))) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"DBSCAN Clustering of Storm Data\",\n       x = \"Wind Speed\",\n       y = \"Pressure\",\n       color = \"Cluster\") +\n  scale_color_viridis_d()\n\n\n\n\n\n\nIn this example, we perform clustering on the storms dataset based on wind speed and pressure, using DBSCAN through the dbscan package. The eps and minPts parameters would ideally be tuned based on the dataset characteristics and specific analysis goals. This process illustrates the application of DBSCAN in identifying natural groupings of storms without predefining the number of clusters, showcasing its ability to handle complex data structures and noise effectively.\nChoosing the Right Algorithm\nThe choice of a clustering algorithm depends on several factors, including:\n\nNature of the Data: The algorithm must be suitable for the scale, dimensionality, and type of the data (e.g., numerical, categorical).\nDesired Outcome: The definition of a “cluster” can vary by application (e.g., spherical clusters, density clusters), influencing the choice of algorithm.\nScalability and Efficiency: Large datasets require algorithms that can scale efficiently with data size.\nSensitivity to Parameters: Some algorithms, like K-Means, are sensitive to the choice of parameters (e.g., the number of clusters), which can affect their robustness and the reproducibility of the clustering.\nChallenges in Clustering\nWhile clustering is a powerful tool for exploratory data analysis, it faces several challenges:\n\nDetermining the Number of Clusters: Many algorithms require the number of clusters to be specified beforehand, which is not always obvious and may require domain knowledge or additional methods like the elbow method or silhouette analysis.\nDealing with High Dimensionality: As the dimensionality of the data increases, the distance between data points becomes less meaningful, a phenomenon known as the “curse of dimensionality.” Dimensionality reduction techniques may be necessary before clustering.\nSensitivity to Noise and Outliers: Clustering algorithms can be sensitive to noise and outliers, which can significantly affect the formation of clusters. Preprocessing steps to remove or reduce the influence of outliers are often necessary."
  }
]