[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 5320 Predictive Analytics",
    "section": "",
    "text": "Introduction\nThese are the lecture notes for STA 5320 - Predictive Analytics.\nPrerequisites: Regression Analysis\n\nCourse Description:\nConcepts, methods, and tools used for predictive modeling and data analytics with applications are considered. The focus of this course is on advanced tools using various multivariate regression techniques, statistical modeling, machine learning, and simulation for forecasting. Practical applications are emphasized."
  },
  {
    "objectID": "01_Tidymodels.html#the-tidymodels-framework",
    "href": "01_Tidymodels.html#the-tidymodels-framework",
    "title": "1  Introduction to Tidymodels",
    "section": "1.1 The tidymodels Framework",
    "text": "1.1 The tidymodels Framework\nThe tidymodels framework brings together the principles of tidy data and the tidyverse, making it easy to integrate data preprocessing, model training, and evaluation into a unified workflow. Below, we’ll explore the core components of the tidymodels framework, each of which plays a vital role in the modeling process."
  },
  {
    "objectID": "01_Tidymodels.html#recipes",
    "href": "01_Tidymodels.html#recipes",
    "title": "1  Introduction to Tidymodels",
    "section": "1.2 recipes",
    "text": "1.2 recipes\nIn predictive analytics, the data we work with often needs to be cleaned and transformed before it can be used for modeling. The recipes package in tidymodels is designed to help automate and streamline this process. A recipe is a blueprint for data preprocessing, allowing you to specify how to transform raw data into a form suitable for modeling. This can include tasks like normalizing numerical variables, encoding categorical variables, handling missing values, and creating new features.\n\n1.2.1 The Core Structure of a Recipe\nA recipe starts by defining a formula, typically in the form of a dependent variable (outcome) and independent variables (predictors), followed by one or more step_ functions that specify the transformations to be applied to the data. These steps are applied in the order they are specified in the recipe, ensuring that your preprocessing is both consistent and repeatable.\nlibrary(recipes)\n\n# Define the recipe\nrecipe_obj = recipe(outcome ~ ., data = data) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_factor_predictors())\nIn this example, we define a recipe where outcome is the dependent variable and all other variables are predictors. We first normalize all numeric predictors using step_normalize(), and then create dummy variables for all factor predictors using step_dummy().\n\n\n1.2.2 Common step_ Functions\nThe recipes package includes several commonly used step_ functions to handle different data transformation tasks. Here are a few of the most frequently used steps:\n\nstep_normalize()\nThe step_normalize() function is used to standardize numeric predictors by scaling them so that they have a mean of 0 and a standard deviation of 1. This is particularly useful when predictors vary greatly in scale, such as when comparing different player statistics like points, assists, and rebounds. Normalization ensures that all variables contribute equally to the model.\n\n\nstep_dummy()\nThe step_dummy() function is used to convert categorical variables (factors) into dummy (binary) variables, which are required for most machine learning models.\n\n\nstep_naomit()\nThe step_naomit() function is used to remove rows with missing values. In sports datasets, it’s common to encounter missing data, and this function can help clean the data by excluding those rows. This step is often used when missing data can’t be easily imputed or when the model cannot handle missing values.\n\n\nstep_center()\nThe step_center() function centers numeric variables by subtracting the mean of each variable from the values in that variable. This can be useful when you want to ensure that all variables are centered around zero, particularly when working with models that are sensitive to the scale of the data, such as linear regression or neural networks.\n\n\nstep_interact()\nThe step_interact() function creates interaction terms between two or more variables. Interaction terms can be important in predictive analytics, where the effect of one variable might depend on the value of another variable. This step allows you to automatically create these interaction terms.\n\n\nstep_zv()\nThe step_zv() function removes any predictors with zero variance, meaning those variables that do not vary at all across the data (e.g., a column where all the values are the same). These variables do not provide any useful information for the model and can safely be removed.\n\n\n\n1.2.3 Combining Steps\nYou can combine multiple step_ functions in a single recipe to handle a variety of preprocessing tasks. This ensures that all the necessary transformations are applied consistently across both training and testing datasets.\n\n\n1.2.4 Applying the Recipe\nOnce a recipe is defined, it can be applied to the data using the prep() function, which prepares the recipe by applying the specified transformations to the dataset. After the recipe is prepared, it can be used to bake (i.e., apply the preprocessing steps) to both training and test datasets:\nprepped_recipe = prep(recipe_obj, training = player_data)\n\n# Apply the transformations to the training and test datasets\ntrain_data = bake(prepped_recipe, new_data = player_data)\ntest_data = bake(prepped_recipe, new_data = player_data)"
  },
  {
    "objectID": "01_Tidymodels.html#parsnip",
    "href": "01_Tidymodels.html#parsnip",
    "title": "1  Introduction to Tidymodels",
    "section": "1.3 parsnip",
    "text": "1.3 parsnip\nThe parsnip package in tidymodels provides a unified interface for specifying machine learning models in R. It allows you to define the type of model you want to fit (such as linear regression, decision trees, or random forests) without worrying about the underlying implementation details. This consistency across different model types makes it easy to switch between models or experiment with multiple models within the same framework.\nThe power of parsnip lies in its simplicity and flexibility. You only need to specify the model type and the engine (the underlying R package or algorithm used to fit the model), and parsnip handles the rest. This approach not only simplifies the modeling process but also allows you to experiment with different modeling techniques using the same set of tools and functions.\n\n1.3.1 Specifying Models with parsnip\nWhen using parsnip, you first define the type of model you want to create. This is done using a function for the specific model, such as linear_reg() for linear regression or rand_forest() for random forests. Once the model type is specified, you set the engine, which determines how the model is fitted (e.g., using lm for linear regression or rpart for decision trees). The next step is to fit the model to the data.\nHere’s an example of how to specify and fit a linear regression model:\nlibrary(parsnip)\n\n# Specify a linear regression model\nmodel = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n# Fit the model to the data\ntrained_model = model |&gt;\n  fit(outcome ~ x1 + x2, data = dat)\nIn this example, linear_reg() specifies a linear regression model, and set_engine(\"lm\") tells parsnip to use the lm engine to fit the model. The fit() function then trains the model using the dat dataset, predicting the outcome variable from the x1 and x2 predictors.\n\n\n1.3.2 Common Models in parsnip\nHere are some of the most common models you can specify using parsnip:\n\nLinear Regression (linear_reg)\nLinear regression is one of the simplest and most commonly used models for predicting a continuous outcome variable. In parsnip, the function linear_reg() is used to specify this model, and you can choose the engine (e.g., lm for the standard linear regression implementation or glmnet for regularized regression).\n# Linear regression model\nmodel_lr = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n\nLogistic Regression (logistic_reg)\nLogistic regression is commonly used when the outcome variable is binary (e.g., win/loss, success/failure). In parsnip, logistic regression is specified using logistic_reg() and can be fitted using engines such as glm or spark.\n# Logistic regression model\nmodel_logistic = logistic_reg() |&gt;\n  set_engine(\"glm\")\n\n\nRandom Forest (rand_forest)\nRandom forests are a powerful ensemble learning method that can be used for both classification and regression. In parsnip, you can specify a random forest model using the rand_forest() function. You can choose the engine (e.g., ranger or randomForest) to fit the model.\n# Random forest model\nmodel_rf = rand_forest() |&gt;\n  set_engine(\"ranger\")\n\n\nDecision Trees (decision_tree)\nDecision trees are a non-linear model that can be used for both classification and regression tasks. In parsnip, decision trees are specified with decision_tree(), and the engine can be set to rpart or party for different implementations.\n# Decision tree model\nmodel_tree = decision_tree() |&gt;\n  set_engine(\"rpart\")\n\n\nSupport Vector Machines (svm_rbf)\nSupport Vector Machines (SVM) are used for both classification and regression tasks and are particularly effective in high-dimensional spaces. In parsnip, you can specify an SVM model using svm_rbf(), and you can choose engines like kernlab or e1071.\n# SVM model with radial basis function kernel\nmodel_svm = svm_rbf() |&gt;\n  set_engine(\"kernlab\")\n\n\nBoosted Trees (boost_tree)\nBoosted trees, such as Gradient Boosting Machines (GBM), are another powerful ensemble method that can be used for both classification and regression. In parsnip, you can specify a boosted tree model using boost_tree() and set the engine to xgboost or lightgbm.\n# Boosted tree model\nmodel_boost = boost_tree() |&gt;\n  set_engine(\"xgboost\")\n\n\n\n1.3.3 Tuning Models\nMany models, such as random forests, SVMs, and boosted trees, have hyperparameters that can be tuned to improve model performance. parsnip provides a consistent interface for setting and tuning these hyperparameters. After specifying a model, you can use the tune package to perform grid search or random search to find the optimal parameters.\nFor example, to tune a random forest model, you can use the tune_grid() function to search over different values for the number of trees (trees) and the maximum number of splits (min_n):\nlibrary(tune)\n\n# Define a grid of hyperparameters\ngrid = grid_regular(trees(range = c(100, 1000)), min_n(range = c(2, 20)), levels = 5)\n\n# Tune the model\ntuned_rf = tune_grid(\n  trained_model_rf,\n  resamples = vfold_cv(dat),\n  grid = grid\n)\nThis code runs a grid search over the number of trees and the minimum number of splits to optimize the performance of the random forest model."
  },
  {
    "objectID": "01_Tidymodels.html#workflows",
    "href": "01_Tidymodels.html#workflows",
    "title": "1  Introduction to Tidymodels",
    "section": "1.4 workflows",
    "text": "1.4 workflows\nThe workflows package in tidymodels is designed to streamline the process of building, training, and evaluating machine learning models by combining the different stages of the modeling process into a single, unified object. This object allows you to encapsulate the entire modeling pipeline, from preprocessing steps to model specification, into a single workflow. Using workflows ensures that your data preprocessing and modeling steps are tightly integrated and that the entire pipeline is reusable, reproducible, and easy to manage.\nThe power of workflows lies in its ability to combine preprocessing (via recipes) with model specification (via parsnip) into a cohesive object that can be easily trained, evaluated, and tuned. This simplifies the process of applying a series of steps to both training and testing data, making your model development process more efficient and organized.\n\n1.4.1 The Structure of a Workflow\nA workflow consists of two main components: a recipe and a model. The recipe defines the preprocessing steps (e.g., data normalization, encoding, etc.), while the model specifies the type of machine learning model to be fit (e.g., linear regression, random forest, etc.). Once the recipe and model are combined into a workflow, the workflow object can be used to fit, evaluate, and tune the model.\nHere’s how you can create a simple workflow:\nlibrary(workflows)\n\n# Define the recipe\nrecipe_obj = recipe(outcome ~ x1 + x2, data = dat) |&gt;\n  step_normalize(x1) |&gt;\n  step_dummy(x2)\n\n# Define the model\nmodel_obj = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n# Create the workflow\nworkflow_obj = workflow() |&gt;\n  add_recipe(recipe_obj) |&gt;\n  add_model(model_obj)\n\n# Fit the workflow to the data\nfitted_workflow = workflow_obj |&gt;\n  fit(data = dat)\nIn this example, we first define a recipe that normalizes the points, assists, and rebounds variables, as well as creates dummy variables for the team variable. We then specify a linear regression model using linear_reg(). Finally, we combine the recipe and model into a single workflow object using workflow(), and the fit() function is used to train the model on the dat dataset.\n\n\n1.4.2 Adding Recipes and Models to a Workflow\nYou can easily add both the recipe and the model to a workflow using the add_recipe() and add_model() functions, respectively. Once these components are added, the workflow object is ready for training, evaluation, and tuning.\n# Add the recipe and model to the workflow\nworkflow_obj = workflow() |&gt;\n  add_recipe(recipe_obj) |&gt;\n  add_model(model_obj)\nThis step ensures that your workflow is built with the appropriate preprocessing steps and model, making it ready to be applied to your data.\n\n\n1.4.3 Tuning a Workflow\nOne of the key benefits of using workflows is that you can integrate tuning directly into the workflow process. For instance, when tuning hyperparameters for models like random forests or support vector machines, you can pass the workflow to the tune_grid() function along with a grid of hyperparameters to optimize.\nlibrary(tune)\n\n# Define a grid of hyperparameters\ngrid = grid_regular(trees(range = c(100, 1000)), min_n(range = c(2, 20)), levels = 5)\n\n# Tune the workflow\ntuned_workflow = tune_grid(\n  workflow_obj,\n  resamples = vfold_cv(dat),\n  grid = grid\n)\nIn this example, we use tune_grid() to tune a random forest model by varying the number of trees (trees) and the minimum number of splits (min_n). The grid search is performed over these hyperparameters using cross-validation, ensuring that the best-performing model configuration is selected.\n\n\n1.4.4 Evaluating a Workflow\nOnce the workflow is trained, you can evaluate its performance using the yardstick package, which provides a consistent set of functions for model evaluation. For instance, after fitting the workflow, you can generate predictions on a test set and calculate metrics such as accuracy, RMSE, or AUC.\nlibrary(yardstick)\n\n# Generate predictions\npredictions = predict(fitted_workflow, new_data = test_data)\n\n# Evaluate the performance\naccuracy(predictions, truth = test_data$outcome)\nHere, the predict() function is used to generate predictions from the fitted workflow, and the accuracy() function is used to calculate the accuracy of the model’s predictions on the test data.\n\n\n1.4.5 Workflow and Resampling\nWorkflows can also be integrated with resampling methods to evaluate the model’s performance across different subsets of the data. For example, you can use rsample to perform cross-validation or bootstrapping and pass the resampled data to the workflow for evaluation.\nlibrary(rsample)\n\n# Create a 5-fold cross-validation resampling object\ncv_folds = vfold_cv(dat, v = 5)\n\n# Tune the workflow with cross-validation\ntuned_workflow_cv = tune_grid(\n  workflow_obj,\n  resamples = cv_folds,\n  grid = grid\n)\nThis example uses vfold_cv() to create a cross-validation object, and the workflow is tuned using this resampling method to get a better estimate of the model’s performance across different data splits."
  },
  {
    "objectID": "02_ML.html#what-is-machine-learning",
    "href": "02_ML.html#what-is-machine-learning",
    "title": "2  Introduction to Machine Learning",
    "section": "2.1 What is Machine Learning?",
    "text": "2.1 What is Machine Learning?\nMachine Learning (ML) is a branch of artificial intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to perform specific tasks without being explicitly programmed. Instead of relying on manually coded instructions, ML algorithms learn patterns from data, improving their performance over time as they process more examples. The core concept of machine learning is to create models that can learn from past data and make predictions or decisions based on that information.\nIn business, ML applications can range from predicting customer churn and optimizing marketing campaigns to forecasting sales and managing risk.\nThe process of machine learning begins with data collection, followed by cleaning and preparing the data for analysis. Once the data is ready, an appropriate machine learning model is chosen based on the type of problem to be solved (e.g., classification or regression). The model is then trained using a training dataset and evaluated on a validation set to ensure it can make accurate predictions on new, unseen data."
  },
  {
    "objectID": "02_ML.html#why-opt-for-machine-learning",
    "href": "02_ML.html#why-opt-for-machine-learning",
    "title": "2  Introduction to Machine Learning",
    "section": "2.2 Why Opt for Machine Learning?",
    "text": "2.2 Why Opt for Machine Learning?\nMachine learning offers a powerful approach for solving complex problems in predictive analytics where traditional methods often fall short. One of the key strengths of machine learning is its ability to improve predictions and decision-making as more data becomes available. Unlike traditional programming, where explicit rules are coded, ML models learn from data, continually refining their accuracy as they are exposed to new information.\nIn predictive analytics, machine learning can handle the complexity and large volumes of data generated by modern technologies. Traditional statistical methods may struggle to identify subtle patterns in these large datasets, but ML algorithms are designed to extract valuable insights from vast and often noisy data sources."
  },
  {
    "objectID": "02_ML.html#machine-learning-applications",
    "href": "02_ML.html#machine-learning-applications",
    "title": "2  Introduction to Machine Learning",
    "section": "2.3 Machine Learning Applications",
    "text": "2.3 Machine Learning Applications\nMachine Learning applications are vast and varied, including image and speech recognition, natural language processing, and more. These tasks employ different techniques, such as neural networks and deep learning, to analyze images, understand speech and language, or forecast trends.\n\nImage and Speech Recognition\nMachine learning algorithms, particularly those in deep learning, have dramatically improved the accuracy of image and speech recognition systems. In image recognition, ML models are trained with vast datasets of images to recognize patterns, objects, and even faces with remarkable precision. This technology underpins applications ranging from security systems that use facial recognition to authenticate identities, to medical imaging software that assists in diagnosing diseases by identifying abnormal patterns in X-rays or MRI scans.\nSpeech recognition has similarly benefited from ML, enabling devices and software to understand and transcribe human speech with high accuracy. This technology is the backbone of virtual assistants like Siri and Alexa, real-time translation services, and accessibility tools for those with disabilities, making technology more accessible and interactive.\n\n\nNatural Language Processing (NLP)\nNLP uses machine learning to understand, interpret, and generate human language in a way that is both meaningful and useful. Applications of NLP include chatbots and virtual assistants that can understand and respond to user queries in natural language, sentiment analysis tools that gauge public opinion from social media content, and machine translation systems that break down language barriers by translating text from one language to another.\n\n\nAutonomous Vehicles\nMachine learning plays a crucial role in the development of autonomous vehicles (AVs). By processing data from various sensors and cameras, ML algorithms help AVs understand their surroundings, make decisions in real-time, and navigate safely through complex environments. This technology promises to revolutionize transportation by reducing human error, enhancing traffic efficiency, and increasing accessibility for those unable to drive.\n\n\nPredictive Analytics\nIn industries ranging from finance to healthcare, ML is used for predictive analytics, leveraging historical data to predict future trends, behaviors, and outcomes. In finance, ML models predict stock market trends, assess credit risk, and detect fraudulent activities. In healthcare, predictive analytics can forecast disease outbreaks, patient admissions, and potential complications, improving patient care and operational efficiency.\n\n\nPersonalization and Recommendation Systems\nMachine learning drives the personalized experiences users have come to expect from online platforms. By analyzing user behavior, preferences, and interactions, ML algorithms can tailor content, recommendations, and services to each user. This technology powers the recommendation engines behind streaming services like Netflix and Spotify, e-commerce platforms like Amazon, and social media feeds, enhancing user engagement and satisfaction.\n\n\nRobotics and Automation\nML is also critical in robotics, where it enables robots to learn from their environment and experience, adapt to new tasks, and perform complex actions with a degree of autonomy. This has applications in manufacturing, where robots can assist in or automate assembly lines, in logistics for warehouse automation, and in service robots that assist in homes and healthcare settings.\nMachine learning applications are diverse and continually evolving, touching nearly every aspect of modern life. From improving how we communicate and access information to enhancing healthcare, financial services, and transportation, ML technologies are at the forefront of digital innovation. As machine learning continues to advance, we can expect its applications to expand further, creating new opportunities and solving challenges in ways we have yet to imagine."
  },
  {
    "objectID": "02_ML.html#types-of-machine-learning-systems",
    "href": "02_ML.html#types-of-machine-learning-systems",
    "title": "2  Introduction to Machine Learning",
    "section": "2.4 Types of Machine Learning Systems",
    "text": "2.4 Types of Machine Learning Systems\nMachine learning systems can be categorized based on the approach they take to learn from data. These categories include supervised versus unsupervised learning, batch versus online learning, and instance-based versus model-based learning. Understanding the differences between these types of systems is essential for applying machine learning effectively in predictive analytics. In this section, we’ll explore these distinctions.\n\n2.4.1 Supervised vs. Unsupervised Learning\n\nSupervised Learning\nIn supervised learning, the algorithm learns from a labeled dataset, where each example in the training data is paired with the correct output. The model is trained to map input data to an output variable. This process is called supervised learning because the model is “supervised” during training by the known labels, guiding the algorithm to make accurate predictions.\nSupervised learning is typically used for two types of problems in predictive analytics: classification and regression. For example:\n\nClassification: Predicting categorical outcomes.\nRegression: Predicting continuous outcomes.\n\n\n\nUnsupervised Learning\nIn contrast to supervised learning, unsupervised learning works with data that does not have labels. The goal is to find hidden patterns, relationships, or groupings within the data without any predefined outcome variable. Unsupervised learning is often used for clustering or dimensionality reduction. Applications include customer segmentation, market basket analysis, and anomaly detection.\n\n\n\n2.4.2 Batch vs. Online Learning\n\nBatch Learning\nBatch learning refers to training a machine learning model on the entire dataset at once. This approach is typically used when the dataset is static and does not change frequently. Once the model is trained, it is deployed to make predictions, and it will not learn or adapt until it is retrained with a new batch of data.\nWhile batch learning is effective for static datasets, it can be inefficient when new data arrives continuously, as the model requires retraining on the entire dataset.\n\n\nOnline Learning\nOnline learning, also known as incremental learning, allows the model to learn continuously as new data becomes available. Instead of retraining the entire model on the full dataset, online learning updates the model incrementally with each new data point. This approach is useful for real-time applications where data is continuously generated, and the model needs to adapt quickly to new patterns or trends.\n\n\n\n2.4.3 Instance-Based vs. Model-Based Learning\n\nInstance-Based Learning\nInstance-based learning is a method where the algorithm memorizes the training data and makes predictions by comparing new data points to the stored instances. The key idea is that similar inputs will have similar outputs, so the model looks for the most similar previous instances to make a prediction. Instance-based learning does not explicitly build a general model but rather relies on the “memory” of previous examples.\nInstance-based learning is particularly useful in situations where it is difficult to derive a formal model and when the relationships between data points are highly local rather than global.\n\n\nModel-Based Learning\nIn contrast to instance-based learning, model-based learning involves creating a general model that learns the relationships between input features and output predictions. These models generalize from the training data to make predictions on unseen data. Examples of model-based learning include decision trees, linear regression, and neural networks.\nModel-based learning is more efficient than instance-based learning for making predictions on new data since the model has learned the underlying patterns and does not need to search through the entire dataset."
  },
  {
    "objectID": "02_ML.html#challenges-in-machine-learning",
    "href": "02_ML.html#challenges-in-machine-learning",
    "title": "2  Introduction to Machine Learning",
    "section": "2.5 Challenges in Machine Learning",
    "text": "2.5 Challenges in Machine Learning\nWhile machine learning offers powerful tools for analyzing and predicting outcomes, it also presents several challenges that must be addressed to build accurate and effective models. These challenges can arise at various stages of the machine learning process, from data collection and preparation to model training and evaluation. In this section, we explore some of the key challenges in machine learning and how they apply to data.\n\n2.5.1 Sufficient and Representative Training Data\nOne of the most important factors in building a successful machine learning model is having access to sufficient and representative training data. For a model to learn effectively, it must be exposed to a wide variety of examples that reflect the real-world scenarios it will encounter.\nSufficient Data: Having a large enough dataset is essential to capture the complexity of the data. Without enough data, a model may fail to learn the underlying patterns and may perform poorly when applied to new data.\nRepresentative Data: It’s not just the size of the data that matters, but also its diversity. If the dataset is not diverse enough, the model may fail to generalize and perform poorly in real-world scenarios.\n\n\n2.5.2 Poor-Quality Data\nData quality is another significant challenge in machine learning. Data can be noisy, incomplete, or inconsistent, and poor-quality data can significantly hinder a model’s ability to make accurate predictions. This issue is especially common in real-time data collection.\nTo address these issues, it’s important to clean and preprocess the data before training the model. This can involve filling in missing values, correcting errors, and smoothing noisy data.\n\n\n2.5.3 Selecting Relevant Features\nFeature selection—the process of identifying the most important variables or attributes to include in the model—is another critical challenge. Occasionally, the number of potential features can be vast. Determining which features will provide the most valuable information for the model requires careful thought and experimentation.\nUsers often use techniques like feature importance analysis or recursive feature elimination to identify the most relevant features and ensure that only the most informative variables are included in the model.\n\n\n2.5.4 Overfitting and Underfitting\nOverfitting and underfitting are two common pitfalls in machine learning that occur when a model either learns the training data too well or fails to learn enough from it.\nOverfitting occurs when a model learns not only the underlying patterns in the training data but also the noise and random fluctuations. As a result, the model performs exceptionally well on the training data but fails to generalize to new, unseen data.\nUnderfitting, on the other hand, occurs when a model is too simple and fails to capture the underlying patterns in the data. This typically happens when the model is not complex enough to handle the intricacies of the dataset. To strike the right balance between overfitting and underfitting, users must choose models with an appropriate level of complexity and use techniques like hyperparameter tuning and regularization to refine model performance.\n\n\n2.5.5 Data Imbalance\nIn many datasets, certain classes of data may be underrepresented, leading to imbalanced datasets. To address this issue, analysts can use techniques like oversampling the minority class, undersampling the majority class, or employing specialized algorithms designed to handle imbalanced data, such as SMOTE (Synthetic Minority Over-sampling Technique).\n\n\n2.5.6 Real-Time Data Challenges\nProcessing and analyzing real-time data can be challenging, especially when decisions need to be made instantly. Machine learning models must be fast and efficient, capable of processing data and providing actionable insights within seconds. This requires sophisticated infrastructure and algorithms capable of handling large volumes of data quickly and accurately, often under pressure."
  },
  {
    "objectID": "02_ML.html#testing-and-validation",
    "href": "02_ML.html#testing-and-validation",
    "title": "2  Introduction to Machine Learning",
    "section": "2.6 Testing and Validation",
    "text": "2.6 Testing and Validation\nTesting and validation are essential stages in the development of machine learning models, ensuring that the models generalize well to new, unseen data. Without proper testing and validation, it is difficult to assess how well a model will perform in real-world situations, where data can vary significantly from the training set. This section will cover key testing and validation techniques used in predictive analytics, including data splitting, cross-validation, and hyperparameter tuning.\n\n2.6.1 Data Splitting\nA foundational step in model evaluation is splitting the dataset into different subsets: the training set, the validation set, and the test set. Each set plays a crucial role in ensuring that the model performs well on unseen data, preventing issues like overfitting.\n\nTraining Set: The training set is used to fit the machine learning model. It contains both the input data and the corresponding target variable. The model learns from this data by identifying patterns and relationships.\nValidation Set: The validation set is used to tune the model’s parameters and help in model selection. This set allows analysts to evaluate the model’s performance during the training process and make adjustments, such as selecting the best algorithm or optimizing hyperparameters, to improve accuracy.\nTest Set: The test set is used to assess the final model’s performance once it has been trained and tuned. This set is completely separate from the training and validation sets, ensuring that the model is evaluated on new, unseen data.\n\nThe goal of splitting the data is to avoid “data leakage,” where information from the test set accidentally influences the model’s training, leading to overly optimistic results. By ensuring that each subset is used for its intended purpose, analysts can better estimate the model’s true performance.\n\n\n2.6.2 Cross-Validation\nCross-validation is a more advanced technique for assessing a model’s effectiveness, particularly when the dataset is limited in size. The most common form of cross-validation is k-fold cross-validation, where the dataset is randomly divided into k equal-sized parts, or “folds.” The model is then trained on k-1 of these folds and tested on the remaining fold. This process is repeated k times, each time with a different fold held out for testing, and the performance across all folds is averaged.\nBy using multiple validation sets, analysts ensure that the model’s performance is not dependent on any one particular subset of data and can provide a more reliable estimate of its ability to generalize.\nBenefits of Cross-Validation: Cross-validation is particularly useful when there are not enough data points to reliably train and test a model. By using each fold for both training and testing, cross-validation provides a more accurate estimate of a model’s performance across all data points, making the results less sensitive to random fluctuations in the data.\n\n\n2.6.3 Hyperparameter Tuning\nMachine learning models often have parameters, called hyperparameters, that control the learning process. These parameters can significantly influence model performance, and selecting the right values for them is critical for building an effective model.\nFor example, in a decision tree model, hyperparameters such as the maximum depth of the tree or the minimum samples per leaf can be adjusted to control the complexity of the model. In predictive analytics, these hyperparameters might affect how a model predicts outcomes. The goal is to find the best combination of hyperparameters that results in the most accurate predictions on unseen data.\nThere are several techniques for hyperparameter tuning, including:\n\nGrid Search: Grid search involves specifying a range of values for each hyperparameter and exhaustively trying all combinations to find the optimal settings. For example, when training a random forest model to predict performance, a grid search might test different values for the number of trees and the depth of each tree.\nRandom Search: Random search, in contrast, randomly samples hyperparameter combinations from a defined search space. While it may not explore every possible combination, it is often more efficient than grid search, especially when dealing with a large number of hyperparameters.\nBayesian Optimization: Bayesian optimization uses a probabilistic model to predict which combinations of hyperparameters are most likely to yield the best results. This technique is more efficient than grid and random search, as it focuses the search on promising areas of the hyperparameter space.\n\nHyperparameter tuning is a crucial step in improving the accuracy and performance of machine learning models. By experimenting with different settings and evaluating the results on the validation set, analysts can refine their models and ensure that they perform optimally on real-world data.\n\n\n2.6.4 Evaluating Model Performance\nOnce the model has been trained and tuned, it is essential to evaluate its performance using a variety of metrics. The choice of performance metrics depends on the type of model and the problem being solved.\n\nClassification Metrics: For classification tasks, common evaluation metrics include:\n\nAccuracy: The percentage of correct predictions out of all predictions made.\nPrecision: The percentage of true positive predictions among all predicted positives.\nRecall: The percentage of true positive predictions among all actual positives.\nF1-Score: The harmonic mean of precision and recall, providing a single metric that balances the two.\n\nRegression Metrics: For regression tasks, typical evaluation metrics include:\n\nMean Absolute Error (MAE): The average absolute difference between the predicted and actual values.\nMean Squared Error (MSE): The average squared difference between the predicted and actual values, with larger errors penalized more heavily.\nR-squared: A measure of how well the model explains the variance in the data.\n\n\n\n\n2.6.5 Validation Techniques for Time-Series Data\nIn predictive analytics, time-series data is common, especially when predicting future outcomes based on historical data. Standard cross-validation techniques can sometimes be inappropriate for time-series data, as they do not account for the temporal order of observations.\nTo properly evaluate models using time-series data, analysts can employ time-based splitting or forward-chaining techniques:\n\nTime-Based Splitting: The dataset is split based on time, with the training set using past data and the test set using future data. This ensures that the model is tested on data that reflects future events, which is important for forecasting tasks.\nForward-Chaining: In forward-chaining, the training set is progressively expanded to include new data points, while the test set always consists of the most recent data. This simulates the real-world scenario of making predictions as new data arrives."
  },
  {
    "objectID": "02_ML.html#navigating-machine-learning",
    "href": "02_ML.html#navigating-machine-learning",
    "title": "2  Introduction to Machine Learning",
    "section": "2.7 Navigating Machine Learning",
    "text": "2.7 Navigating Machine Learning\nUnderstanding machine learning (ML) involves not only mastering the technical aspects of algorithms and data manipulation but also recognizing when and how to apply these methods effectively. To successfully navigate the complexities of machine learning, it is essential to stay informed about ongoing advancements, collaborate across disciplines, address ethical concerns, and manage the deployment of models.\n\n2.7.1 Staying Updated with Advances\nThe field of machine learning is rapidly evolving, with new algorithms, techniques, and best practices emerging regularly. As data becomes more sophisticated and computational power continues to increase, it’s important for analysts to stay up to date with the latest developments in machine learning. This includes exploring emerging models, such as deep learning, that can process complex.\nTo keep up with advancements in machine learning, analysts should:\n\nFollow academic research and industry publications related to thier area of study.\nParticipate in online courses, workshops, and conferences to learn new techniques and tools.\nExperiment with new libraries and frameworks, such as TensorFlow or PyTorch, that are pushing the boundaries of what’s possible in predictive analytics.\n\nBy staying current with the latest machine learning techniques, analysts can continue to push the boundaries of predictive analytics.\n\n\n2.7.2 Ethical Considerations and Bias Mitigation\nAs with all fields involving data, machine learning in predictive analytics comes with its ethical considerations. It is crucial to ensure that machine learning models are developed and deployed fairly, avoiding unintended consequences like bias, discrimination, or privacy violations.\nFor instance, if a model is trained using biased historical data, such as skewed customer data, the model might make inaccurate predictions that disadvantage customers.\nTo mitigate bias, analysts should:\n\nEnsure that training data is diverse and representative of the scenarios it is meant to model.\nRegularly test models for fairness and adjust them if they show bias against certain groups.\nBe transparent about the data and assumptions used in the model, particularly when these models are used for decision-making.\n\n\n\n2.7.3 Collaboration Across Disciplines\nMachine learning is an interdisciplinary field that requires collaboration between domain experts, data scientists, and engineers. In predictive analytics, this collaboration is essential because machine learning models must be contextualized within the specific domain. For example, while a data scientist may have the technical skills to build a predictive model, a domain expert can provide valuable insights into the nuances of the area of research.\nBy working together, interdisciplinary teams can create more relevant, impactful machine learning models that support better decision-making.\n\n\n2.7.4 Scalability and Deployment\nBuilding machine learning models is only one part of the process. The real challenge lies in deploying these models into real-world settings where they can provide ongoing value. This often means deploying models that can handle real-time data.\nScalability is a key consideration when deploying machine learning models. A model trained on historical data might perform well in a controlled environment, but when deployed in real-time, it must be able to handle streaming data, process new information quickly, and update predictions in near real-time.\nTo ensure successful deployment, analysts should:\n\nWork closely with engineers and software developers to ensure that models can be integrated into existing systems used by coaches and managers.\nTest models in a live setting to ensure that they can handle the volume and velocity of real-time data.\nContinuously monitor model performance after deployment to identify potential issues and make necessary adjustments."
  },
  {
    "objectID": "03_Logistic.html#key-concepts-of-logistic-regression",
    "href": "03_Logistic.html#key-concepts-of-logistic-regression",
    "title": "\n3  Logistic Regression\n",
    "section": "\n3.1 Key Concepts of Logistic Regression:",
    "text": "3.1 Key Concepts of Logistic Regression:\nBinary Outcome: Binary outcomes are central to the concept of logistic regression, a statistical method tailored for scenarios where the outcome variable is dichotomous, meaning it has two possible states such as “yes” or “no,” “success” or “failure,” or is represented numerically as 0 and 1. This characteristic is pivotal for modeling and analyzing situations where the response is bifurcated and does not adhere to a continuous scale. Logistic regression excels in these settings by estimating the probability that a given input set belongs to a particular category (e.g., success or failure).\nExamples of Binary Outcome Applications\n\nMedical Diagnosis: In the medical field, logistic regression can be used to predict the presence or absence of a disease based on various predictors, such as age, sex, blood pressure, cholesterol levels, and other clinical measurements. For instance, it could model the likelihood of a patient having heart disease, where 1 represents the presence of heart disease and 0 represents its absence.\nEmail Spam Filtering: This application involves classifying emails as spam (1) or not spam (0), based on features extracted from the emails, such as the frequency of certain words, the presence of specific phrases, or the sender’s details. Logistic regression can be trained on a dataset of emails that have been manually classified to predict the categorization of new, unseen emails.\nCredit Approval Processes: Financial institutions often use logistic regression to predict the probability of a loan applicant defaulting on a loan. The binary outcome would be default (1) or no default (0), with predictors including credit score, income level, employment history, and other financial metrics.\nCustomer Churn Prediction: Companies use logistic regression to identify the likelihood of a customer discontinuing service or subscription, where 1 indicates churn (the customer leaves) and 0 indicates retention (the customer stays). Predictive factors might include usage patterns, customer service interactions, payment history, and satisfaction levels.\nPolitical Election Outcomes: In the context of binary electoral outcomes (win or lose), logistic regression can analyze polling data to predict whether a candidate will win (1) or lose (0) an election. The model might consider variables such as voter demographics, campaign spending, and historical voting patterns.\n\nThese examples underscore the versatility and practicality of logistic regression in handling binary outcomes across various domains, providing valuable insights and predictions that aid in decision-making processes.\nOdds Ratio\nIn logistic regression, the odds ratio is used to measure the association between a predictor variable and the probability of a particular outcome. Specifically, the odds ratio (OR) quantifies how the odds of the outcome change with a one-unit increase in the predictor variable, assuming all other variables in the model are held constant.\nThe odds of an event is defined as the probability of the event occurring divided by the probability of the event not occurring. Mathematically, if \\(p\\) represents the probability of the event, then the odds are given by \\(\\frac{p}{1-p}\\).\nAn odds ratio of:\n\n\nOR = 1 suggests there is no association between the predictor and the outcome.\n\nOR &gt; 1 indicates that the event is more likely to occur as the predictor increases.\n\nOR &lt; 1 suggests that the event is less likely to occur as the predictor increases.\n\nExamples of Odds Ratio\n\nMedical Research: Suppose a study investigates the effect of a new drug on reducing the risk of a disease. If the odds of the disease in the group receiving the drug are 0.5 and the odds in the placebo group are 1.0, the odds ratio is 0.5. This means the odds of getting the disease are 50% lower in the drug group compared to the placebo group.\nEducational Study: In an educational context, consider a study examining the effect of a special tutoring program on passing an exam. If students in the tutoring program have odds of passing the exam of 3 (i.e., they are three times as likely to pass than fail) and the odds for students not in the program are 1 (equal likelihood of passing and failing), the odds ratio is 3. This suggests that students in the tutoring program are three times as likely to pass the exam compared to those not in the program.\nMarket Research: A market research might explore the influence of customer satisfaction on the likelihood of repeat purchases. If the odds of making a repeat purchase for satisfied customers are 2 (twice as likely to repurchase as not) and for unsatisfied customers the odds are 0.5, the odds ratio would be 4 (2 divided by 0.5). This indicates that satisfied customers are four times as likely to make a repeat purchase compared to unsatisfied customers.\n\nUnderstanding and correctly interpreting the odds ratio in logistic regression allows for meaningful insights into the relationship between variables and outcomes. By quantifying how changes in predictor variables affect the odds of a particular outcome, researchers and practitioners can make informed decisions and predictions across various fields including medicine, education, and business.\nSigmoid Curve\nThe Sigmoid Curve, characterized by its S-shaped curve, is used in logistic regression to model the probability that a given input belongs to a particular category (e.g., success or failure). This curve ranges between 0 and 1, making it an effective tool for representing probabilities in binary outcomes. The formula for the logistic function is:\n\\[\nP(Y=1) = \\frac{1}{1 + e^{-(\\beta_0 +        \\beta_1X)}}\n\\]\nwhere:\n\n\n\\(P(Y=1)\\) is the probability that the dependent variable equals 1 (e.g., success, presence of a condition).\n\n\\(e\\) is the base of the natural logarithm.\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are the coefficients that the model estimates.\n\n\\(X\\) is the independent variable.\n\nThe logistic function essentially transforms linear combinations of predictors into probabilities by taking values from \\(-\\infty\\) to \\(+\\infty\\) and mapping them into a (0, 1) range.\nThe function that links the linear combination to the sigmoid curve is called the link function. There are a number of possible link functions that could be used. The one we will use (and the one presented in the equation above) is called the logit link function.\nEstimation\nIn logistic regression, the estimation of model coefficients is a crucial step that directly influences the accuracy and interpretability of the model’s predictions. The process relies on Maximum Likelihood Estimation (MLE), a statistical method used to estimate the parameters of a statistical model. MLE works by finding the parameter values that maximize the likelihood function, which measures how probable the observed data is, given a set of parameter values.\nThe likelihood function for a logistic regression model is a product of individual probabilities, each representing the likelihood of observing a particular outcome (e.g., success or failure) for a given set of predictor values. The goal of MLE is to adjust the model’s coefficients (parameters) such that this product of probabilities (likelihood) is maximized across all observations in the dataset.\nA practical way to visualize the MLE process is by plotting the likelihood function over a range of values for a model coefficient. Initially, the likelihood might be low for arbitrary coefficient values. However, as the MLE algorithm iterates, adjusting the coefficients to maximize the likelihood, we can observe the likelihood function reaching a peak. This peak represents the maximum likelihood estimates of the model coefficients, providing the best fit to the observed data.\nBy employing MLE, logistic regression models can accurately estimate the probability of binary outcomes based on a set of predictors. This method’s ability to find the most likely parameter values for a given dataset makes it a powerful tool in the hands of data analysts and researchers across various fields.\nInterpretation\nIn logistic regression, the coefficients obtained from the model provide the log odds of the dependent variable being 1 (for binary outcomes) for a one-unit increase in the predictor variable, keeping all other predictors constant. This log odds can be a bit abstract, so it’s common to convert these log odds into odds ratios by exponentiating the coefficients. This makes interpretation more intuitive:\n\nOdds Ratio (OR) &gt; 1: Indicates that the likelihood of the event (outcome=1) increases with a one-unit increase in the predictor.\nOdds Ratio (OR) &lt; 1: Suggests that the likelihood of the event decreases with a one-unit increase in the predictor.\nOdds Ratio (OR) = 1: Means there is no effect of the predictor on the likelihood of the event.\nExample 1: Health Research: Suppose a logistic regression model is developed to study the impact of smoking (predictor variable) on the likelihood of developing heart disease (outcome variable). If the coefficient for smoking is 0.5, exponentiating this gives an odds ratio of exp(0.5) ≈ 1.65. This can be interpreted as smokers having a 65% higher odds of developing heart disease compared to non-smokers, holding all other variables constant.\nExample 2: Marketing Analytics: Consider a model analyzing the impact of online ad exposure (predictor) on the probability of a purchase (outcome). If the coefficient for ad exposure is -0.3, the odds ratio is exp(-0.3) ≈ 0.74. This suggests that with each additional exposure to the ad, the odds of making a purchase decrease by 26%, assuming all other factors remain constant.\nExample 3: Education: A logistic regression could examine the effect of study hours (predictor) on passing an exam (outcome). If the coefficient for study hours is 0.2, the odds ratio is exp(0.2) ≈ 1.22. This means for each additional hour of study, the odds of passing the exam increase by 22%, with other factors held constant.\n\nInterpreting logistic regression coefficients through odds ratios provides valuable insights into the influence of predictors on an outcome. It helps in understanding which factors are more critical and how changes in these factors can increase or decrease the likelihood of the outcome. This makes logistic regression a powerful tool for decision-making in various fields such as health, marketing, and education, allowing practitioners to make informed decisions based on quantifiable evidence."
  },
  {
    "objectID": "03_Logistic.html#implementing-logistic-regression-with-tidymodels",
    "href": "03_Logistic.html#implementing-logistic-regression-with-tidymodels",
    "title": "\n3  Logistic Regression\n",
    "section": "\n3.2 Implementing Logistic Regression with tidymodels",
    "text": "3.2 Implementing Logistic Regression with tidymodels\nWhen implementing logistic regression with tidymodels, you typically follow these steps:\n\n\nData Preprocessing: Before fitting a logistic regression model, data must be cleaned and prepared. tidymodels offers the recipes package for this purpose, enabling feature engineering, normalization, and data splitting. A typical workflow might involve:\n\n\n\nCreating a recipe: Specify preprocessing steps such as normalization, dummy variable creation for categorical variables, and missing data imputation.\n\nData splitting: Use rsample to split the data into training and testing sets, ensuring that the model can be trained on one subset and validated on another to test its generalizability.\n\nExample: In this example, we’ll use the palmerpenguins dataset, specifically aiming to predict the species of penguins based on physical measurements. The dataset includes variables such as bill length, bill depth, flipper length, body mass, and sex.\nLoading the Dataset\nFirst, ensure the palmerpenguins package is installed and loaded, along with tidymodels.\n\nlibrary(palmerpenguins)\nlibrary(tidymodels)\n\nInitial Dataset Examination\nExamine the dataset to understand its structure and identify any preprocessing needs.\n\nglimpse(penguins)\n\nRows: 344\nColumns: 7\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n\n\nWe can see right away that there are some missing values. Let’s remove these observations. We will also make the species binary since we are doing logistic regression and species has three levels in this dataset. We will only model species as chinstrap or other (not chinstrap).\n\npenguins = drop_na(penguins)\npenguins = penguins |&gt; \n  mutate(\n    species = factor(if_else(species==\"Chinstrap\", \"Chinstrap\", \"Other\"))\n  )\nglimpse(penguins)\n\nRows: 333\nColumns: 7\n$ species           &lt;fct&gt; Other, Other, Other, Other, Other, Other, Other, Oth…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n\n\nCreating a Recipe\n\nDefine the outcome variable and predictors.\nSpecify preprocessing steps, such as normalization for numerical variables and encoding for categorical variables.\n\n\npenguin_recipe = recipe(species ~ bill_length_mm + bill_depth_mm +\n                           flipper_length_mm + body_mass_g + sex, \n                         data = penguins) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors())\n\nData Splitting\nSplit the data into training and testing sets to prepare for model training and evaluation.\n\nset.seed(123)\npenguin_split = initial_split(penguins, prop = 0.75)\npenguin_train = training(penguin_split)\npenguin_test = testing(penguin_split)\n\nPreparing Processed Data\nApply the recipe to the training data to prepare it for model fitting.\n\nprepared_data = prep(penguin_recipe, training = penguin_train) |&gt;\n  bake(new_data = NULL)\n\nBy following these steps, you will have effectively prepared your dataset for logistic regression analysis with tidymodels. This preprocessing includes normalization and encoding necessary for the logistic regression model to properly interpret and learn from the data.\n\n\nModel Specification with parsnip: After preparing the dataset, the next step in implementing logistic regression with tidymodels is to specify the model. We use the parsnip package for this purpose, which offers a unified interface for model specification across various modeling techniques, including logistic regression. The logistic_reg() function is utilized to specify a logistic regression model, which can be customized with engine-specific options according to the analysis needs.\n\nExample: Continuing with the palmerpenguins dataset example, we aim to predict the species of a penguin based on measurements like bill length, bill depth, flipper length, body mass, and sex. Given that the species prediction is a classification task, we specify a logistic regression model for binary or multinomial outcomes, depending on the number of species we wish to distinguish between.\nSpecify a logistic regression model using parsnip. For this example, we focus on a binary classification, though the palmerpenguins dataset allows for multinomial classification as well.\n\n# Specify a logistic regression model\npenguin_model = logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  set_mode(\"classification\")\n\n\n\nModel Fitting: Fit the model to the training data. This step involves estimating the model coefficients using the data prepared in the previous steps.\n\n\n# Fit the model to the training data\npenguin_fit = fit(penguin_model, species ~ bill_length_mm + \n                     bill_depth_mm + flipper_length_mm + \n                     body_mass_g + sex, \n                   data = penguin_train)\n\nThis command initiates the fitting process, where the glm engine estimates the coefficients that best predict the penguin species from the specified predictors. The set_mode(\"classification\") indicates that we are dealing with a classification problem, aiming to categorize penguins into different species.\n\n\nSummary and Interpretation: Once the model is fitted, we can summarize its findings to understand the impact of each predictor on the species classification.\n\n\n# Summarize the model fit\ntidy(penguin_fit)\n\n# A tibble: 6 × 5\n  term              estimate std.error statistic p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        73.2     38.0         1.93   0.0538\n2 bill_length_mm     -2.75     1.21       -2.27   0.0231\n3 bill_depth_mm      -1.22     0.718      -1.70   0.0893\n4 flipper_length_mm   0.0622   0.121       0.514  0.607 \n5 body_mass_g         0.0151   0.00721     2.10   0.0359\n6 sexmale             5.99     4.24        1.41   0.158 \n\n\nThe summary output will provide insights into the relationship between the predictors and the probability of a penguin belonging to a specific species. Coefficients with positive values indicate an increase in the likelihood of a penguin belonging to the target species with an increase in the predictor variable, while negative coefficients suggest the opposite.\nWe can obtain the impact on the odds ratio for each coefficient:\n\ntidy(penguin_fit, exponentiate = TRUE)\n\n# A tibble: 6 × 5\n  term              estimate std.error statistic p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)       6.14e+31  38.0         1.93   0.0538\n2 bill_length_mm    6.42e- 2   1.21       -2.27   0.0231\n3 bill_depth_mm     2.95e- 1   0.718      -1.70   0.0893\n4 flipper_length_mm 1.06e+ 0   0.121       0.514  0.607 \n5 body_mass_g       1.02e+ 0   0.00721     2.10   0.0359\n6 sexmale           4.01e+ 2   4.24        1.41   0.158 \n\n\n\n\nModel Evaluation: After fitting our logistic regression model to the penguin dataset, we aim to evaluate its performance to understand how well it predicts penguin species. We will use the yardstick package to compute the accuracy and ROC AUC of our model on the test set, and then generate a confusion matrix for a detailed breakdown of prediction results.\n\n\n# Load necessary library\nlibrary(yardstick)\n\n# Make predictions on the test set\npenguin_pred = predict(penguin_fit, new_data = penguin_test, \n                        type = \"prob\")\npenguin_class = predict(penguin_fit, new_data = penguin_test, \n                        type = \"class\")\npenguin_results = bind_cols(penguin_test, penguin_pred, penguin_class)\n\n# Calculate Accuracy\npenguin_results |&gt;\n  metrics(truth = species, estimate = .pred_class) |&gt;\n  filter(.metric == \"accuracy\")\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.988\n\n#plot ROC\npenguin_results |&gt; \n  roc_curve(truth = species, .pred_Chinstrap) |&gt; \n  autoplot()\n\n\n\n# Calculate ROC AUC\npenguin_results |&gt;\n  roc_auc(truth = species, .pred_Chinstrap)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary             1\n\n# Generate and display a Confusion Matrix\npenguin_conf_matrix = penguin_results |&gt;\n  conf_mat(truth = species, estimate = .pred_class)\n\npenguin_conf_matrix\n\n           Truth\nPrediction  Chinstrap Other\n  Chinstrap        20     0\n  Other             1    63\n\n\nIn this example: - We predict the species of penguins in the test set using our fitted logistic regression model. - We compute the accuracy of our predictions, providing a simple metric that indicates the overall proportion of correct predictions. - We calculate the ROC AUC (Area Under the Receiver Operating Characteristic Curve), which provides a measure of model performance across all classification thresholds. This metric is particularly useful for evaluating binary classification models in scenarios with imbalanced class distributions. - We generate a confusion matrix, offering a detailed view of the model’s predictions, including true positives, false positives, true negatives, and false negatives. This matrix helps us understand the types of errors our model is making.\nThis approach to model evaluation offers a comprehensive view of performance, balancing simplicity with depth of insight, and equips us with the information needed to make informed decisions about potential model improvements.\n\n\nModel Tuning: Model tuning aims to optimize the performance of a logistic regression model by systematically searching for the best hyperparameter values. In this example, we focus on using regularization to prevent overfitting and improve model generalizability. Regularization adds a penalty to the size of coefficients to shrink them towards zero, thus simplifying the model. We will use the glmnet engine, which supports both L1 (lasso) and L2 (ridge) regularization, and the tune package to find the optimal mix of regularization type and strength.\nSpecify the Model with Regularization Parameters\nFirst, we specify a logistic regression model and indicate that the regularization parameters penalty (lambda) and mixture (alpha) are to be tuned.\n\nlibrary(glmnet)\n\nlogistic_spec = logistic_reg(penalty = tune(), mixture = tune()) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\") \n\nDefine the Tuning Grid\nWe define a tuning grid that specifies a range of values for penalty and a set of values for mixture. The penalty controls the strength of the regularization, while the mixture determines the balance between L1 and L2 regularization.\n\ntuning_grid = grid_regular(\n  penalty(range = c(0.001, 1), trans = log10_trans()),\n  mixture( c(0.1, 0.9)),\n  levels = 20\n)\n\nCross-Validation Setup\nWe prepare for cross-validation by splitting the data into training and testing sets and defining the resampling method.\n\nset.seed(123)\ndata_split = initial_split(penguins, prop = 0.75)\ncv_folds = vfold_cv(training(data_split), v = 5)\n\nModel Tuning\nUsing the tune_grid() function, we search over the grid of hyperparameters within the cross-validation framework to find the combination that maximizes the model’s performance, typically using accuracy or ROC AUC as the criterion.\n\ntune_results = tune_grid(\n  object = workflow() |&gt;\n    add_recipe(penguin_recipe) |&gt;\n    add_model(logistic_spec),\n  resamples = cv_folds,\n  grid = tuning_grid,\n  metrics = metric_set(roc_auc, accuracy)\n)\n\nSelect Best Hyperparameters\nAfter the tuning process, we identify the best hyperparameters based on the chosen performance metric.\n\nbest_params = tune_results |&gt; select_best(metric = \"accuracy\")\nbest_params\n\n# A tibble: 1 × 3\n  penalty mixture .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1    1.00     0.1 Preprocessor1_Model001\n\n\nFinalize the Model\nWith the best hyperparameters identified, we finalize the model by fitting it to the full training set using these parameters.\n\nfinal_model = finalize_workflow(\n  workflow() |&gt;\n    add_recipe(penguin_recipe) |&gt;\n    add_model(logistic_spec),\n  best_params\n) |&gt;\n  fit(data = training(data_split))\n\nThis model tuning example demonstrates how to systematically explore the hyperparameter space to find the optimal settings for a logistic regression model, enhancing its predictive performance and generalizability."
  },
  {
    "objectID": "03_Logistic.html#advanced-considerations",
    "href": "03_Logistic.html#advanced-considerations",
    "title": "\n3  Logistic Regression\n",
    "section": "\n3.3 Advanced Considerations",
    "text": "3.3 Advanced Considerations\n\n3.3.1 Feature Interactions\n\n\nIn-depth Exploration: Beyond simply investigating interactions, it’s crucial to understand the nature and extent of the effect these interactions have on the outcome. This involves exploring different combinations of features to see if they produce synergistic (where the combined effect is greater than the sum of individual effects) or antagonistic interactions (where the combined effect is less than expected).\n\nPractical Implementation: Use the step_interact() function in the recipes package to create new interaction terms in your dataset. It’s important to limit the number of interaction terms to avoid overfitting and to focus on interactions that are theoretically plausible or supported by previous research.\n\nVisualization and Interpretation: Visualize these interactions using interaction plots or partial dependence plots to better understand their effects on the predicted outcome. This helps in interpreting the model in the context of these interactions and making more informed decisions.\n\n3.3.2 Model Resampling\n\n\nAdvanced Techniques: Explore beyond basic bootstrapping or k-fold cross-validation by incorporating techniques like stratified sampling or repeated cross-validation, which can provide more robust estimates of model performance, especially in datasets with imbalanced classes or complex structures.\n\nImpact Analysis: Evaluate how different resampling strategies affect model stability and performance. This can be done by comparing metrics across different resampling techniques and noting variations in model performance, which can inform the choice of the most appropriate resampling strategy.\n\nSoftware Implementation: Utilize the rsample package within tidymodels to implement advanced resampling techniques. Functions like vfold_cv() for cross-validation and bootstraps() for bootstrapping are highly customizable and can be adapted to specific requirements.\n\n3.3.3 Post-modeling Analysis\n\n\nDiagnostic Tools: Expand the toolbox for model diagnostics by including techniques like variance inflation factor (VIF) to check for multicollinearity, residual plots for heteroscedasticity, and influence plots to identify outliers or influential observations that could unduly affect the model.\n\nModel Comparison and Selection: Beyond diagnostics, employ model comparison techniques such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) for logistic regression models to select the model that best balances fit and complexity.\n\nSensitivity Analysis: Conduct sensitivity analyses to understand how changes in model inputs or assumptions affect outcomes. This can involve scenarios like changing the threshold for classification, altering the class balance, or simulating variations in predictor variables.\n\n3.3.4 Additional Considerations\n\n\nModel Interpretability: Leverage tools like vip (Variable Importance Plots) and DALEX to enhance model interpretability, providing insights into which features are driving predictions and how changes in feature values affect the predicted outcome.\n\nDeployment and Monitoring: Outline strategies for deploying logistic regression models in real-world applications, including considerations for model updating, monitoring model performance over time, and dealing with concept drift.\n\nEthical Considerations: Address potential ethical issues, such as bias and fairness, in logistic regression models. This includes assessing the model for disparate impact on different groups and taking steps to mitigate any identified biases."
  },
  {
    "objectID": "04_SVM.html#linear-svm-classification",
    "href": "04_SVM.html#linear-svm-classification",
    "title": "\n4  Support Vector Machines\n",
    "section": "\n4.1 Linear SVM Classification",
    "text": "4.1 Linear SVM Classification\nLinear Support Vector Machine (SVM) Classification is a powerful method in machine learning that aims to find the optimal hyperplane which separates different classes in a dataset with the widest possible margin, hence ensuring robust classification boundaries. This section elaborates on the concept using the iris dataset as an example. This famous data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.\nLet’s first only consider the features petal length and petal width. We will also only focus on binary classification. First, let’s consider only the species setosa and versicolor.\n\ndata(iris)\nlibrary(tidyverse)\n# Filter the dataset\niris_filtered = iris |&gt;\n  filter(Species %in% c(\"versicolor\", \"setosa\")) |&gt; \n  mutate(Species = factor(Species))\n\niris_filtered |&gt; \n  ggplot(aes(x = Petal.Length, y = Petal.Width, color = Species))+\n  geom_point()\n\n\n\n\nIn this example, there is a clear separation of the two species in terms of the features petal length and petal width. A linear SVM tries to find a line that will separate the two classes. In this example, there are a lot of lines that would do this. However, if you have a line that is too close to the versicolor class, then when we have new observations that we want to predict, we are more likely to have some that would cross that line and be missclassified as setosa.\nIf we were to move to line too close to the setosa group, then we are more likely to have a new observation that would be missclassified as versicolor. So we need a line that will be somewhere in the “middle”. That is, we want the line that stays as far away from the closest training instances as possible.\nAn SVM classifier can be visualized as creating the broadest possible “street” (denoted by parallel dashed lines) separating the classes, a process known as large margin classification.\n\n\n\n\n\nThere are other lines that would give us this “street” that would separate the classes, however, the margin (width) of this street will be smaller like below.\n\n\n\n\n\nWhen we have a narrower street, then we have a higher chance of missclassifying observations when we have new data. Thus, our goal is to make the street as wide as possible. The middle solid line is then the decision boundary. Any new observation on the side closest to the blue points would be classified as versicolor and any observation on the other side of that line would be classified as setosa.\nNote that incorporating additional training observations away from the “street” does not impact the decision boundary; it is entirely shaped by (or “supported” by) the samples situated on the street’s edge. These particular samples are known as the support vectors.\nSoft Margin Classification\nImagine we have a situation where there is no clear separation in the two classes. Or perhaps we have an outlier like in the example below.\n\n\n\n\n\nWe would not be able to find a “street” that would separate these classes. When we enforce a rule requiring all data points to be off the street and positioned on the correct side, this approach is termed hard margin classification. However, hard margin classification encounters two primary challenges. The first challenge is that it demands the data to be linearly separable for effective application. The second challenge is its vulnerability to outliers, meaning that even a few anomalous data points can significantly impact the classification outcome.\nTo circumvent these problems, it’s beneficial to adopt a more adaptable approach. The goal is to strike an optimal balance between maximizing the width of the street and minimizing margin violations, which occur when observations land in the middle of the street or on the incorrect side. This approach is known as soft margin classification.\nThe balance between the width of the street and the margin violations is governed by the hyperparemeter \\(C\\). If \\(C\\) is to large, the street becomes too narrow but with few margin violations. If \\(C\\) is too small, then the street is too wide which results in a lot of margin violations. We will need to tune this hyperparameter. That is, try different values of \\(C\\) and find which one does the best job at predicting new observations.\n\n4.1.1 Feature Scaling\nIn the context of SVMs, the importance of feature scaling cannot be overstated. SVMs are sensitive to the scale of the features because the aim is to maximize the margin between classes. If one feature dominates because of its larger scale, the SVM might not perform effectively. Hence, preprocessing steps involving normalization or standardization of features are crucial.\nExample: Linear SVM Classification with the Iris Dataset Using tidymodels\nTo illustrate linear SVM classification using the tidymodels framework in R, we will focus on classifying two species of the iris dataset: virginica and versicolor.\nLoading the Dataset and Preprocessing\nFirst, we load the iris dataset and filter it to only include the two species of interest: virginica and versicolor (instead of setosa and versicolor above). We then split the dataset into a training set and a testing set to evaluate the model’s performance. We also will setup the training data to do cross validation to tune the hyperparameter $C4.\n\nlibrary(tidymodels)\nlibrary(tidyverse)\n\ndata(iris)\n# Filter the dataset\niris_filtered = iris |&gt;\n  filter(Species %in% c(\"versicolor\", \"virginica\")) |&gt; \n  mutate(Species = factor(Species))\n\n# Create a data split\nset.seed(123)\ndata_split = initial_split(iris_filtered, prop = 0.75)\ntrain_data = training(data_split)\ntest_data = testing(data_split)\n\ndat_folds = vfold_cv(train_data, v = 5, strata = Species)\n\nBelow is a plot of these species in the training data in terms of petal length and petal width. When we fit this model, we will use all four features (sepal width, sepal length, petal width, and petal length).\n\ntrain_data |&gt; \nggplot(aes(x = Petal.Length,y = Petal.Width, color = Species))+\n  geom_point()\n\n\n\n\nWhen we incorporate all four features, we can do a better job at predicting, however, it becomes difficult to visualize what the “street” look like. Instead of lines, the street is made up of hyperplanes which we cannot visualize.\nSpecifying the Model and Preprocessing Steps\nWe specify a linear SVM model using svm_linear() from the parsnip package, which is part of the tidymodels framework. The \\(C\\) hyperparameter is called cost in the svm_linear function. We will tell the model that we want to tune this hyperparameter. Additionally, we define preprocessing steps, including feature scaling, to ensure that all features contribute equally to the model.\n\n# Specify the model\nsvm_model = svm_linear(cost = tune()) |&gt;\n  set_engine(\"kernlab\") |&gt;\n  set_mode(\"classification\")\n\n# Preprocessing\nrecipe = recipe(Species ~ ., data = train_data) |&gt;\n  step_normalize(all_predictors())\n\nTune the Model\nWe will setup a grid of possible values of \\(C\\) to try. The function grid_regular checks from -10 to 5 on the log base 2 scale. That is, it checks from \\(2^{-10}=0.00098\\) to \\(2^5 = 32\\). The values of levels tells the function how many equally spaced values on this interval to check.\nWe then pass this tuning grid along with the 5-folded data to a workflow that includes the recipe and the model. We will find the value of \\(C\\) that maximizes accuracy\n\n#setup the possible values of C to check\ntuning_grid = grid_regular(\n  cost(),\n  levels = 20\n)\n\ntune_results = tune_grid(\n  object = workflow() |&gt;\n    add_recipe(recipe) |&gt;\n    add_model(svm_model),\n  resamples = dat_folds,\n  grid = tuning_grid\n)\n\nbest_params = select_best(tune_results, metric = \"accuracy\")\nbest_params\n\n# A tibble: 1 × 2\n   cost .config              \n  &lt;dbl&gt; &lt;chr&gt;                \n1 0.402 Preprocessor1_Model12\n\n\nTraining the Model\nWith the model and preprocessing steps specified, we can now train the SVM model on the training data.\n\n# Workflow\nfitted_model = finalize_workflow(\n  workflow() |&gt;\n    add_recipe(recipe) |&gt;\n    add_model(svm_model),\n  best_params\n) |&gt;\n  fit(data = train_data)\n\n Setting default kernel parameters  \n\n\nEvaluating the Model\nFinally, we evaluate the model’s performance on the testing set to understand its effectiveness in classifying the two species.\n\n# Predictions\npredictions = predict(fitted_model, new_data = test_data) |&gt;\n  bind_cols(test_data)\n\n# Evaluate\nmetrics = metrics(predictions, truth = Species, estimate = .pred_class)\nconf_mat = conf_mat(predictions, truth = Species, estimate = .pred_class)\n\nprint(metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.96 \n2 kap      binary         0.920\n\nprint(conf_mat)\n\n            Truth\nPrediction   versicolor virginica\n  versicolor         12         0\n  virginica           1        12\n\n\nThis example demonstrates how to implement linear SVM classification using the tidymodels framework in R, focusing on the nuances of dealing with a subset of the iris dataset. Through this example, we see the robustness of SVM in creating a model that effectively separates the two chosen species based on their feature measurements, underscoring the importance of feature scaling and model specification in the process."
  },
  {
    "objectID": "04_SVM.html#nonlinear-svm-classification",
    "href": "04_SVM.html#nonlinear-svm-classification",
    "title": "\n4  Support Vector Machines\n",
    "section": "\n4.2 Nonlinear SVM Classification",
    "text": "4.2 Nonlinear SVM Classification\nFor datasets that are not linearly separable, SVMs can be extended to perform nonlinear classification. By adding polynomial features or using similarity features, datasets can often be transformed into a linearly separable form. We will discuss methods to handle nonlinear datasets, including using polynomial features and the kernel trick, which allows for operating in a high-dimensional space without explicitly computing the coordinates of the data points in that space.\n\n4.2.1 Introduction to Kernel Trick\nThe kernel trick is central to enabling SVMs to perform complex nonlinear classifications. This technique involves mapping input features into high-dimensional spaces where the data points that are not linearly separable in the original space might become linearly separable. Crucially, the kernel trick does this mapping without explicitly computing the coordinates in the high-dimensional space, thereby avoiding the computational complexity that such calculations would entail. Common kernels include:\n\n\nPolynomial Kernel: Adds polynomial features of a given degree. This kernel is powerful for capturing the interaction between features up to a specific degree.\n\nRadial Basis Function (RBF) or Gaussian Kernel: Considers all possible polynomials of all degrees, giving more weight to the features that are closer to the target. This kernel is particularly effective for cases where the relationship between the class boundaries is not only nonlinear but also varies in complexity across the dataset.\n\nSigmoid Kernel: Mirrors the use of sigmoid functions in neural networks and can transform the feature space in ways that are beneficial for certain types of datasets.\n\nAdvantages of Kernel SVM\nThe primary advantage of using kernel SVMs lies in their flexibility and ability to handle real-world data that often display complex patterns and nonlinear relationships. Kernel SVMs can capture intricate structures without requiring a massive increase in computational resources typically associated with high-dimensional space mapping.\nChoosing the Right Kernel\nThe choice of kernel significantly affects the model’s performance. No single kernel universally outperforms others across all tasks; the decision is highly data-dependent. Cross-validation can help in selecting the best kernel and its parameters for a given dataset.\n\n\nPolynomial kernels are suitable when the relationship between variables is expected to be polynomial.\n\nRBF kernels are a good default when there is little prior knowledge about the data.\n\nSigmoid kernels can be useful but are less commonly used than RBF or polynomial kernels.\n\n4.2.2 Parameter Tuning in Kernel SVM\nTwo critical parameters in kernel SVMs are \\(C\\) (the regularization parameter) and the kernel-specific parameter (like degree in polynomial kernels or \\(\\gamma\\) in RBF kernels). The parameter \\(C\\) controls the trade-off between achieving a low training error and a low testing error (generalization), whereas kernel-specific parameters control the shape of the boundary.\nBelow are examples of different hyperparameter values using RBF kernels.\n\n\n4.2.3 Implementing Nonlinear SVM\nWhen implementing nonlinear SVMs, the process typically involves several key steps:\n\nFeature Preprocessing: Scaling features to a similar scale is crucial because kernel SVMs are sensitive to the feature scales.\nModel Selection: Choosing between different SVM kernels based on the problem at hand and the nature of the data.\nCross-Validation: Employing cross-validation techniques to fine-tune hyperparameters (such as \\(C\\), kernel parameters like degree for polynomial kernel, or \\(\\gamma\\) for the RBF kernel) is essential for balancing the model’s complexity with its ability to generalize to unseen data.\nEvaluation: Assessing the model’s performance using appropriate metrics (like accuracy, precision, recall, F1 score for classification tasks) on a validation set not seen by the model during the training phase.\nApplication: Once tuned and evaluated, the model can be applied to new, unseen data for prediction tasks.\n\nChallenges and Considerations\nWhile nonlinear SVMs are powerful, they come with their own set of challenges and considerations:\n\n\nComputation Cost: The computational complexity can be higher than for linear models, especially for large datasets and complex kernels.\n\nModel Interpretability: The decision boundaries created by nonlinear SVMs can be difficult to interpret compared to linear SVMs.\n\nOverfitting Risk: There is a higher risk of overfitting, especially with very flexible models like those using high-degree polynomial kernels or small \\(\\gamma\\) values in RBF kernels. Regularization and proper parameter tuning are vital to mitigate this risk."
  },
  {
    "objectID": "04_SVM.html#svm-regression",
    "href": "04_SVM.html#svm-regression",
    "title": "\n4  Support Vector Machines\n",
    "section": "\n4.3 SVM Regression",
    "text": "4.3 SVM Regression\nSVMs can also be applied to regression problems by reversing the objective: instead of trying to maximize the margin while keeping the instances outside the margin, SVM regression attempts to fit as many instances as possible within the margin while limiting margin violations. This approach is known as (\\(\\epsilon\\))-insensitive loss, where the model tries to find a line that captures as many instances as possible within a specified margin.\nHere’s an example demonstrating how to use SVM Regression with the tidymodels framework in R, specifically applied to the diamonds dataset. We’ll predict the price of diamonds based on their features such as carat, cut, color, and clarity.\nPrepare the Data\nThe diamonds dataset is available in the ggplot2 package. We’ll use a subset of the dataset to make the training process faster for this example.\n\ndata(\"diamonds\", package = \"ggplot2\")\nset.seed(123)\ndiamonds_sample = diamonds |&gt; sample_n(size = 2000)\n\n# Split the data into training and testing sets\nsplit = initial_split(diamonds_sample, prop = 0.75)\ntrain_data = training(split)\ntest_data = testing(split)\n\nDefine the Recipe\n\nrec = recipe(price ~ carat + cut + color + clarity, data = train_data) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_normalize(all_numeric_predictors())\n\nSpecify the Model\nWe’ll use an SVM model for regression. The svm_rbf() function from the parsnip package is suitable for this task. The RBF (Radial Basis Function) kernel is commonly used for non-linear regression problems.\n\nsvm_mod = svm_rbf(cost = tune(), rbf_sigma = tune()) |&gt;\n  set_engine(\"kernlab\") |&gt;\n  set_mode(\"regression\")\n\nTune the Model\nWe’ll use cross-validation to tune the hyperparameters of the model.\n\n# Create a 5-fold cross-validation object\nfolds = vfold_cv(train_data, v = 5)\n\n# Create a grid for tuning\ngrid = grid_regular(cost(range = c(-5, 2)),\n                     rbf_sigma(range = c(-5, 2)),\n                     levels = 5)\n\n# Tune the model\ntune_res = tune_grid(\n  object = workflow() |&gt;\n        add_recipe(rec) |&gt;\n        add_model(svm_mod),\n  resamples = folds,\n  grid = grid\n)\n\n# Select the best hyperparameters\nbest_params = select_best(tune_res, metric=\"rmse\")\nbest_params\n\n# A tibble: 1 × 3\n   cost rbf_sigma .config              \n  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                \n1     4    0.0316 Preprocessor1_Model15\n\n\nFit the Final Model\nAfter finding the best hyperparameters, we fit the final model to the training data.\n\nfinal_svm = finalize_workflow(\n  workflow() |&gt;\n    add_recipe(rec) |&gt;\n    add_model(svm_mod),\n  best_params\n) |&gt;\n  fit(data = train_data)\n\nEvaluate the Model\nFinally, we evaluate the model’s performance on the testing set.\n\npredictions = predict(final_svm, new_data = test_data) |&gt;\n  bind_cols(test_data)\n\nmetrics = metrics(predictions, truth = price, estimate = .pred)\nmetrics\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     780.   \n2 rsq     standard       0.962\n3 mae     standard     491."
  }
]